{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Concise Summary of Transformers: Part 1 - Core Concepts\n",
    "\n",
    "\n",
    "\n",
    " ---\n",
    "\n",
    " This summary focuses on the fundamental building blocks and concepts of the Transformer architecture, tailored for a quick review.\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Table of Contents\n",
    "\n",
    "\n",
    "\n",
    " - [1. The Transformer Layer: Core Building Block](#1-the-transformer-layer-core-building-block)\n",
    "\n",
    " - [2. Deconstructing the Transformer Layer](#2-deconstructing-the-transformer-layer)\n",
    "\n",
    "   - [2.1 Multi-Head Attention: Learning from Different Perspectives](#21-multi-head-attention-learning-from-different-perspectives)\n",
    "\n",
    "   - [2.2 Scaled Dot-Product Self-Attention: The Engine of a Head](#22-scaled-dot-product-self-attention-the-engine-of-a-head)\n",
    "\n",
    "   - [2.3 Feed-Forward Network (FFN): Adding Depth](#23-feed-forward-network-ffn-adding-depth)\n",
    "\n",
    "   - [2.4 Add & Norm: Stabilizing and Enabling Deep Networks](#24-add--norm-stabilizing-and-enabling-deep-networks)\n",
    "\n",
    " - [3. Positional Encoding: Understanding Sequence Order](#3-positional-encoding-understanding-sequence-order)\n",
    "\n",
    " - [4. Transformers & Natural Language: Basic Preprocessing](#4-transformers--natural-language-basic-preprocessing)\n",
    "\n",
    "   - [4.1 Word Embeddings: Representing Words as Vectors](#41-word-embeddings-representing-words-as-vectors)\n",
    "\n",
    "   - [4.2 Tokenization: Breaking Down Text](#42-tokenization-breaking-down-text)\n",
    "\n",
    " - [Reference](#reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## <a id=\"1-the-transformer-layer-core-building-block\"></a>1. The Transformer Layer: Core Building Block\n",
    "\n",
    "\n",
    "\n",
    " The Transformer architecture is built by stacking multiple identical **Transformer Layers**. Each layer processes a sequence of input vectors (tokens) and outputs a sequence of vectors of the same dimension. Its primary role is to refine the representation of each token by considering its context within the entire sequence.\n",
    "\n",
    "\n",
    "\n",
    " A Transformer layer typically consists of two main sub-layers:\n",
    "\n",
    " 1.  A **Multi-Head Self-Attention** mechanism.\n",
    "\n",
    " 2.  A **Position-wise Feed-Forward Network (FFN)**.\n",
    "\n",
    "\n",
    "\n",
    " Residual connections and layer normalization are applied around each of these sub-layers.\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"image/Figure_9.png\" width=\"250px\"/>\n",
    "    <p><em>Figure 9: Architecture of one transformer layer, showcasing its main components.</em></p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    " ---\n",
    "\n",
    " <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## <a id=\"2-deconstructing-the-transformer-layer\"></a>2. Deconstructing the Transformer Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ### <a id=\"21-multi-head-attention-learning-from-different-perspectives\"></a>2.1 Multi-Head Attention: Learning from Different Perspectives\n",
    "\n",
    "\n",
    "\n",
    " Instead of performing a single attention function, Transformers employ **Multi-Head Attention**. This allows the model to jointly attend to information from different representational subspaces at different positions.\n",
    "\n",
    "\n",
    "\n",
    " * **Mechanism:** The input queries, keys, and values are linearly projected multiple times (once for each \"head\") into different lower-dimensional spaces. Attention is then performed in parallel for each of these projected versions. The outputs of the heads are concatenated and linearly projected again to produce the final output.\n",
    "\n",
    " * **Benefit:** It enables the model to capture various types of relationships and nuances in the data that a single attention mechanism might miss by averaging them out.\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"image/Figure_8.png\" width=\"500px\"/>\n",
    "    <p><em>Figure 8: Information flow for multi-head attention. The input is split, processed by several attention \"heads\" in parallel, and then their outputs are combined.</em></p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ### <a id=\"22-scaled-dot-product-self-attention-the-engine-of-a-head\"></a>2.2 Scaled Dot-Product Self-Attention: The Engine of a Head\n",
    "\n",
    "\n",
    "\n",
    " Each head in Multi-Head Attention uses **Scaled Dot-Product Self-Attention**. This is where tokens in a sequence interact to compute attention scores.\n",
    "\n",
    "\n",
    "\n",
    " * **Queries, Keys, Values (Q, K, V):** For each input token, three vectors are typically derived through learnable linear transformations:\n",
    "\n",
    "     * **Query (Q):** Represents the current token seeking information.\n",
    "\n",
    "     * **Key (K):** Represents an input token advertising its information.\n",
    "\n",
    "     * **Value (V):** Represents the actual content/features of an input token.\n",
    "\n",
    " * **Process:**\n",
    "\n",
    "     1.  **Similarity Scores:** The dot product of a token's Query vector with all other tokens' Key vectors is computed. This measures similarity.\n",
    "\n",
    "     2.  **Scaling:** These scores are scaled by dividing by the square root of the dimension of the key vectors ($\\sqrt{D_k}$). This helps stabilize gradients during training.\n",
    "\n",
    "     3.  **Softmax:** A softmax function is applied to the scaled scores to obtain attention weights (probabilities) that sum to 1. These weights determine how much focus to place on other tokens.\n",
    "\n",
    "     4.  **Weighted Sum:** The final output for the token is a weighted sum of all Value vectors, using the computed attention weights.\n",
    "\n",
    "\n",
    "\n",
    " The formula is:\n",
    "\n",
    " $$ \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{D_k}}\\right)V $$\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"image/Figure_6.png\" width=\"250px\"/>\n",
    "    <p><em>Figure 6: Information flow in a scaled dot-product self-attention mechanism, the core of an attention head.</em></p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    " The initial Q, K, V vectors are derived from the input token embeddings using separate learnable weight matrices ($W^{(q)}, W^{(k)}, W^{(v)}$), allowing the model to learn optimal projections for attention.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"image/Figure_4.png\" width=\"550px\"/>\n",
    "#     <p><em>Figure 4: Calculation of QK<sup>T</sup> from input X and learnable weight matrices.</em></p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    " ---\n",
    "\n",
    " <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ### <a id=\"23-feed-forward-network-ffn-adding-depth\"></a>2.3 Feed-Forward Network (FFN): Adding Depth\n",
    "\n",
    "\n",
    "\n",
    " Following the multi-head attention sub-layer, each position's output is passed through a **Position-wise Feed-Forward Network (FFN)**.\n",
    "\n",
    "\n",
    "\n",
    " * **Mechanism:** This is typically a two-layer fully connected neural network with a non-linear activation function (e.g., ReLU) in between. Importantly, the *same* FFN (with the same weights) is applied independently to each token's representation in the sequence.\n",
    "\n",
    " * **Benefit:** It introduces additional non-linearity and allows the model to learn more complex transformations of each token's representation after contextual information has been aggregated by the attention mechanism.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ### <a id=\"24-add--norm-stabilizing-and-enabling-deep-networks\"></a>2.4 Add & Norm: Stabilizing and Enabling Deep Networks\n",
    "\n",
    "\n",
    "\n",
    " Around each of the two main sub-layers (Multi-Head Attention and FFN) in a Transformer layer, two operations are applied:\n",
    "\n",
    "\n",
    "\n",
    " 1.  **Residual Connection (Add):** The input to the sub-layer is added to the output of that sub-layer. This helps mitigate the vanishing gradient problem, allowing for much deeper networks to be trained effectively.\n",
    "\n",
    "     * Output = `SubLayer(Input) + Input`\n",
    "\n",
    " 2.  **Layer Normalization (Norm):** This operation normalizes the activations across the features for each token independently. It helps stabilize the learning process and reduces sensitivity to the initialization of weights.\n",
    "\n",
    "     * Final Output = `LayerNorm(SubLayer(Input) + Input)`\n",
    "\n",
    "\n",
    "\n",
    " These \"Add & Norm\" steps are crucial for the successful training of deep Transformer models.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## <a id=\"3-positional-encoding-understanding-sequence-order\"></a>3. Positional Encoding: Understanding Sequence Order\n",
    "\n",
    "\n",
    "\n",
    " The self-attention mechanism, by its nature, does not inherently consider the order of tokens in a sequence. If input tokens were shuffled, the attention scores would simply be permuted accordingly, losing the sequential information. This is problematic for tasks like language understanding where word order is critical.\n",
    "\n",
    "\n",
    "\n",
    " * **Solution:** **Positional Encodings** are added to the input embeddings at the bottom of the Transformer stack (before the first layer). These are vectors of the same dimension as the token embeddings.\n",
    "\n",
    " * **Purpose:** They inject information about the relative or absolute position of tokens in the sequence.\n",
    "\n",
    " * **Methods:**\n",
    "\n",
    "     * **Sinusoidal Positional Encodings:** A common method uses sine and cosine functions of different frequencies across the embedding dimensions. This method has the advantage of potentially generalizing to sequence lengths not seen during training.\n",
    "\n",
    "     * **Learned Positional Encodings:** Alternatively, the positional encodings can be learnable parameters, similar to token embeddings, where each position has a unique learned vector.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"image/Figure_10_b.png\" width=\"350px\"/>\n",
    "    <p><em>Figure 10b: Heatmap illustrating sinusoidal positional encoding vectors, where each row is a position and each column is an embedding dimension.</em></p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    " ---\n",
    "\n",
    " <br>\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## <a id=\"4-transformers--natural-language-basic-preprocessing\"></a>4. Transformers & Natural Language: Basic Preprocessing\n",
    "\n",
    "\n",
    "\n",
    " While Transformers are general-purpose sequence processors, their initial success was in Natural Language Processing (NLP). Key preprocessing steps are needed to convert raw text into a format suitable for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ### <a id=\"41-word-embeddings-representing-words-as-vectors\"></a>4.1 Word Embeddings: Representing Words as Vectors\n",
    "\n",
    "\n",
    "\n",
    " * **Challenge:** Neural networks operate on numbers, not raw text.\n",
    "\n",
    " * **Solution:** Words are mapped to dense vector representations called **word embeddings**. These embeddings capture semantic similarities, meaning words with similar meanings are closer in the vector space.\n",
    "\n",
    " * **Learning:** Methods like `word2vec` (e.g., CBOW, Skip-gram) learn these embeddings from large text corpora by predicting words from their context or vice-versa. These embeddings can be pre-trained or learned as part of the Transformer model itself.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"image/Figure_11_a.png\" width=\"350px\"/>\n",
    "    <p><em>Figure 11a: The Continuous Bag of Words (CBOW) model for learning word embeddings.</em></p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ### <a id=\"42-tokenization-breaking-down-text\"></a>4.2 Tokenization: Breaking Down Text\n",
    "\n",
    "\n",
    "\n",
    " * **Challenge:** Dealing with vast vocabularies, rare words, misspellings, and sub-word structures.\n",
    "\n",
    " * **Solution:** **Tokenization** breaks down text into smaller units called tokens, which are often sub-words or characters rather than full words. This helps manage vocabulary size and handle out-of-vocabulary words.\n",
    "\n",
    " * **Methods:** Algorithms like Byte Pair Encoding (BPE) start with individual characters and iteratively merge the most frequent pairs to form a vocabulary of tokens.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "    <img src=\"image/Figure_12.png\" width=\"450px\"/>\n",
    "    <p><em>Figure 12: Illustration of Byte Pair Encoding, where frequent character pairs like 'pe' are merged into single tokens.</em></p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    " ---\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ### <a id=\"reference\"></a>Reference\n",
    "\n",
    "\n",
    "\n",
    " Bishop, C. M. (2024). *Deep Learning: Foundations and Concepts*. Springer. (Chapter 12: Transformers).\n",
    "\n",
    " ---\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extra: how to make bag of words\n",
    "\n",
    "Bag of words (BoW) represents text as word frequency vectors using `sklearn.feature_extraction.text.CountVectorizer`  \n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) weights words by importance using `sklearn.feature_extraction.text.TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog ran in the park\",\n",
    "    \"A cat and a dog in the park\"\n",
    "]\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the sentences\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and' 'cat' 'dog' 'in' 'mat' 'on' 'park' 'ran' 'sat' 'the']\n",
      "\n",
      "Bag of Words representation:\n",
      "   and  cat  dog  in  mat  on  park  ran  sat  the\n",
      "0    0    1    0   0    1   1     0    0    1    2\n",
      "1    0    0    1   1    0   0     1    1    0    2\n",
      "2    1    1    1   1    0   0     1    0    0    1\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary:\", feature_names)\n",
    "print(\"\\nBag of Words representation:\")\n",
    "print(bow_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AB2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
