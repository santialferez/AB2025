{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Hugging Face Transformers: A Practical Introduction\n",
    "\n",
    "\n",
    "\n",
    " Welcome! This notebook will guide you through the Hugging Face ecosystem, focusing on the `transformers` library. We'll cover how to use pre-trained models for various tasks, how to fine-tune a model for text classification, and briefly touch upon other NLP tasks and sharing your work.\n",
    "\n",
    "\n",
    "\n",
    " **Target Audience:** Undergraduate biomedical engineering students.\n",
    "\n",
    " **Prior Knowledge:** Transformer architecture theory.\n",
    "\n",
    "\n",
    "\n",
    " Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Table of Contents\n",
    "\n",
    "\n",
    "\n",
    " - [1. Introduction to the Hugging Face Ecosystem](#1-introduction-to-the-hugging-face-ecosystem)\n",
    "\n",
    "   - [1.1. The Hugging Face Hub](#11-the-hugging-face-hub)\n",
    "\n",
    "   - [1.2. Core Libraries](#12-core-libraries)\n",
    "\n",
    " - [2. Using Pipelines](#2-using-pipelines)\n",
    "\n",
    "   - [2.1. Sentiment Analysis](#21-sentiment-analysis)\n",
    "\n",
    "   - [2.2. Text Generation](#22-text-generation)\n",
    "\n",
    "   - [2.3. Zero-Shot Classification](#23-zero-shot-classification)\n",
    "\n",
    "   - [2.4. Other Common Use Cases](#24-other-common-use-cases)\n",
    "\n",
    "   - [Exercise 1: Translation with Pipelines](#exercise-1-translation-with-pipelines)\n",
    "\n",
    " - [3. Fine-tuning for Text Classification](#3-fine-tuning-for-text-classification)\n",
    "\n",
    "   - [3.1. Load Dataset](#31-load-dataset)\n",
    "\n",
    "   - [3.2. Preprocessing with a Tokenizer](#32-preprocessing-with-a-tokenizer)\n",
    "\n",
    "   - [3.3. Load Pretrained Model](#33-load-pretrained-model)\n",
    "\n",
    "   - [3.4. Define Training Arguments](#34-define-training-arguments)\n",
    "\n",
    "   - [3.5. Define Metrics](#35-define-metrics)\n",
    "\n",
    "   - [3.6. Initialize Trainer](#36-initialize-trainer)\n",
    "\n",
    "   - [3.7. Train the Model](#37-train-the-model)\n",
    "\n",
    "   - [3.8. Evaluate the Model](#38-evaluate-the-model)\n",
    "\n",
    "   - [3.9. Brief Mention of Token Classification](#39-brief-mention-of-token-classification)\n",
    "\n",
    "   - [Exercise 2: Fine-tune on a Different Dataset](#exercise-2-fine-tune-on-a-different-dataset)\n",
    "\n",
    " - [4. Other NLP Tasks](#4-other-nlp-tasks)\n",
    "\n",
    "   - [4.1. Question Answering](#41-question-answering)\n",
    "\n",
    "   - [4.2. Summarization](#42-summarization)\n",
    "\n",
    "   - [4.3. Masked Language Modeling (Fill-Mask)](#43-masked-language-modeling-fill-mask)\n",
    "\n",
    "   - [Exercise 3: Question Answering](#exercise-3-question-answering)\n",
    "\n",
    " - [5. Model Sharing & Demos](#5-model-sharing--demos)\n",
    "\n",
    "   - [5.1. Sharing Models on the Hugging Face Hub](#51-sharing-models-on-the-hugging-face-hub)\n",
    "\n",
    "   - [5.2. Creating Simple Demos with Gradio](#52-creating-simple-demos-with-gradio)\n",
    "\n",
    "   - [Exercise 4: Gradio for Question Answering](#exercise-4-gradio-for-question-answering)\n",
    "\n",
    " - [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Setup\n",
    "\n",
    "\n",
    "\n",
    " First, let's install the necessary libraries. If you're running this in Google Colab, some of these might already be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's good practice to install specific versions for reproducibility,\n",
    "# but for a general intro, the latest stable versions are usually fine.\n",
    "# !pip install transformers datasets evaluate torch accelerate gradio -q\n",
    "\n",
    "# optional to download the models faster\n",
    "# !pip install huggingface_hub[hf_xet]\n",
    "# !pip install hf_xet\n",
    "\n",
    "# sometimes needed\n",
    "# !pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Introduction to the Hugging Face Ecosystem\n",
    "\n",
    "\n",
    "\n",
    " The Hugging Face ecosystem is a collection of tools and resources designed to make state-of-the-art machine learning, especially Natural Language Processing (NLP), accessible to everyone.\n",
    "\n",
    "\n",
    "\n",
    " ![HF Ecosystem](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg)\n",
    "\n",
    " *(Image Source: Hugging Face NLP Course)*\n",
    "\n",
    "\n",
    "\n",
    " ### 1.1. The Hugging Face Hub\n",
    "\n",
    "\n",
    "\n",
    " The [Hugging Face Hub](https://huggingface.co/) is a central platform where the community shares:\n",
    "\n",
    " - **Models**: Thousands of pre-trained models for various tasks (NLP, Computer Vision, Audio).\n",
    "\n",
    " - **Datasets**: A vast collection of datasets for training and evaluation.\n",
    "\n",
    " - **Spaces**: Interactive demos of ML models.\n",
    "\n",
    "\n",
    "\n",
    " You can browse, download, and contribute to the Hub. We'll be using it extensively.\n",
    "\n",
    "\n",
    "\n",
    " ### 1.2. Core Libraries\n",
    "\n",
    "\n",
    "\n",
    " The ecosystem revolves around several key open-source libraries:\n",
    "\n",
    "\n",
    "\n",
    " - **`transformers`**:\n",
    "\n",
    "   - Provides access to thousands of pre-trained Transformer models (like BERT, GPT-2, T5, etc.).\n",
    "\n",
    "   - Offers a unified API for loading models, tokenizers, and using them for inference and fine-tuning.\n",
    "\n",
    "   - Simplifies complex architectures into easy-to-use classes.\n",
    "\n",
    "\n",
    "\n",
    " - **`datasets`**:\n",
    "\n",
    "   - A library for easily accessing and processing datasets, especially large ones.\n",
    "\n",
    "   - Provides tools for downloading, caching, mapping (preprocessing), and evaluating datasets.\n",
    "\n",
    "   - Integrates seamlessly with `transformers` and other ML frameworks like PyTorch and TensorFlow.\n",
    "\n",
    "\n",
    "\n",
    " - **`tokenizers`**:\n",
    "\n",
    "   - Offers highly optimized (Rust-backed) tokenizer implementations.\n",
    "\n",
    "   - Handles the conversion of raw text into numerical inputs that models can understand.\n",
    "\n",
    "   - Supports various tokenization strategies like BPE, WordPiece, and Unigram.\n",
    "\n",
    "\n",
    "\n",
    " - **`evaluate`**:\n",
    "\n",
    "   - A library for easily evaluating machine learning models and datasets.\n",
    "\n",
    "   - Provides implementations for many common metrics (e.g., accuracy, F1, BLEU, ROUGE).\n",
    "\n",
    "\n",
    "\n",
    " - **`accelerate`**:\n",
    "\n",
    "   - Simplifies running PyTorch training scripts on any distributed configuration (single GPU, multiple GPUs, TPUs).\n",
    "\n",
    "   - Requires minimal changes to your existing PyTorch code.\n",
    "\n",
    "\n",
    "\n",
    " These libraries work together to provide a comprehensive toolkit for your NLP projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Using Pipelines\n",
    "\n",
    "\n",
    "\n",
    " The `pipeline` function from the `transformers` library is the easiest way to use pre-trained models for inference. It abstracts away most of the preprocessing and postprocessing steps.\n",
    "\n",
    "\n",
    "\n",
    " ![NLP Pipeline](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)\n",
    "\n",
    " *(Image Source: Hugging Face NLP Course)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.1. Sentiment Analysis\n",
    "\n",
    "\n",
    "\n",
    " Let's start with a sentiment analysis pipeline. This will classify a piece of text as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classifier = pipeline(\"sentiment-analysis\")\n",
    "results = sentiment_classifier([\"I love using Hugging Face!\", \"This is not what I expected.\"])\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.2. Text Generation\n",
    "\n",
    "\n",
    "\n",
    " Next, let's try generating some text. This pipeline uses a model like GPT-2 to complete a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "import textwrap\n",
    "\n",
    "set_seed(42) # For reproducible results\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=\"gpt2\") # You can specify other models like \"distilgpt2\"\n",
    "prompt = \"Biomedical engineering is a field that\"\n",
    "generated_texts = text_generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATED TEXT:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "generated_text = generated_texts[0]['generated_text']\n",
    "wrapped_text = textwrap.fill(generated_text, width=60)\n",
    "print(wrapped_text)\n",
    "\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.3. Zero-Shot Classification\n",
    "\n",
    "\n",
    "\n",
    " This pipeline allows you to classify text into categories you define on the fly, without needing to fine-tune a model specifically for those categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_classifier = pipeline(\"zero-shot-classification\")\n",
    "sequence_to_classify = \"This new medical device can detect heart anomalies.\"\n",
    "candidate_labels = [\"cardiology\", \"oncology\", \"neurology\", \"pediatrics\"]\n",
    "results = zero_shot_classifier(sequence_to_classify, candidate_labels)\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.4. Other Common Use Cases\n",
    "\n",
    "\n",
    "\n",
    " Pipelines support many other tasks, including:\n",
    "\n",
    " - **Named Entity Recognition (NER)**: `pipeline(\"ner\")` - Identifies entities like persons, organizations, locations.\n",
    "\n",
    " - **Question Answering**: `pipeline(\"question-answering\")` - Extracts answers from a given context.\n",
    "\n",
    " - **Translation**: `pipeline(\"translation_en_to_fr\")` - Translates text between languages.\n",
    "\n",
    " - **Fill-Mask (Masked Language Modeling)**: `pipeline(\"fill-mask\")` - Fills in masked words in a sentence.\n",
    "\n",
    "\n",
    "\n",
    " You can find more tasks and models on the [Hugging Face Hub](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 1: Translation with Pipelines\n",
    "\n",
    "\n",
    "\n",
    " **Task:** Use a pipeline to translate the following English sentence into French: \"Hugging Face is making NLP easy.\"\n",
    "\n",
    "\n",
    "\n",
    " **Hint:** You'll need to specify the task and a model suitable for English-to-French translation (e.g., `google-t5/t5-base`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Student Solution for Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Student Code Cell\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Fine-tuning for Text Classification\n",
    "\n",
    "![](https://raw.githubusercontent.com/nlp-with-transformers/notebooks/0cb211095b4622fa922f80fbdc9d83cc5d9e0c34/images/chapter01_transfer-learning.png)\n",
    "\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/nlp-with-transformers/notebooks/0cb211095b4622fa922f80fbdc9d83cc5d9e0c34/images/chapter02_encoder-fine-tuning.png)\n",
    "\n",
    " While pre-trained models are powerful, fine-tuning them on a specific dataset can significantly improve performance for your particular task. We'll walk through fine-tuning DistilBERT for sentiment analysis on the `emotion` dataset.\n",
    "\n",
    "\n",
    "\n",
    " This dataset contains tweets classified into six basic emotions: anger, fear, joy, love, sadness, and surprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate # The new library for metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.1. Load Dataset\n",
    "\n",
    "\n",
    "\n",
    " We'll use the `emotion` dataset from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"emotion\")\n",
    "print(raw_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.2. Preprocessing with a Tokenizer\n",
    "\n",
    "\n",
    "\n",
    " We need to convert the text inputs into numerical representations (tokens) that the model can understand. We'll use the `AutoTokenizer` to load the tokenizer associated with `distilbert-base-uncased`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a tokenization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Apply the tokenization function to all splits of the dataset using `map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention masks** are binary arrays (0s and 1s) that tell Transformer models which tokens are real content (1) versus padding (0). When batching sequences of different lengths, shorter texts get padded to match the longest one. Attention masks ensure the model ignores these meaningless padding tokens during processing, preventing them from affecting the actual content's representation. They're automatically created by Hugging Face tokenizers alongside `input_ids`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine two examples from the tokenized training set\n",
    "print(\"Example 1:\")\n",
    "print(f\"Text: {tokenized_datasets['train'][0]['text']}\")\n",
    "print(f\"Label: {tokenized_datasets['train'][0]['label']}\")\n",
    "print(f\"Input IDs (first 20): {tokenized_datasets['train'][0]['input_ids'][:20]}\")\n",
    "print(f\"Attention Mask (first 20): {tokenized_datasets['train'][0]['attention_mask'][:20]}\")\n",
    "\n",
    "print(\"\\nExample 2:\")\n",
    "print(f\"Text: {tokenized_datasets['train'][1]['text']}\")\n",
    "print(f\"Label: {tokenized_datasets['train'][1]['label']}\")\n",
    "print(f\"Input IDs (first 20): {tokenized_datasets['train'][1]['input_ids'][:20]}\")\n",
    "print(f\"Attention Mask (first 20): {tokenized_datasets['train'][1]['attention_mask'][:20]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.3. Load Pretrained Model\n",
    "\n",
    "\n",
    "\n",
    " We load `distilbert-base-uncased` with a sequence classification head. We need to specify the number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = raw_datasets[\"train\"].features[\"label\"].num_classes\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login in HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face (optional, but recommended for saving models)\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Uncomment the line below to login to Hugging Face\n",
    "# This will allow you to save your trained model to the Hugging Face Hub\n",
    "# notebook_login()\n",
    "\n",
    "print(\"Note: If you want to save your model to Hugging Face Hub, uncomment the notebook_login() line above\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.4. Define Training Arguments\n",
    "\n",
    "\n",
    "\n",
    " `TrainingArguments` is a class that contains all the hyperparameters for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's good practice to log in to Hugging Face if you want to push your model to the Hub.\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login() # Uncomment and run this if you want to push to hub\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_emotion_model\", # Directory to save the model\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1, # For a quick demo, usually 2-4 epochs are better\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",       # Save at the end of each epoch\n",
    "    load_best_model_at_end=True,\n",
    "    # push_to_hub=True, # Uncomment if you logged in and want to push\n",
    "    # hub_model_id=\"your_username/my_awesome_emotion_model\" # Uncomment and set your username\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.5. Define Metrics\n",
    "\n",
    "\n",
    "\n",
    " We'll use the `evaluate` library to compute accuracy and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\") # Use \"weighted\" for imbalanced datasets\n",
    "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1[\"f1\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.6. Initialize Trainer\n",
    "\n",
    "\n",
    "\n",
    " The `Trainer` class handles the training and evaluation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, let's take smaller subsets of the data\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000)) # 1000 samples for training\n",
    "small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(200)) # 200 samples for evaluation\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset, # Use tokenized_datasets[\"train\"] for full training\n",
    "    eval_dataset=small_eval_dataset,   # Use tokenized_datasets[\"validation\"] for full evaluation\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.7. Train the Model\n",
    "\n",
    "\n",
    "\n",
    " Now, we can start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a few minutes depending on your hardware.\n",
    "# If running on CPU, it will be much slower.\n",
    "# Consider using Google Colab with a GPU for faster training.\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.8. Evaluate the Model\n",
    "\n",
    "\n",
    "\n",
    " After training, evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.9. Brief Mention of Token Classification\n",
    "\n",
    "\n",
    "\n",
    " **Token Classification** is another important task where each token in a sentence is assigned a label.\n",
    "\n",
    " - **Named Entity Recognition (NER)**: Labels tokens as part of entities (e.g., Person, Organization, Location).\n",
    "\n",
    " - **Part-of-Speech (POS) Tagging**: Labels tokens with their grammatical role (e.g., Noun, Verb, Adjective).\n",
    "\n",
    "\n",
    "\n",
    " Fine-tuning for token classification is similar to sequence classification, but:\n",
    "\n",
    " 1. You use `AutoModelForTokenClassification`.\n",
    "\n",
    " 2. The labels are sequences of tags, one for each token.\n",
    "\n",
    " 3. Special care is needed to align labels with tokens after subword tokenization (as some words might be split into multiple tokens). The `word_ids()` method from a fast tokenizer is helpful here.\n",
    "\n",
    " 4. Metrics like `seqeval` are used for evaluation.\n",
    "\n",
    "\n",
    "\n",
    " **Data Collator for Token Classification:**\n",
    "\n",
    " For tasks like NER or POS tagging, a specialized data collator `DataCollatorForTokenClassification` is used. This collator correctly pads not only the input IDs and attention mask, but also the labels. It ensures that labels for padding tokens are set to a special value (often -100) so they are ignored by the loss function. This is crucial because, unlike sequence classification where you have one label per sequence, here you have a label for each token, and the padding needs to be handled consistently across inputs and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 2: Fine-tune on a Different Dataset\n",
    "\n",
    "\n",
    "\n",
    " **Task:** Modify the fine-tuning script above to use the `sst2` (Stanford Sentiment Treebank) dataset from the GLUE benchmark. This is a binary sentiment classification task (positive/negative).\n",
    "\n",
    "\n",
    "\n",
    " **Hints:**\n",
    "\n",
    " 1. Load the `sst2` dataset: `raw_datasets = load_dataset(\"glue\", \"sst2\")`.\n",
    "\n",
    " 2. The text column in `sst2` is named `\"sentence\"`.\n",
    "\n",
    " 3. The `sst2` dataset has 2 labels.\n",
    "\n",
    " 4. You might need to adjust batch sizes or number of samples for quicker execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Student Solution for Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Student Code Cell\n",
    "# Your code here. Try to load \"glue\", \"sst2\", adapt the tokenize_function,\n",
    "# num_labels, and then train and evaluate.\n",
    "# For a quick run, you can use small subsets like:\n",
    "# small_sst2_train_dataset = tokenized_sst2_datasets[\"train\"].shuffle(seed=42).select(range(200))\n",
    "# small_sst2_eval_dataset = tokenized_sst2_datasets[\"validation\"].shuffle(seed=42).select(range(50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Other NLP Tasks\n",
    "\n",
    "\n",
    "\n",
    " Transformers excel at a wide range of NLP tasks. Here's a quick look at a few more, using the `pipeline` for simplicity.\n",
    "\n",
    "\n",
    "\n",
    " ### 4.1. Question Answering\n",
    "\n",
    "\n",
    "\n",
    " Extractive Question Answering models find the answer to a question within a given text (context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "Hugging Face is a company developing tools for machine learning.\n",
    "It is famous for its Transformers library, which provides thousands of pre-trained models.\n",
    "The main office is in New York City.\n",
    "\"\"\"\n",
    "question = \"Where is Hugging Face's main office?\"\n",
    "answer = qa_pipeline(question=question, context=context)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer['answer']} (Score: {answer['score']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 4.2. Summarization\n",
    "\n",
    "\n",
    "\n",
    " Summarization models generate a shorter version of a long text while preserving key information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-6-6\") # Using a specific smaller model for faster demo\n",
    "long_text = \"\"\"\n",
    "Biomedical engineering (BME) or medical engineering is the application of engineering principles and design concepts\n",
    "to medicine and biology for healthcare purposes (e.g., diagnostic or therapeutic). \n",
    "BME is also traditionally logical sciences to advance health care treatment, including diagnosis, \n",
    "monitoring, and therapy. Also included under the scope of a biomedical engineer is the management \n",
    "of current medical equipment within hospitals while adhering to relevant industry standards. \n",
    "This involves making equipment recommendations, procurement, routine testing, and preventive maintenance, \n",
    "a role also known as a Biomedical Equipment Technician (BMET) or as clinical engineering.\n",
    "\"\"\"\n",
    "summary = summarizer(long_text, max_length=45, min_length=10, do_sample=False)\n",
    "print(\"Original Text Length:\", len(long_text))\n",
    "print(\"Summary:\", summary[0]['summary_text'])\n",
    "print(\"Summary Length:\", len(summary[0]['summary_text']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 4.3. Masked Language Modeling (Fill-Mask)\n",
    "\n",
    "\n",
    "\n",
    " These models predict masked (hidden) tokens in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_masker = pipeline(\"fill-mask\", model=\"distilbert-base-uncased\")\n",
    "masked_sentence = \"Biomedical engineers design [MASK] and systems.\"\n",
    "predictions = fill_masker(masked_sentence, top_k=3)\n",
    "for pred in predictions:\n",
    "    print(f\"{pred['sequence']} (Score: {pred['score']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 3: Question Answering\n",
    "\n",
    "\n",
    "\n",
    " **Task:** Use the question answering pipeline to find the answer to \"What is the capital of France?\" using the following context: \"France is a country in Western Europe. Its capital and largest city is Paris.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Student Solution for Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Student Code Cell\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Model Sharing & Demos\n",
    "\n",
    "\n",
    "\n",
    " Sharing your models and creating interactive demos is crucial for collaboration and showcasing your work.\n",
    "\n",
    "\n",
    "\n",
    " ### 5.1. Sharing Models on the Hugging Face Hub\n",
    "\n",
    "\n",
    "\n",
    " The Hugging Face Hub makes it easy to share your fine-tuned models.\n",
    "\n",
    "\n",
    "\n",
    " **Method 1: Using `TrainingArguments`**\n",
    "\n",
    "   - Set `push_to_hub=True` in `TrainingArguments`.\n",
    "\n",
    "   - Optionally, set `hub_model_id=\"your_username/your_model_name\"`.\n",
    "\n",
    "   - You need to be logged in (`huggingface-cli login` or `notebook_login()`).\n",
    "\n",
    "   - The `Trainer` will automatically upload your model during/after training.\n",
    "\n",
    "\n",
    "\n",
    " **Method 2: Manual Push**\n",
    "\n",
    "   - After training, you can use:\n",
    "\n",
    "     ```python\n",
    "\n",
    "     # Assuming `model` and `tokenizer` are your trained objects\n",
    "\n",
    "     model.push_to_hub(\"your_username/your_model_name\")\n",
    "\n",
    "     tokenizer.push_to_hub(\"your_username/your_model_name\")\n",
    "\n",
    "     ```\n",
    "\n",
    "   - This creates a repository on the Hub and uploads the model files.\n",
    "\n",
    "\n",
    "\n",
    " **Model Cards:**\n",
    "\n",
    " It's important to create a \"model card\" (a `README.md` file in your model repository) that describes your model, its training data, intended uses, limitations, and biases. The `Trainer` often generates a basic one.\n",
    "\n",
    "\n",
    "\n",
    " ![Model Card](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/model_card.png)\n",
    "\n",
    " *(Image Source: Hugging Face NLP Course)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 5.2. Creating Simple Demos with Gradio\n",
    "\n",
    "\n",
    "\n",
    " [Gradio](https://www.gradio.app/) is a Python library that allows you to quickly create web-based UIs for your machine learning models.\n",
    "\n",
    "\n",
    "\n",
    " ![Gradio Demo](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter9/gradio-demo-overview.png)\n",
    "\n",
    " *(Image Source: Hugging Face NLP Course)*\n",
    "\n",
    "\n",
    "\n",
    " Here's a very simple example using the sentiment classifier we fine-tuned (or a pre-trained one if fine-tuning was skipped/failed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Let's use the pre-trained sentiment pipeline for this demo\n",
    "# If you successfully fine-tuned your_awesome_emotion_model, you could use that.\n",
    "# sentiment_pipeline_gradio = pipeline(\"sentiment-analysis\", model=\"my_awesome_emotion_model\")\n",
    "sentiment_pipeline_gradio = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    results = sentiment_pipeline_gradio(text)\n",
    "    # Gradio's Label component expects a dictionary of labels to scores\n",
    "    return {res['label']: res['score'] for res in results}\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=predict_sentiment,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter text here...\"),\n",
    "    outputs=gr.Label(num_top_classes=2), # Show top 2 classes (POSITIVE, NEGATIVE)\n",
    "    title=\"Sentiment Analyzer\",\n",
    "    description=\"Enter some text and see its predicted sentiment.\"\n",
    ")\n",
    "\n",
    "# Launch the demo\n",
    "# iface.launch() # This will launch in a new tab or inline if in a notebook.\n",
    "# For this script, we'll just show how to define it.\n",
    "# To run it, you'd typically save this script and run `python your_script_name.py`\n",
    "# or run the cell in a Jupyter notebook.\n",
    "print(\"Gradio interface defined. Call iface.launch() to run it.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Running the Gradio Demo:**\n",
    "\n",
    " - If you run `iface.launch()`, Gradio will start a local web server.\n",
    "\n",
    " - You can access the demo in your browser (usually at `http://127.0.0.1:7860`).\n",
    "\n",
    " - You can also easily share a temporary live link by setting `share=True` in `launch()`.\n",
    "\n",
    " - For permanent hosting, you can use [Hugging Face Spaces](https://huggingface.co/spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise 4: Gradio for Question Answering\n",
    "\n",
    "\n",
    "\n",
    " **Task:** Create a simple Gradio interface for the question answering pipeline.\n",
    "\n",
    " It should take two text inputs: `context` and `question`, and output the `answer` as text.\n",
    "\n",
    "\n",
    "\n",
    " **Hints:**\n",
    "\n",
    " 1. Your prediction function will take `context` and `question` as arguments.\n",
    "\n",
    " 2. The `inputs` for `gr.Interface` will be a list of two `gr.Textbox` components.\n",
    "\n",
    " 3. The `outputs` will be a single `gr.Textbox`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Student Solution for Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Student Code Cell\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Conclusion\n",
    "\n",
    "\n",
    "\n",
    " You've now had a practical introduction to the Hugging Face ecosystem!\n",
    "\n",
    " We've covered:\n",
    "\n",
    " - The main libraries: `transformers`, `datasets`, `tokenizers`, `evaluate`, `accelerate`.\n",
    "\n",
    " - Using `pipeline` for quick inference on various tasks.\n",
    "\n",
    " - The process of fine-tuning a Transformer model for text classification, including data loading, tokenization, training, and evaluation.\n",
    "\n",
    " - A brief look at other NLP tasks like Question Answering and Summarization.\n",
    "\n",
    " - How to share your models on the Hub and create simple demos with Gradio.\n",
    "\n",
    "\n",
    "\n",
    " This foundation should help you explore more advanced topics and apply Transformers to your own projects in biomedical engineering and beyond! Remember that the Hugging Face Hub and documentation are your best friends for finding models, datasets, and learning more.\n",
    "\n",
    "\n",
    "\n",
    " Happy transforming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick minimal scripts for NLP\n",
    "\n",
    "https://github.com/muellerzr/minimal-trainer-zoo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
