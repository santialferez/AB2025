{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification with PyTorch\n",
    "\n",
    "This notebook demonstrates how to implement a neural network for MNIST digit classification using PyTorch. It follows a standard machine learning workflow:\n",
    "\n",
    "1. Data loading and preprocessing\n",
    "2. Model definition\n",
    "3. Training and validation\n",
    "4. Evaluation and visualization\n",
    "5. Model saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, we import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "We'll load the MNIST dataset using torchvision, apply transformations, and split it into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize with mean and std of MNIST\n",
    "])\n",
    "\n",
    "# Load MNIST dataset directly from torchvision\n",
    "train_full_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset into training and validation sets\n",
    "train_size = int(0.8 * len(train_full_dataset))\n",
    "val_size = len(train_full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for batch processing\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization\n",
    "\n",
    "Let's create a function to visualize some examples from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display sample images\n",
    "def plot_example(images, labels, num_samples=5):\n",
    "    \"\"\"Plot a selection of images and their labels\"\"\"\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        \n",
    "        # If input is a tensor, convert to numpy and reshape\n",
    "        if isinstance(images, torch.Tensor):\n",
    "            img = images[i].cpu().numpy()\n",
    "            if img.shape[0] == 1:  # If it's in format [1, 28, 28]\n",
    "                img = img.reshape(28, 28)\n",
    "        else:\n",
    "            img = images[i].reshape(28, 28)\n",
    "            \n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(f\"Label: {labels[i]}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few training examples\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "plot_example(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definition\n",
    "\n",
    "Now we'll define our neural network model for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        # Input layer: 28x28 = 784 input features\n",
    "        # First hidden layer: 256 neurons with ReLU activation and dropout\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        # Second hidden layer: 128 neurons with ReLU activation and dropout\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        # Output layer: 10 neurons (one for each digit 0-9)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input from [batch_size, 1, 28, 28] to [batch_size, 784]\n",
    "        x = x.view(-1, 28*28)\n",
    "        # Pass through first hidden layer\n",
    "        x = self.layer1(x)\n",
    "        # Pass through second hidden layer\n",
    "        x = self.layer2(x)\n",
    "        # Pass through output layer\n",
    "        logits = self.output(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simpler alternative using only  `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Define the neural network model using nn.Sequential\n",
    "# model = nn.Sequential(\n",
    "#     nn.Flatten(),  # Reshape input from [batch_size, 1, 28, 28] to [batch_size, 784]\n",
    "#     nn.Linear(28*28, 256),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(0.4),\n",
    "#     nn.Linear(256, 128),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(0.3),\n",
    "#     nn.Linear(128, 10)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and move it to the device\n",
    "model = MNISTClassifier().to(device)\n",
    "print(model)\n",
    "\n",
    "# Calculate the total number of trainable parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of trainable model parameters: {num_params}\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick explanation of Momentum in SGD\n",
    "\n",
    "Momentum in SGD adds a \"velocity\" component to parameter updates, similar to how \n",
    "a ball rolling down a hill accumulates momentum:\n",
    "\n",
    "v_t = μ*v_{t-1} + η*∇J(θ)  # velocity update\n",
    "θ_t = θ_{t-1} - v_t        # parameter update\n",
    "\n",
    "Where:\n",
    "- μ is the momentum coefficient (typically 0.9)\n",
    "- η is the learning rate\n",
    "- ∇J(θ) is the gradient of the loss function\n",
    "\n",
    "Example: Imagine a ball rolling down a valley. Without momentum, the ball moves \n",
    "directly downhill at each point (standard SGD). With momentum, the ball retains \n",
    "some of its velocity from previous movements, allowing it to:\n",
    "- Roll through small bumps (local minima)\n",
    "- Move faster in consistent directions\n",
    "- Dampen oscillations in narrow valleys\n",
    "\n",
    "This makes training faster and more stable, particularly for complex loss landscapes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics\n",
    "\n",
    "Let's define a function to compute accuracy and loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute accuracy and loss\n",
    "def compute_metrics(model, dataloader, criterion=None, device='cpu', calculate_loss=True, calculate_accuracy=True):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            \n",
    "            # Calculate loss only if requested and criterion is provided\n",
    "            if calculate_loss and criterion is not None:\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy only if requested\n",
    "            if calculate_accuracy:\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate average loss and accuracy based on what was requested\n",
    "    avg_loss = running_loss / len(dataloader) if calculate_loss and criterion is not None else None\n",
    "    accuracy = correct / total if calculate_accuracy else None\n",
    "    \n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Function\n",
    "\n",
    "Now we'll define the training function that will train our model over multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    # Lists to store metrics for plotting\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Print progress every 100 batches\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                      f\"Batch [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        # Calculate training accuracy only (skip loss calculation since we already have it)\n",
    "        train_accuracy, _ = compute_metrics(model, train_loader, device=device, calculate_loss=False)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_accuracy, val_loss = compute_metrics(model, val_loader, criterion, device, calculate_loss=True, calculate_accuracy=True)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Calculate total training time\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training\n",
    "\n",
    "Let's train our model and visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "\n",
    "Now let's evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model on test set\n",
    "def evaluate_on_test_set(model, test_loader, criterion, device):\n",
    "    print(\"\\n=== Final Evaluation on Test Set ===\")\n",
    "    print(\"Note: Test set has not been used during training or model selection\")\n",
    "    \n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Compute metrics on test set\n",
    "    test_accuracy, test_loss = compute_metrics(model, test_loader, criterion, device, calculate_loss=True, calculate_accuracy=True)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"==================================\\n\")\n",
    "    \n",
    "    return test_accuracy, test_loss\n",
    "\n",
    "# Evaluate the model on the test set (only at the end, after all training is complete)\n",
    "test_accuracy, test_loss = evaluate_on_test_set(model, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizing Model Predictions\n",
    "\n",
    "Let's visualize some of the model's predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize model predictions\n",
    "def visualize_predictions(model, dataloader, device, num_samples=5):\n",
    "    model.eval()\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Move tensors back to CPU for visualization\n",
    "    images = images.cpu()\n",
    "    labels = labels.cpu()\n",
    "    predicted = predicted.cpu()\n",
    "    \n",
    "    # Plot images with true and predicted labels\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        img = images[i][0].numpy()\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        \n",
    "        # Green if correct, red if wrong\n",
    "        color = 'green' if predicted[i] == labels[i] else 'red'\n",
    "        plt.title(f\"True: {labels[i]}\\nPred: {predicted[i]}\", color=color)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some predictions\n",
    "visualize_predictions(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyzing Misclassifications\n",
    "\n",
    "Let's find and visualize some examples that the model misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and visualize misclassified examples\n",
    "def visualize_errors(model, dataloader, device, num_errors=5):\n",
    "    model.eval()\n",
    "    errors_images = []\n",
    "    errors_labels = []\n",
    "    errors_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Find indices where predictions are wrong\n",
    "            error_indices = (predicted != labels).nonzero(as_tuple=True)[0]\n",
    "            \n",
    "            for idx in error_indices:\n",
    "                errors_images.append(data[idx].cpu())\n",
    "                errors_labels.append(labels[idx].item())\n",
    "                errors_preds.append(predicted[idx].item())\n",
    "                \n",
    "                if len(errors_images) >= num_errors:\n",
    "                    break\n",
    "            \n",
    "            if len(errors_images) >= num_errors:\n",
    "                break\n",
    "    \n",
    "    # Plot the errors\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i in range(min(num_errors, len(errors_images))):\n",
    "        plt.subplot(1, num_errors, i + 1)\n",
    "        img = errors_images[i][0].numpy()\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(f\"True: {errors_labels[i]}\\nPred: {errors_preds[i]}\", color='red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some misclassified examples\n",
    "visualize_errors(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Saving and Loading the Model\n",
    "\n",
    "Finally, let's save our trained model and demonstrate how to load it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"mnist_model.pth\")\n",
    "print(\"Model saved to mnist_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to load the model\n",
    "def load_model():\n",
    "    loaded_model = MNISTClassifier().to(device)\n",
    "    loaded_model.load_state_dict(torch.load(\"mnist_model.pth\"))\n",
    "    loaded_model.eval()\n",
    "    return loaded_model\n",
    "\n",
    "# Load the saved model and verify it works\n",
    "loaded_model = load_model()\n",
    "loaded_accuracy, loaded_loss = compute_metrics(loaded_model, test_loader, criterion, device,\n",
    " calculate_loss=True, calculate_accuracy=True)\n",
    " \n",
    "print(f\"Loaded model test loss: {loaded_loss:.4f}\")\n",
    "print(f\"Loaded model test accuracy: {loaded_accuracy:.4f}\")\n",
    "\n",
    "# Verify this matches the original model's accuracy\n",
    "print(f\"Original model test loss: {test_loss:.4f}\")\n",
    "print(f\"Original model test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Accuracy difference: {abs(loaded_accuracy - test_accuracy):.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Write a simple MNIST classifier using PyTorch\n",
    "1. Load only images of digits 0, and 8 from the MNIST dataset. Hint: uses `torch.utils.data.Subset` to filter the dataset.\n",
    "2. Create a simple neural network with one hidden layer (input -> 128 neurons -> output)\n",
    "3. Train for 5 epochs and print accuracy on test set\n",
    "4. Create a confusion matrix (can use scikit learn and seaborn or matplotlib ) to visualize the model's performance\n",
    "\n",
    "Your code should:\n",
    "- Filter the MNIST dataset to keep only digits 0, and 8\n",
    "- Use nn.Linear layers to build your model\n",
    "- Include proper training and evaluation loops\n",
    "- Display the confusion matrix as a heatmap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
