{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Transformers Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Table of Contents\n",
    "\n",
    "\n",
    "\n",
    " - [Introduction to Transformers](#introduction-to-transformers)\n",
    "\n",
    "   - [Key Advantages of Transformers](#key-advantages-of-transformers)\n",
    "\n",
    " - [1. Attention: The Heart of Transformers](#1-attention-the-heart-of-transformers)\n",
    "\n",
    "   - [1.1 Transformer Processing](#11-transformer-processing)\n",
    "   - [1.2 Attention Coefficients](#12-attention-coefficients)\n",
    "   - [1.3 Self-Attention: Looking at Itself](#13-self-attention-looking-at-itself)\n",
    "   - [1.4 Adding Learnable Parameters](#14-adding-learnable-parameters)\n",
    "   - [1.5 Scaled Self-Attention](#15-scaled-self-attention)\n",
    "     - [Exercise 1: Scaled Dot-Product Self-Attention Dimensions]\n",
    "   - [1.6 Multi-Head Attention: Multiple Perspectives](#16-multi-head-attention-multiple-perspectives)\n",
    "     - [Exercise 2: Multi-Head Attention Output]\n",
    "   - [1.7 Transformer Layers: Putting it Together](#17-transformer-layers-putting-it-together)\n",
    "     - [Exercise 3: Transformer Layer Components]\n",
    "   - [1.8 Computational Complexity](#18-computational-complexity)\n",
    "   - [1.9 Positional Encoding: Where are You?](#19-positional-encoding-where-are-you)\n",
    "     - [Exercise 4: Purpose of Positional Encoding]\n",
    "\n",
    " - [2. Transformers for Natural Language](#2-transformers-for-natural-language)\n",
    "\n",
    "   - [2.1 Word Embedding: Words as Vectors](#21-word-embedding-words-as-vectors)\n",
    "     - [Exercise 5: Word Embedding Lookup](#exercise-5-word-embedding-lookup)\n",
    "   - [2.2 Tokenization: Breaking Down Text](#22-tokenization-breaking-down-text)\n",
    "   - [2.3 Bag of Words: Simple but Orderless](#23-bag-of-words-simple-but-orderless)\n",
    "   - [2.4 Autoregressive Models: Adding Order](#24-autoregressive-models-adding-order)\n",
    "   - [2.5 Recurrent Neural Networks (RNNs): Processing Sequences Step-by-Step](#25-recurrent-neural-networks-rnns-processing-sequences-step-by-step)\n",
    "\n",
    " - [Reference](#reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Introduction to Transformers\n",
    "\n",
    "\n",
    "\n",
    " * **What are they?** Transformers are a groundbreaking development in deep learning. They're neural networks that are particularly good at understanding and processing sequences of data.\n",
    "\n",
    " * **Core Idea: Attention!** They use a mechanism called \"attention,\" which lets the network weigh the importance of different parts of the input data when making predictions. Think of it like how you pay more attention to certain words in a sentence to understand its meaning.\n",
    "\n",
    " * **Transformation Power:** They are called \"transformers\" because they transform an input set of data points (vectors) into a new set of data points in a different representation space. The goal is for this new space to have a richer, more useful representation for solving tasks.\n",
    "\n",
    " * **Versatility:** Transformers can handle various types of data, like unstructured sets of vectors (e.g., sensor readings), ordered sequences (like text or DNA), and more.\n",
    "\n",
    " * **NLP Revolution:** Originally introduced for Natural Language Processing (NLP) tasks like translation, they quickly outperformed older methods like Recurrent Neural Networks (RNNs).\n",
    "\n",
    " * **Beyond Text:** They've also shown excellent results in computer vision (image processing), and in models that combine multiple types of data (multimodal transformers for text, images, audio, etc.).\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Key Advantages of Transformers\n",
    "\n",
    "\n",
    "\n",
    " * **Transfer Learning:** Models trained on large datasets can be effectively adapted (fine-tuned) for many other specific tasks. These large, adaptable models are known as \"foundation models\".\n",
    "\n",
    " * **Self-Supervised Learning:** They can be trained on vast amounts of unlabeled data (like text from the internet), which is a huge plus.\n",
    "\n",
    " * **Scalability (The Scaling Hypothesis):** A key idea is that simply making the models bigger (more parameters) and training them on more data leads to significant performance improvements, even without changing the fundamental architecture.\n",
    "\n",
    " * **Parallel Processing:** Transformers are well-suited for modern parallel hardware like GPUs, allowing us to train extremely large models with trillions of parameters. These large models can show surprising \"emergent properties\"—capabilities that weren't explicitly programmed.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Attention: The Heart of Transformers\n",
    "\n",
    "\n",
    "\n",
    " The core concept that makes transformers so powerful is **attention**. It was first developed as an add-on for RNNs but was later shown to be incredibly effective on its own, leading to transformers replacing RNNs in most applications.\n",
    "\n",
    "\n",
    "\n",
    " * **Why Attention? Context Matters!**\n",
    "\n",
    "     Consider these sentences:\n",
    "\n",
    "     1.  \"I swam across the **river** to get to the other **bank**.\"\n",
    "\n",
    "     2.  \"I walked across the road to get **cash** from the **bank**.\"\n",
    "\n",
    "     The word \"bank\" means different things. How do we know? By looking at the context words like \"river\" or \"cash\". An attention mechanism allows a network to \"attend\" to these important context words.\n",
    "\n",
    "\n",
    "\n",
    "     <img src=\"image/Figure_1.png\" width=\"550px\"/>\n",
    "\n",
    "\n",
    "\n",
    "     *Figure 1: How 'river' and 'swam' influence the meaning of 'bank'. Thicker lines mean more influence.*\n",
    "\n",
    "\n",
    "\n",
    " * **Dynamic Weights:** In standard neural networks, the influence of inputs is determined by fixed weights learned during training. In contrast, attention uses weighting factors whose values *depend on the specific input data*.\n",
    "\n",
    "\n",
    "\n",
    "     <img src=\"image/Figure_2.jpg\" width=\"550px\"/>\n",
    "\n",
    "\n",
    "\n",
    "     *Figure 2: Example of learned attention weights. Lines show which words attend to others.*\n",
    "\n",
    "\n",
    "\n",
    " * **Richer Embeddings:** A transformer can be seen as creating richer \"embeddings\" (vector representations) where a word's representation depends on the other words in the sequence. So, \"bank\" in the two example sentences would have different transformed representations.\n",
    "\n",
    " * **Example: Proteins:** Proteins are sequences of amino acids. In 3D space, amino acids far apart in the sequence can become physically close and interact. Transformers allow these distant amino acids to \"attend\" to each other, improving 3D structure modeling.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.1 Transformer Processing\n",
    "\n",
    "\n",
    "\n",
    " * **Input = Tokens:** The input to a transformer is a set of vectors \\{$x_n$\\}, called **tokens**. A token could be a word in a sentence, a patch of an image, or an amino acid in a protein.\n",
    "\n",
    " * **Data Matrix X:** These token vectors are usually grouped into a matrix X, where each row is a token.\n",
    "\n",
    "\n",
    "\n",
    "     <img src=\"image/Figure_3.png\" width=\"300px\"/>\n",
    "\n",
    "\n",
    "\n",
    "     *Figure 3: The data matrix X, with N tokens and D features per token.*\n",
    "\n",
    "\n",
    "\n",
    " * **Transformer Layer:** The basic building block is a function that takes this matrix X and outputs a transformed matrix $\\tilde{X}$ of the same size: $\\tilde{X} = \\text{TransformerLayer}[X]$. Deep networks are built by stacking these layers.\n",
    "\n",
    " * **Two Stages in a Layer:**\n",
    "\n",
    "     1.  **Attention Stage:** Mixes information *across* different tokens.\n",
    "\n",
    "     2.  **Feed-Forward Stage:** Processes each token *independently*.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.2 Attention Coefficients\n",
    "\n",
    "\n",
    "\n",
    " How do we decide which input tokens are important for a given output token?\n",
    "\n",
    " * Let's say we want to compute an output vector $y_n$. This $y_n$ will be a weighted sum of all input vectors $x_m$:\n",
    "\n",
    "     $$y_n = \\sum_{m=1}^{N} a_{nm} x_m$$\n",
    "\n",
    "\n",
    "\n",
    " * The $a_{nm}$ are the **attention weights** or coefficients.\n",
    "\n",
    " * These weights must satisfy two conditions:\n",
    "\n",
    "     1.  $a_{nm} \\ge 0$ (non-negative)\n",
    "\n",
    "     2.  $\\sum_{m=1}^{N} a_{nm} = 1$ (sum to one for each output token $n$)\n",
    "\n",
    " * This means attention is distributed – paying more attention to one input means paying less to others. These coefficients are calculated based on the input data itself.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.3 Self-Attention: Looking at Itself\n",
    "\n",
    "\n",
    "\n",
    " How are the attention coefficients $a_{nm}$ determined? We use a concept called **self-attention**, drawing an analogy from information retrieval.\n",
    "\n",
    "\n",
    "\n",
    " * **Queries, Keys, and Values:**\n",
    "\n",
    "     * Imagine searching for a movie. You have a **query** (what you're looking for). Each movie in a database has a **key** (its attributes, like genre, actors) and a **value** (the movie file itself). The system matches your query to the keys to find the best movie (value).\n",
    "\n",
    " * **In Self-Attention:**\n",
    "\n",
    "     * Each input token $x_n$ plays three roles:\n",
    "\n",
    "         1.  It's a **Query (Q)**: Used to ask \"who am I similar to?\"\n",
    "\n",
    "         2.  It's a **Key (K)**: Used to say \"these are my characteristics.\"\n",
    "\n",
    "         3.  It's a **Value (V)**: Used to contribute to the output.\n",
    "\n",
    "     * Essentially, each token $x_n$ (as a query) is compared against all other tokens $x_m$ (as keys) to determine how much attention $x_n$ should pay to $x_m$.\n",
    "\n",
    " * **Calculating Similarity:** A simple way to measure similarity between a query $x_n$ and a key $x_m$ is their **dot product**: $x_n^T x_m$.\n",
    "\n",
    " * **Softmax for Weights:** To satisfy the non-negativity and sum-to-one constraints for attention weights $a_{nm}$, we use the softmax function on these dot products:\n",
    "\n",
    "     $$a_{nm} = \\frac{\\exp(x_n^T x_m)}{\\sum_{m'=1}^{N} \\exp(x_n^T x_{m'})}$$\n",
    "\n",
    "\n",
    "\n",
    " * **Output Calculation:** The output $y_n$ is then a weighted sum of the *value* vectors (which, in basic self-attention, are just the input vectors $x_m$ themselves).\n",
    "\n",
    " * **Matrix Form:** $$Y = \\text{Softmax}[XX^T]X$$ (where X provides queries, keys, and values). This is called **dot-product self-attention**.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.4 Adding Learnable Parameters\n",
    "\n",
    "\n",
    "\n",
    " The basic self-attention has no learnable parameters. To make it learnable and more flexible:\n",
    "\n",
    "\n",
    "\n",
    " * We introduce separate learnable weight matrices $W^{(q)}$, $W^{(k)}$, and $W^{(v)}$ to transform the input X into Queries (Q), Keys (K), and Values (V) respectively:\n",
    "\n",
    "     * $Q = XW^{(q)}$\n",
    "\n",
    "     * $K = XW^{(k)}$\n",
    "\n",
    "     * $V = XW^{(v)}$\n",
    "\n",
    " * These W matrices allow the network to learn how to best project the input tokens into representations suitable for querying, keying, and valuing.\n",
    "\n",
    " * The dimensions of Q and K must allow dot products. Typically, the output V has the same dimension as the input X to facilitate stacking layers and residual connections.\n",
    "\n",
    " * The attention output is then: $$Y = \\text{Softmax}[QK^T]V$$\n",
    "\n",
    "\n",
    "\n",
    "     <img src=\"image/Figure_4.png\" width=\"550px\"/>\n",
    "\n",
    "\n",
    "\n",
    "     *Figure 4: Calculating $QK^T$ for attention coefficients.*\n",
    "\n",
    "\n",
    "\n",
    "     <img src=\"image/Figure_5.png\" width=\"550px\"/>\n",
    "\n",
    "\n",
    "\n",
    "     *Figure 5: Calculating the final output Y using $QK^T$ and V.*\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.5 Scaled Self-Attention\n",
    "\n",
    "\n",
    "\n",
    " * **The Problem:** If the dot products $QK^T$ become very large, the gradients of the softmax function can become tiny, making learning slow or ineffective.\n",
    "\n",
    " * **The Solution:** Scale the dot products before the softmax. If query and key vectors have elements with zero mean and unit variance, their dot product has a variance of $D_k$ (the dimension of keys). So, we scale by $1/\\sqrt{D_k}$.\n",
    "\n",
    " * **Scaled Dot-Product Self-Attention**:\n",
    "\n",
    "     $$Y = \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{D_k}}\\right)V$$\n",
    "\n",
    "\n",
    "\n",
    "     <img src=\"image/Figure_6.png\" width=\"250px\"/>\n",
    "\n",
    "\n",
    "\n",
    "     *Figure 6: Information flow in a scaled dot-product self-attention head.*\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Algorithm 1: Scaled Dot-Product Self-Attention (PyTorch Implementation)\n",
    "\n",
    "\n",
    "\n",
    " Here's a PyTorch implementation of the Scaled Dot-Product Self-Attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q: torch.Size([2, 5, 64])\n",
      "Shape of K: torch.Size([2, 7, 64])\n",
      "Shape of V: torch.Size([2, 7, 128])\n",
      "Shape of Context Vectors: torch.Size([2, 5, 128])\n",
      "Shape of Attention Weights: torch.Size([2, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes Scaled Dot-Product Attention.\n",
    "    Input:\n",
    "        Q (Query): Tensor of shape (batch_size, n_queries, d_k)\n",
    "        K (Key):   Tensor of shape (batch_size, n_keys, d_k)\n",
    "        V (Value): Tensor of shape (batch_size, n_keys, d_v)\n",
    "        mask (optional): Tensor for masking future tokens (batch_size, n_queries, n_keys)\n",
    "    Output:\n",
    "        context: Tensor of shape (batch_size, n_queries, d_v)\n",
    "        attn_weights: Tensor of shape (batch_size, n_queries, n_keys)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        d_k = Q.size(-1)  # Dimension of keys (and queries)\n",
    "\n",
    "        # 1. Compute QK^T\n",
    "        # Q: (batch_size, n_queries, d_k)\n",
    "        # K.transpose(-2, -1): (batch_size, d_k, n_keys)\n",
    "        # scores: (batch_size, n_queries, n_keys)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "        # 2. Scale by sqrt(d_k)\n",
    "        scaled_scores = scores / math.sqrt(d_k)\n",
    "\n",
    "        # 3. Apply mask (if provided)\n",
    "        # The mask sets future positions to -infinity before softmax\n",
    "        if mask is not None:\n",
    "            # Ensure mask has the same shape or is broadcastable\n",
    "            # Mask values are typically 0 for positions to attend to, and 1 for masked positions.\n",
    "            # We want to set masked positions to a very small number (-inf) before softmax.\n",
    "            scaled_scores = scaled_scores.masked_fill(mask == 1, -1e9) # or float('-inf')\n",
    "\n",
    "        # 4. Apply softmax to get attention weights\n",
    "        # attn_weights: (batch_size, n_queries, n_keys)\n",
    "        attn_weights = F.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "        # 5. Multiply by V to get context vectors\n",
    "        # attn_weights: (batch_size, n_queries, n_keys)\n",
    "        # V: (batch_size, n_keys, d_v)\n",
    "        # context: (batch_size, n_queries, d_v)\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "# Example Usage:\n",
    "# Let's define some dimensions\n",
    "batch_size = 2\n",
    "n_queries = 5  # Sequence length of queries (e.g., target sequence)\n",
    "n_keys = 7     # Sequence length of keys/values (e.g., source sequence)\n",
    "d_k = 64       # Dimension of keys/queries\n",
    "d_v = 128      # Dimension of values\n",
    "\n",
    "# Create dummy tensors\n",
    "Q = torch.randn(batch_size, n_queries, d_k)\n",
    "K = torch.randn(batch_size, n_keys, d_k)\n",
    "V = torch.randn(batch_size, n_keys, d_v)\n",
    "\n",
    "# Optional: Create a mask (e.g., for causal attention in decoders)\n",
    "# This mask would prevent attention to future tokens.\n",
    "# For self-attention where n_queries == n_keys, a causal mask would be upper triangular.\n",
    "# Here, n_queries != n_keys, so a causal mask isn't directly applicable in the same way.\n",
    "# If this were self-attention (n_queries=n_keys=seq_len), a causal mask:\n",
    "# seq_len = 5\n",
    "# causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "# causal_mask = causal_mask.unsqueeze(0).expand(batch_size, -1, -1) # (batch_size, seq_len, seq_len)\n",
    "\n",
    "# Instantiate the attention module\n",
    "attention_module = ScaledDotProductAttention()\n",
    "\n",
    "# Get the output\n",
    "context_vectors, attention_weights = attention_module(Q, K, V) #, mask=causal_mask if applicable)\n",
    "\n",
    "print(\"Shape of Q:\", Q.shape)\n",
    "print(\"Shape of K:\", K.shape)\n",
    "print(\"Shape of V:\", V.shape)\n",
    "print(\"Shape of Context Vectors:\", context_vectors.shape)\n",
    "print(\"Shape of Attention Weights:\", attention_weights.shape)\n",
    "# print(\"Attention Weights (first batch, first query):\\n\", attention_weights[0, 0, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### <a id=\"exercise-1-scaled-dot-product-self-attention-dimensions\"></a>Exercise 1: Scaled Dot-Product Self-Attention Dimensions\n",
    "\n",
    "\n",
    "\n",
    " **Question:** In the Scaled Dot-Product Self-Attention, if your Query matrix `Q` has shape `(batch_size, 10, 32)`, your Key matrix `K` has shape `(batch_size, 12, 32)`, and your Value matrix `V` has shape `(batch_size, 12, 64)`, what will be the shape of the output context vector and the attention weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### <a id=\"solution-1\"></a>Solution 1\n",
    "\n",
    "`solve here`\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.6 Multi-Head Attention: Multiple Perspectives\n",
    "\n",
    "\n",
    "\n",
    " A single attention mechanism (\"head\") might average out different types of relationships (e.g., grammatical vs. semantic).\n",
    "\n",
    "\n",
    "\n",
    " * **Solution:** Use multiple attention heads in parallel, each with its own set of $W^{(q)}$, $W^{(k)}$, and $W^{(v)}$ matrices. This is like using multiple filters in a Convolutional Neural Network (CNN) layer.\n",
    "\n",
    " * For H heads, we get H output matrices $H_1, ..., H_H$.\n",
    "\n",
    " * These are concatenated and then linearly transformed by another weight matrix $W^{(o)}$ to produce the final output Y, which has the same dimension as the input X:\n",
    "\n",
    "     $$Y(X) = \\text{Concat}[H_1, ..., H_H]W^{(o)}$$\n",
    "\n",
    "\n",
    "\n",
    "     <img src=\"image/Figure_7.png\" width=\"450px\"/>\n",
    "\n",
    "\n",
    "\n",
    "     *Figure 7: Multi-head attention architecture. Outputs of heads are combined.*\n",
    "\n",
    "\n",
    "\n",
    "     <img src=\"image/Figure_8.png\" width=\"500px\"/>\n",
    "\n",
    "\n",
    "\n",
    "     *Figure 8: Information flow for multi-head attention.*\n",
    "\n",
    "\n",
    "\n",
    " * Typically, if the input/output dimension is D (embed_dim), each head might output a $D_v = D/H$ dimensional vector, so the concatenated vector is D-dimensional. The key/query dimension per head is often $D_k = D/H$ as well.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Algorithm 2: Multi-Head Attention (PyTorch Implementation from scratch)\n",
    "\n",
    "\n",
    "\n",
    " This implementation shows how Multi-Head Attention can be built using multiple Scaled Dot-Product Attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input X: torch.Size([4, 10, 256])\n",
      "Shape of Output from MultiHeadAttention: torch.Size([4, 10, 256])\n",
      "Shape of Attention Weights from MultiHeadAttention: torch.Size([4, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module.\n",
    "    Input:\n",
    "        X (Input Tensor): Tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        mask (optional): Mask for attention.\n",
    "    Output:\n",
    "        output: Tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        attn_weights: Tensor of shape (batch_size, num_heads, seq_len, seq_len) (or seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads,_bias=True): # Added _bias for compatibility with nn.MultiheadAttention\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim  # Total dimension of the model (D)\n",
    "        self.num_heads = num_heads  # Number of attention heads (H)\n",
    "        self.head_dim = embed_dim // num_heads  # Dimension of each head (d_k = d_v = D/H)\n",
    "\n",
    "        # Linear layers for Q, K, V for all heads (can be done in one go)\n",
    "        # These will project X from embed_dim to embed_dim (num_heads * head_dim)\n",
    "        # W_q, W_k, W_v in the paper are effectively these linear layers.\n",
    "        # Each W_h^(q), W_h^(k), W_h^(v) processes X to get Q_h, K_h, V_h.\n",
    "        # We can implement this by having one large linear layer and then splitting.\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=_bias) # Projects X to Q' (embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=_bias) # Projects X to K' (embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=_bias) # Projects X to V' (embed_dim)\n",
    "\n",
    "        # Scaled dot-product attention module (one for all heads after splitting)\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "        # Final linear layer W_o\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim, bias=_bias)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Splits the last dimension into (num_heads, head_dim).\n",
    "        Input x: (batch_size, seq_len, embed_dim)\n",
    "        Output: (batch_size, num_heads, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "    def combine_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Combines the heads back from (batch_size, num_heads, seq_len, head_dim)\n",
    "        to (batch_size, seq_len, embed_dim).\n",
    "        \"\"\"\n",
    "        x = x.transpose(1, 2).contiguous() # (batch_size, seq_len, num_heads, head_dim)\n",
    "        return x.view(batch_size, -1, self.embed_dim) # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "    def forward(self, X_query, X_key, X_value, mask=None):\n",
    "        # X_query, X_key, X_value are expected to be (batch_size, seq_len, embed_dim)\n",
    "        # For self-attention, X_query = X_key = X_value = X\n",
    "        batch_size = X_query.size(0)\n",
    "\n",
    "        # 1. Linearly project Q, K, V for all heads\n",
    "        # Q_proj, K_proj, V_proj will have shape (batch_size, seq_len, embed_dim)\n",
    "        Q_proj = self.W_q(X_query)\n",
    "        K_proj = self.W_k(X_key)\n",
    "        V_proj = self.W_v(X_value)\n",
    "\n",
    "        # 2. Split into multiple heads\n",
    "        # Q_heads, K_heads, V_heads will have shape (batch_size, num_heads, seq_len, head_dim)\n",
    "        Q_heads = self.split_heads(Q_proj, batch_size)\n",
    "        K_heads = self.split_heads(K_proj, batch_size)\n",
    "        V_heads = self.split_heads(V_proj, batch_size)\n",
    "\n",
    "        # 3. Apply scaled dot-product attention for each head\n",
    "        # context_heads: (batch_size, num_heads, seq_len_q, head_dim)\n",
    "        # attn_weights_heads: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        # If mask is provided, it should be compatible with (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        # or broadcastable, e.g., (batch_size, 1, seq_len_q, seq_len_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) # Add head dimension for broadcasting\n",
    "\n",
    "        context_heads, attn_weights_heads = self.attention(Q_heads, K_heads, V_heads, mask)\n",
    "\n",
    "        # 4. Concatenate heads (combine_heads)\n",
    "        # context_concat: (batch_size, seq_len_q, embed_dim)\n",
    "        context_concat = self.combine_heads(context_heads, batch_size)\n",
    "\n",
    "        # 5. Final linear projection (W_o)\n",
    "        # output: (batch_size, seq_len_q, embed_dim)\n",
    "        output = self.W_o(context_concat)\n",
    "\n",
    "        return output, attn_weights_heads\n",
    "\n",
    "\n",
    "# Example Usage (Self-Attention):\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "\n",
    "# Input tensor X\n",
    "X = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "# Instantiate MultiHeadAttention\n",
    "mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "\n",
    "# Forward pass (for self-attention, query, key, and value are the same)\n",
    "output, attention_weights = mha(X, X, X)\n",
    "\n",
    "print(\"Shape of Input X:\", X.shape)\n",
    "print(\"Shape of Output from MultiHeadAttention:\", output.shape)\n",
    "print(\"Shape of Attention Weights from MultiHeadAttention:\", attention_weights.shape)\n",
    "# attention_weights[0,0] would be the attention map for the first head of the first batch item.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Using `torch.nn.MultiheadAttention`\n",
    "\n",
    "\n",
    "\n",
    " PyTorch provides a built-in `MultiheadAttention` layer which is optimized and handles much of the complexity internally.\n",
    "\n",
    "\n",
    "\n",
    " **Key parameters for `torch.nn.MultiheadAttention`:**\n",
    "\n",
    " - `embed_dim`: Total dimension of the model.\n",
    "\n",
    " - `num_heads`: Number of parallel attention heads. `embed_dim` will be split across `num_heads`.\n",
    "\n",
    " - `dropout`: Dropout probability on attn_output_weights. Default: 0.0.\n",
    "\n",
    " - `bias`: If `True`, add bias to input/output projections. Default: `True`.\n",
    "\n",
    " - `batch_first`: If `True`, then the input and output tensors are provided as (batch, seq, feature). Default: `False` (seq, batch, feature). **It's common to set this to `True`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch.nn.MultiheadAttention:\n",
      "Shape of Query: torch.Size([4, 10, 256])\n",
      "Shape of Key: torch.Size([4, 12, 256])\n",
      "Shape of Value: torch.Size([4, 12, 256])\n",
      "Shape of Attention Output: torch.Size([4, 10, 256])\n",
      "Shape of Attention Output Weights (averaged): torch.Size([4, 10, 12])\n",
      "\n",
      "Self-Attention with nn.MultiheadAttention:\n",
      "Shape of X_pt: torch.Size([4, 10, 256])\n",
      "Shape of Self-Attention Output: torch.Size([4, 10, 256])\n",
      "Shape of Self-Attention Weights (averaged): torch.Size([4, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# Example using torch.nn.MultiheadAttention\n",
    "embed_dim_pt = 256\n",
    "num_heads_pt = 8\n",
    "batch_size_pt = 4\n",
    "seq_len_pt = 10 # For query\n",
    "seq_len_kv_pt = 12 # For key/value, can be different for cross-attention\n",
    "\n",
    "# Input tensors\n",
    "query = torch.randn(batch_size_pt, seq_len_pt, embed_dim_pt) # (N, L, E) if batch_first=True\n",
    "key   = torch.randn(batch_size_pt, seq_len_kv_pt, embed_dim_pt) # (N, S, E) if batch_first=True\n",
    "value = torch.randn(batch_size_pt, seq_len_kv_pt, embed_dim_pt) # (N, S, E) if batch_first=True\n",
    "\n",
    "# Instantiate nn.MultiheadAttention\n",
    "# IMPORTANT: Set batch_first=True if your inputs are (batch, seq, feature)\n",
    "pytorch_mha = nn.MultiheadAttention(embed_dim=embed_dim_pt, num_heads=num_heads_pt, batch_first=True)\n",
    "\n",
    "# Forward pass\n",
    "# The nn.MultiheadAttention layer expects query, key, value.\n",
    "# For self-attention, query, key, and value are the same.\n",
    "# It returns:\n",
    "# attn_output: (N, L, E) if batch_first=True\n",
    "# attn_output_weights: (N, L, S) - average weights over heads if need_weights=True\n",
    "# (N, num_heads, L, S) if average_attn_weights=False (PyTorch 1.9+)\n",
    "# For simplicity, we'll use the default average_attn_weights=True\n",
    "attn_output, attn_output_weights = pytorch_mha(query, key, value) # No mask applied here for simplicity\n",
    "\n",
    "print(\"Using torch.nn.MultiheadAttention:\")\n",
    "print(\"Shape of Query:\", query.shape)\n",
    "print(\"Shape of Key:\", key.shape)\n",
    "print(\"Shape of Value:\", value.shape)\n",
    "print(\"Shape of Attention Output:\", attn_output.shape)\n",
    "print(\"Shape of Attention Output Weights (averaged):\", attn_output_weights.shape)\n",
    "\n",
    "# Self-attention example with nn.MultiheadAttention\n",
    "X_pt = torch.randn(batch_size_pt, seq_len_pt, embed_dim_pt)\n",
    "self_attn_output, self_attn_weights = pytorch_mha(X_pt, X_pt, X_pt)\n",
    "print(\"\\nSelf-Attention with nn.MultiheadAttention:\")\n",
    "print(\"Shape of X_pt:\", X_pt.shape)\n",
    "print(\"Shape of Self-Attention Output:\", self_attn_output.shape)\n",
    "print(\"Shape of Self-Attention Weights (averaged):\", self_attn_weights.shape)\n",
    "\n",
    "\n",
    "# To get per-head attention weights (PyTorch 1.9+):\n",
    "# pytorch_mha_per_head = nn.MultiheadAttention(embed_dim=embed_dim_pt, num_heads=num_heads_pt, batch_first=True, average_attn_weights=False)\n",
    "# _, attn_output_weights_per_head = pytorch_mha_per_head(query, key, value, need_weights=True)\n",
    "# print(\"Shape of Attention Output Weights (per head):\", attn_output_weights_per_head.shape) # (N, num_heads, L, S)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### <a id=\"exercise-2-multi-head-attention-output\"></a>Exercise 2: Multi-Head Attention Output\n",
    "\n",
    "\n",
    "\n",
    " **Question:** If you have an input `X` of shape `(batch_size=16, seq_len=20, embed_dim=512)` and you use a Multi-Head Attention layer with `num_heads=8`, what is the expected shape of the final output of this layer? What is the dimension (`head_dim`) of queries, keys, and values within each attention head?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### <a id=\"solution-2\"></a>Solution 2\n",
    "\n",
    "`solve here`\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.7 Transformer Layers: Putting it Together\n",
    "\n",
    "\n",
    "\n",
    " A full transformer layer combines multi-head attention with other components:\n",
    "\n",
    "\n",
    "\n",
    " 1.  **Multi-Head Self-Attention:** As described above.\n",
    "\n",
    " 2.  **Add & Norm (Residual Connection + Layer Normalization):**\n",
    "\n",
    "     * The output of the multi-head attention is added to the original input X (a **residual connection**). This helps with training deep networks.\n",
    "\n",
    "     * Then, **Layer Normalization** is applied, which stabilizes training.\n",
    "\n",
    "     * So, $$Z = \\text{LayerNorm}[\\text{MultiHeadAttention}(X) + X]$$\n",
    "\n",
    " 3.  **Feed-Forward Network (Position-wise MLP):**\n",
    "\n",
    "     * The attention mechanism is primarily linear in how it combines value vectors. To add more non-linearity and processing power, the output Z from the \"Add & Norm\" step is passed through a standard Multi-Layer Perceptron (MLP).\n",
    "\n",
    "     * This MLP is applied independently to each token's representation (i.e., to each row of Z). It typically has one hidden layer.\n",
    "\n",
    " 4.  **Add & Norm (Again):**\n",
    "\n",
    "     * Another residual connection and layer normalization:\n",
    "\n",
    "         $$\\tilde{X} = \\text{LayerNorm}[\\text{MLP}(Z) + Z]$$\n",
    "\n",
    "\n",
    "\n",
    " This entire structure forms one **transformer layer**. Multiple such layers are stacked to build deep transformer models.\n",
    "\n",
    "\n",
    "\n",
    " <img src=\"image/Figure_9.png\" width=\"250px\"/>\n",
    "\n",
    "\n",
    "\n",
    " *Figure 9: Architecture of one transformer layer.*\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 3: Transformer Layer (Conceptual PyTorch Implementation)\n",
    "\n",
    "\n",
    "\n",
    " This shows how a single Transformer Encoder layer is constructed. `torch.nn.TransformerEncoderLayer` provides this out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input to TransformerEncoderLayer (Scratch): torch.Size([4, 10, 256])\n",
      "Shape of Output from TransformerEncoderLayer (Scratch): torch.Size([4, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\" Implements the Position-wise FeedForward network (MLP). \"\"\"\n",
    "    def __init__(self, embed_dim, ffn_dim, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        # ReLU is commonly used as the activation function\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, embed_dim)\n",
    "        x = self.linear1(x)         # (batch_size, seq_len, ffn_dim)\n",
    "        x = self.activation(x)      # (batch_size, seq_len, ffn_dim)\n",
    "        x = self.dropout(x)         # (batch_size, seq_len, ffn_dim)\n",
    "        x = self.linear2(x)         # (batch_size, seq_len, embed_dim)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayerScratch(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer Encoder Layer implemented from scratch components.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, dropout=0.1):\n",
    "        super(TransformerEncoderLayerScratch, self).__init__()\n",
    "        # Multi-Head Self-Attention\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads) # Using our scratch MHA\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.ffn = PositionwiseFeedForward(embed_dim, ffn_dim, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        # src: (batch_size, seq_len, embed_dim)\n",
    "        # src_mask: for self-attention, e.g., padding mask\n",
    "\n",
    "        # 1. Multi-Head Self-Attention part\n",
    "        # attn_output: (batch_size, seq_len, embed_dim)\n",
    "        attn_output, _ = self.self_attn(src, src, src, mask=src_mask)\n",
    "        # Add & Norm (Residual connection + LayerNorm)\n",
    "        # src has shape (batch_size, seq_len, embed_dim)\n",
    "        # attn_output has shape (batch_size, seq_len, embed_dim)\n",
    "        out1 = self.norm1(src + self.dropout1(attn_output))\n",
    "\n",
    "        # 2. Feed-Forward part\n",
    "        # ffn_output: (batch_size, seq_len, embed_dim)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        # Add & Norm\n",
    "        # out1 has shape (batch_size, seq_len, embed_dim)\n",
    "        # ffn_output has shape (batch_size, seq_len, embed_dim)\n",
    "        out2 = self.norm2(out1 + self.dropout2(ffn_output))\n",
    "\n",
    "        return out2\n",
    "\n",
    "# Example Usage:\n",
    "embed_dim_layer = 256\n",
    "num_heads_layer = 8\n",
    "ffn_dim_layer = 512 # Typically 2x to 4x embed_dim\n",
    "batch_size_layer = 4\n",
    "seq_len_layer = 10\n",
    "dropout_layer = 0.1\n",
    "\n",
    "src_tensor = torch.randn(batch_size_layer, seq_len_layer, embed_dim_layer)\n",
    "\n",
    "# Using the scratch implementation\n",
    "encoder_layer_scratch = TransformerEncoderLayerScratch(embed_dim_layer, num_heads_layer, ffn_dim_layer, dropout_layer)\n",
    "output_scratch = encoder_layer_scratch(src_tensor)\n",
    "print(\"Shape of Input to TransformerEncoderLayer (Scratch):\", src_tensor.shape)\n",
    "print(\"Shape of Output from TransformerEncoderLayer (Scratch):\", output_scratch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Using `torch.nn.TransformerEncoderLayer`\n",
    "\n",
    "\n",
    "\n",
    " PyTorch provides a convenient `TransformerEncoderLayer` module.\n",
    "\n",
    "\n",
    "\n",
    " **Key parameters for `torch.nn.TransformerEncoderLayer`:**\n",
    "\n",
    " - `d_model`: The number of expected features in the input (i.e., `embed_dim`).\n",
    "\n",
    " - `nhead`: The number of heads in the multiheadattention models.\n",
    "\n",
    " - `dim_feedforward`: The dimension of the feedforward network model. Default: 2048.\n",
    "\n",
    " - `dropout`: The dropout value. Default: 0.1.\n",
    "\n",
    " - `activation`: The activation function of the intermediate layer, can be a string (\"relu\" or \"gelu\") or a unary callable. Default: “relu”.\n",
    "\n",
    " - `batch_first`: If `True`, then the input and output tensors are provided as (batch, seq, feature). Default: `False`. **Set this to `True` for consistency.**\n",
    "\n",
    " - `norm_first`: If `True`, layer norm is applied before attention and feedforward operations, otherwise after. Default: `False`. (Pre-LN can sometimes lead to more stable training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of Output from torch.nn.TransformerEncoderLayer: torch.Size([4, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "# Example using torch.nn.TransformerEncoderLayer\n",
    "pytorch_encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=embed_dim_layer,\n",
    "    nhead=num_heads_layer,\n",
    "    dim_feedforward=ffn_dim_layer,\n",
    "    dropout=dropout_layer,\n",
    "    batch_first=True # Important!\n",
    ")\n",
    "\n",
    "output_pytorch_layer = pytorch_encoder_layer(src_tensor) # src_mask can also be passed\n",
    "print(\"\\nShape of Output from torch.nn.TransformerEncoderLayer:\", output_pytorch_layer.shape)\n",
    "\n",
    "# Note: To build a full Transformer Encoder, you would stack multiple TransformerEncoderLayer\n",
    "# instances using torch.nn.TransformerEncoder.\n",
    "# transformer_encoder = nn.TransformerEncoder(pytorch_encoder_layer, num_layers=6)\n",
    "# output_full_encoder = transformer_encoder(src_tensor)\n",
    "# print(\"Shape of Output from torch.nn.TransformerEncoder (6 layers):\", output_full_encoder.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### <a id=\"exercise-3-transformer-layer-components\"></a>Exercise 3: Transformer Layer Components\n",
    "\n",
    "\n",
    "\n",
    " **Question:** What are the two main sub-layers within a standard Transformer Encoder layer, and what is typically applied after each of these sub-layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### <a id=\"solution-3\"></a>Solution 3\n",
    "\n",
    "`solve here`\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.8 Computational Complexity\n",
    "\n",
    "\n",
    "\n",
    " * A transformer layer is generally more computationally efficient than a fully connected network for sequence data.\n",
    "\n",
    " * The attention part has a computational cost roughly proportional to $N^2D$ (where N is sequence length, D is token dimension).\n",
    "\n",
    " * The MLP part costs about $ND^2$.\n",
    "\n",
    " * The number of parameters is mainly in $D^2$.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.9 Positional Encoding: Where are You?\n",
    "\n",
    "\n",
    "\n",
    " * **The Problem:** The self-attention mechanism itself doesn't care about the order of tokens. If you shuffle the input tokens, the output tokens will be the same but also shuffled (this is called permutation equivariance). But for many tasks, like language, order is crucial! \"Man bites dog\" is different from \"Dog bites man\".\n",
    "\n",
    " * **The Solution:** We need to give the model information about the position of each token in the sequence. This is done by adding a **positional encoding vector** $r_n$ to each input token embedding $x_n$:\n",
    "\n",
    "     $$\\tilde{x}_n = x_n + r_n$$\n",
    "\n",
    "\n",
    "\n",
    " * The positional encoding vector $r_n$ has the same dimension as the token embedding $x_n$.\n",
    "\n",
    " * **Sinusoidal Positional Encodings:** A common method uses sine and cosine functions of different frequencies for different dimensions of the encoding vector:\n",
    "\n",
    "     For a position `pos` and dimension `i` (where `i` ranges from 0 to `embed_dim-1`):\n",
    "\n",
    "     $$\n",
    "\n",
    "     PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/embed\\_dim})\n",
    "\n",
    "     $$\n",
    "\n",
    "     $$\n",
    "\n",
    "     PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/embed\\_dim})\n",
    "\n",
    "     $$\n",
    "\n",
    "     (The $10000$ is a hyperparameter $L$ from the formula; $embed\\_dim$ is $D$).\n",
    "\n",
    "     * This creates unique encodings for each position, is bounded, can generalize to longer sequences, and makes it easier for the model to learn about relative positions.\n",
    "\n",
    "\n",
    "\n",
    "     <img src=\"image/Figure_10_a.png\" width=\"350px\"/>\n",
    "\n",
    "\n",
    "\n",
    "     *(a) Visualization of sinusoidal functions for positional encoding components.*\n",
    "\n",
    "\n",
    "\n",
    "     <img src=\"image/Figure_10_b.png\" width=\"350px\"/>\n",
    "\n",
    "\n",
    "\n",
    "     *(b) Heatmap of positional encoding vectors.*\n",
    "\n",
    "\n",
    "\n",
    " * **Learned Positional Encodings:** Another approach is to learn the positional encoding vectors during training (e.g., using an `nn.Embedding` layer where the input is position indices). This works well if sequence lengths are relatively constant but may not generalize to unseen lengths as effectively as sinusoidal encodings.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### PyTorch Implementation of Sinusoidal Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input Embeddings: torch.Size([4, 20, 512])\n",
      "Shape of Output with Positional Encodings: torch.Size([4, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the model embedding.\n",
    "            max_len (int): Maximum possible sequence length.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(SinusoidalPositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a long enough P matrix that can be sliced\n",
    "        # P: (max_len, embed_dim)\n",
    "        position = torch.arange(max_len).unsqueeze(1) # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim)) # (embed_dim/2)\n",
    "\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # Apply to even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # Apply to odd indices\n",
    "\n",
    "        # pe is of shape (max_len, embed_dim)\n",
    "        # We want to register it as a buffer so it's part of the model's state,\n",
    "        # but not trained as a parameter.\n",
    "        # We add an unsqueezed dimension for batch compatibility (though it's often sliced).\n",
    "        self.register_buffer('pe', pe.unsqueeze(0)) # (1, max_len, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, embed_dim)\n",
    "                        or (seq_len, batch_size, embed_dim) if not batch_first.\n",
    "                        We assume batch_first=True for this example based on common use.\n",
    "        Returns:\n",
    "            Tensor: Output tensor with positional encodings added.\n",
    "                    Shape: (batch_size, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        # x is (batch_size, seq_len, embed_dim)\n",
    "        # self.pe is (1, max_len, embed_dim)\n",
    "        # We need to add pe[:, :x.size(1)] to x\n",
    "        # The positional encoding is added to the input embeddings.\n",
    "        # The `pe` buffer is sliced up to the sequence length of the input `x`.\n",
    "        # x.size(1) gives the sequence length.\n",
    "        # self.pe[:, :x.size(1), :] has shape (1, seq_len, embed_dim)\n",
    "        # This will broadcast across the batch dimension of x.\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Example Usage:\n",
    "embed_dim_pe = 512\n",
    "max_seq_len_pe = 100\n",
    "batch_size_pe = 4\n",
    "current_seq_len_pe = 20 # Actual sequence length of the current batch\n",
    "\n",
    "# Create dummy input embeddings (e.g., after word embedding layer)\n",
    "input_embeddings = torch.randn(batch_size_pe, current_seq_len_pe, embed_dim_pe)\n",
    "\n",
    "# Instantiate positional encoding\n",
    "pos_encoder = SinusoidalPositionalEncoding(embed_dim=embed_dim_pe, max_len=max_seq_len_pe)\n",
    "\n",
    "# Add positional encodings\n",
    "output_with_pe = pos_encoder(input_embeddings)\n",
    "\n",
    "print(\"Shape of Input Embeddings:\", input_embeddings.shape)\n",
    "print(\"Shape of Output with Positional Encodings:\", output_with_pe.shape)\n",
    "# Check if values changed (they should have)\n",
    "# print(\"Original first token embedding (sum):\", input_embeddings[0,0,:].sum())\n",
    "# print(\"First token embedding with PE (sum):\", output_with_pe[0,0,:].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### <a id=\"exercise-4-purpose-of-positional-encoding\"></a>Exercise 4: Purpose of Positional Encoding\n",
    "\n",
    "\n",
    "\n",
    " **Question:** Why is positional encoding necessary in Transformer models, especially when processing sequential data like text? What would happen if it were omitted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### <a id=\"solution-4\"></a>Solution 4\n",
    "\n",
    "\n",
    "`solve here`\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Transformers for Natural Language\n",
    "\n",
    "\n",
    "\n",
    " Transformers were first developed for language tasks and have since become state-of-the-art.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.1 Word Embedding: Words as Vectors\n",
    "\n",
    "\n",
    "\n",
    " * **The Challenge:** Computers need numbers, not words. How do we represent words numerically?\n",
    "\n",
    " * **One-Hot Encoding:** Assign each word a unique vector with one '1' and the rest '0's. Simple, but:\n",
    "\n",
    "     * Vectors become very long for large vocabularies (e.g., hundreds of thousands of dimensions).\n",
    "\n",
    "     * Doesn't capture similarity (e.g., \"cat\" and \"kitten\" are just as different as \"cat\" and \"car\").\n",
    "\n",
    " * **Word Embeddings (Dense Vectors):** Map words into a lower-dimensional space (e.g., a few hundred dimensions) where similar words have similar vector representations.\n",
    "\n",
    "     * This is done using an embedding matrix E. If $x_n$ is a one-hot vector for a word, its embedding is $v_n = Ex_n$. In practice, we use integer indices for words and an embedding layer looks up these indices.\n",
    "\n",
    " * **Learning Embeddings (e.g., word2vec):**\n",
    "\n",
    "     * Embeddings are learned from large amounts of text. A popular method is **word2vec**.\n",
    "\n",
    "     * It's essentially a simple neural network trained on a \"predict the context\" or \"predict the word from context\" task.\n",
    "\n",
    "         * **Continuous Bag of Words (CBOW):** Predict the middle word from its surrounding context words.\n",
    "\n",
    "         * **Skip-grams:** Predict the surrounding context words given the middle word.\n",
    "\n",
    "\n",
    "\n",
    "         <img src=\"image/Figure_11_a.png\" width=\"350px\"/>\n",
    "\n",
    "\n",
    "\n",
    "         *(a) Continuous Bag of Words (CBOW)*\n",
    "\n",
    "\n",
    "\n",
    "         <img src=\"image/Figure_11_b.png\" width=\"350px\"/>\n",
    "\n",
    "\n",
    "\n",
    "         *(b) Skip-grams*\n",
    "\n",
    "     * This self-supervised learning helps capture semantic relationships. For example, vector(\"Paris\") - vector(\"France\") + vector(\"Italy\") ≈ vector(\"Rome\").\n",
    "\n",
    "     * Word embeddings can be pre-trained or learned as the first layer of a larger deep learning model.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### PyTorch `nn.Embedding` Layer\n",
    "\n",
    "\n",
    "\n",
    " PyTorch provides `torch.nn.Embedding` which is a simple lookup table that stores embeddings of a fixed dictionary and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input Indices: torch.Size([2, 5])\n",
      "Shape of Output Embeddings: torch.Size([2, 5, 300])\n"
     ]
    }
   ],
   "source": [
    "# Example of nn.Embedding\n",
    "num_embeddings = 1000  # Vocabulary size (K)\n",
    "embedding_dim = 300   # Dimension of the embedding space (D)\n",
    "\n",
    "# Create an embedding layer\n",
    "embedding_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "# Example input: a batch of sequences of word indices\n",
    "# Let's say batch_size = 2, sequence_length = 5\n",
    "# Word indices should be between 0 and num_embeddings-1\n",
    "input_indices = torch.LongTensor([[10, 2, 45, 700, 3],\n",
    "                                  [200, 34, 5, 0, 999]]) # Shape: (2, 5)\n",
    "\n",
    "# Get the embeddings\n",
    "output_embeddings = embedding_layer(input_indices) # Shape: (2, 5, 300)\n",
    "\n",
    "print(\"Shape of Input Indices:\", input_indices.shape)\n",
    "print(\"Shape of Output Embeddings:\", output_embeddings.shape)\n",
    "# print(\"Embedding for the first word of the first sequence ('10'):\\n\", output_embeddings[0, 0, :])\n",
    "\n",
    "# The weights of the embedding layer (the embedding matrix E) can be accessed:\n",
    "# print(\"Shape of Embedding Layer Weights:\", embedding_layer.weight.shape) # (num_embeddings, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### <a id=\"exercise-5-word-embedding-lookup\"></a>Exercise 5: Word Embedding Lookup\n",
    "\n",
    "\n",
    "\n",
    " **Question:** You have an `nn.Embedding` layer initialized with `num_embeddings=5000` (vocabulary size) and `embedding_dim=100`. If you provide an input tensor of word indices with shape `(batch_size=4, sequence_length=15)`, what will be the shape of the output tensor from this embedding layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### <a id=\"solution-5\"></a>Solution 5\n",
    "\n",
    "`solve here`\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.2 Tokenization: Breaking Down Text\n",
    "\n",
    "\n",
    "\n",
    " * **Beyond Fixed Dictionaries:** Using a fixed dictionary of words has problems with:\n",
    "\n",
    "     * Words not in the dictionary (Out-Of-Vocabulary or OOV words).\n",
    "\n",
    "     * Misspellings, punctuation, or other character sequences like code.\n",
    "\n",
    " * **Character-Level:** Using individual characters as input solves OOV but loses word structure and makes sequences very long.\n",
    "\n",
    " * **Tokenization:** The common solution is to break text into **tokens**, which are often sub-word units (small groups of characters).\n",
    "\n",
    "     * This allows the model to handle OOV words by representing them as a sequence of known sub-word tokens.\n",
    "\n",
    "     * Common words might be single tokens, while rare words are broken down.\n",
    "\n",
    "     * Helps with word variations like \"cook,\" \"cooks,\" \"cooked,\" \"cooking\" – they share the \"cook\" token.\n",
    "\n",
    " * **Byte Pair Encoding (BPE):** A popular tokenization algorithm.\n",
    "\n",
    "     1.  Start with individual characters as the initial set of tokens.\n",
    "\n",
    "     2.  Iteratively find the most frequently occurring adjacent pair of tokens in the text and merge them to form a new token.\n",
    "\n",
    "     3.  Repeat until a desired vocabulary size is reached.\n",
    "\n",
    "\n",
    "\n",
    "     <img src=\"image/Figure_12.png\" width=\"450px\"/>\n",
    "\n",
    "\n",
    "\n",
    "     *Figure 12: Example of Byte Pair Encoding. 'pe' is merged first, then 'ck', etc.*\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.3 Bag of Words: Simple but Orderless\n",
    "\n",
    "\n",
    "\n",
    " A very basic way to model a sequence of words $x_1, ..., x_N$:\n",
    "\n",
    " * Assume all words are drawn independently from the same distribution:\n",
    "\n",
    "     $$p(x_1, ..., x_N) = \\prod_{n=1}^N p(x_n)$$\n",
    "\n",
    "\n",
    "\n",
    " * This model completely **ignores the order of words** (hence \"bag of words\").\n",
    "\n",
    " * Useful for simple tasks like **text classification** (e.g., sentiment analysis using Naive Bayes, where word probabilities differ per class).\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.4 Autoregressive Models: Adding Order\n",
    "\n",
    "\n",
    "\n",
    " To account for word order, we can use autoregressive models:\n",
    "\n",
    " * The joint probability of a sequence is factored as a product of conditional probabilities:\n",
    "\n",
    "     $$p(x_1, ..., x_N) = \\prod_{n=1}^N p(x_n | x_1, ..., x_{n-1})$$\n",
    "\n",
    "\n",
    "\n",
    "     (The probability of a word depends on all preceding words).\n",
    "\n",
    " * **n-gram Models:** A simplification where the probability of a word depends only on the $L$ preceding words (e.g., $L=1$ for bi-grams, $L=2$ for tri-grams).\n",
    "\n",
    "     * For $L=2$ (tri-gram):\n",
    "\n",
    "         $$p(x_1, ..., x_N) = p(x_1)p(x_2|x_1)\\prod_{n=3}^N p(x_n | x_{n-1}, x_{n-2})$$\n",
    "\n",
    "\n",
    "\n",
    " * **Limitation:** These models struggle to capture **long-range dependencies** in language because the context is limited, and the number of parameters grows exponentially with $L$.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.5 Recurrent Neural Networks (RNNs): Processing Sequences Step-by-Step\n",
    "\n",
    "\n",
    "\n",
    " RNNs were the standard for sequence modeling before transformers.\n",
    "\n",
    " * **Core Idea:** Process a sequence one element at a time, maintaining a **hidden state** $z_n$ that summarizes the information seen so far.\n",
    "\n",
    " * The network takes the current input $x_n$ and the previous hidden state $z_{n-1}$ to produce an output $y_n$ and the next hidden state $z_n$. Weights are shared across time steps.\n",
    "\n",
    "\n",
    "\n",
    "     <img src=\"image/Figure_13.png\" width=\"450px\"/>\n",
    "\n",
    "\n",
    "\n",
    "     *Figure 13: A general Recurrent Neural Network (RNN).*\n",
    "\n",
    "\n",
    "\n",
    " * **Example: Machine Translation (Encoder-Decoder RNN)**\n",
    "\n",
    "     * **Encoder:** An RNN reads the input sentence (e.g., English) and compresses its meaning into a final hidden state vector $z^*$.\n",
    "\n",
    "     * **Decoder:** Another RNN takes $z^*$ and generates the output sentence (e.g., Dutch) word by word.\n",
    "\n",
    "\n",
    "\n",
    "     <img src=\"image/Figure_14.png\" width=\"550px\"/>\n",
    "\n",
    "\n",
    "\n",
    "     *Figure 14: RNN for language translation (encoder-decoder architecture).*\n",
    "\n",
    "\n",
    "\n",
    " * **Training RNNs:** Using an algorithm called Backpropagation Through Time (BPTT).\n",
    "\n",
    " * **Problems with RNNs:**\n",
    "\n",
    "     1.  **Vanishing/Exploding Gradients:** During BPTT for long sequences, gradients can become extremely small (vanish) or large (explode), making training difficult.\n",
    "\n",
    "     2.  **Long-Range Dependencies:** Standard RNNs struggle to capture relationships between distant elements in a sequence.\n",
    "\n",
    "     3.  **Information Bottleneck:** In encoder-decoder models, the entire input sequence's meaning must be squeezed into a single fixed-size vector $z^*$, which is a major limitation for long sequences.\n",
    "\n",
    "     4.  **Sequential Computation:** They process sequences step-by-step, which limits parallelization on modern hardware like GPUs.\n",
    "\n",
    " * **LSTMs and GRUs:** More advanced RNN variants (Long Short-Term Memory and Gated Recurrent Units) were developed to mitigate these issues but still have limitations with very long dependencies and are often slower to train.\n",
    "\n",
    " * **Transformers were designed to overcome these RNN limitations**.\n",
    "\n",
    "\n",
    "\n",
    " ---\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### <a id=\"reference\"></a>Reference\n",
    "\n",
    "\n",
    "\n",
    " Bishop, C. M. (2024). *Deep Learning: Foundations and Concepts*. Springer. (Chapter 12: Transformers)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AB2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
