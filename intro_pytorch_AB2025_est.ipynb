{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4215c3e6",
   "metadata": {},
   "source": [
    "```\n",
    "Introduction to PyTorch\n",
    "├── 1 What is PyTorch\n",
    "│   └── 1.1 The three core components of PyTorch\n",
    "├── 1.2 Defining deep learning\n",
    "├── 1.3 Installing PyTorch\n",
    "├── 2 Understanding tensors\n",
    "│   ├── 2.1 Scalars, vectors, matrices, and tensors\n",
    "│   ├── 2.2 Tensor data types\n",
    "│   └── 2.3 Common PyTorch tensor operations\n",
    "├── 3 Seeing models as computation graphs\n",
    "├── 4 Automatic differentiation made easy\n",
    "├── 5. Implementing multilayer neural networks\n",
    "├── 6 Setting up efficient data loaders\n",
    "├── 7. A typical training loop\n",
    "└── 8 Saving and loading models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7fc8a0-280c-4979-b0c7-fc3a99b3b785",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9762ec62",
   "metadata": {},
   "source": [
    "This lesson is designed to equip readers with the necessary skills and knowledge to implement neural networks and apply deep learning in practice. PyTorch, a popular Python-based deep learning library, serves as the primary tool. The lesson guides users through setting up a deep learning environment with PyTorch and GPU support.\n",
    "\n",
    "It covers essential concepts such as tensors and their usage within PyTorch. The material also explores PyTorch's automatic differentiation engine, which enables efficient implementation of backpropagation for neural network training.\n",
    "\n",
    "Designed as an introductory resource for newcomers to PyTorch, this lesson explains foundational concepts without providing exhaustive library coverage. The focus remains on core PyTorch fundamentals required for implementing neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf13d2-8fc2-483e-88cc-6b4310221e68",
   "metadata": {},
   "source": [
    "## 1 What is PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d65d83f",
   "metadata": {},
   "source": [
    "*   **PyTorch:**\n",
    "    *   Open-source Python deep learning library ([pytorch.org](https://pytorch.org/)).\n",
    "    *   Widely used in research since 2019 ([Papers With Code](https://paperswithcode.com/trends)).\n",
    "    *   Growing adoption: ~40% of respondents in Kaggle survey ([Kaggle Survey 2022](https://www.kaggle.com/c/kaggle-survey-2022)).\n",
    "*   **Key Advantages:**\n",
    "    *   User-friendly interface and efficient.\n",
    "    *   Flexible for advanced customization.\n",
    "    *   Balances usability and features for researchers and practitioners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb9d8b",
   "metadata": {},
   "source": [
    "### 1.1 The three core components of PyTorch\n",
    "\n",
    "PyTorch is a relatively comprehensive library, and one way to approach it is to focus on its three broad components, summarized in figure 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ba1ab-3306-4965-8618-1ed5f24fb939",
   "metadata": {},
   "source": [
    "<img src=\"images2/Figure1.webp\" width=\"600px\">\n",
    "\n",
    "> **Figure 1** PyTorch’s three main components include a tensor library as a\\\n",
    " fundamental building block for computing, automatic differentiation for model\\\n",
    " optimization, and deep learning utility functions, making it easier to implement\\\n",
    " and train deep neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbd3da",
   "metadata": {},
   "source": [
    "*   **Tensor Library:**\n",
    "    *   Extends NumPy with GPU acceleration.\n",
    "    *   Seamless CPU/GPU switching.\n",
    "*   **Autograd Engine:**\n",
    "    *   Automatic differentiation for tensor operations.\n",
    "    *   Simplifies backpropagation and model optimization.\n",
    "*   **Deep Learning Library:**\n",
    "    *   Modular, flexible, and efficient building blocks.\n",
    "    *   Pretrained models, loss functions, and optimizers.\n",
    "    *   Supports a wide range of deep learning models.\n",
    "    *   Caters to researchers and developers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebc569d",
   "metadata": {},
   "source": [
    "## 1.2 Defining deep learning\n",
    "Confused about AI, Machine Learning, Deep Learning, and LLMs? Let's clarify:\n",
    " \n",
    "*   **AI (Artificial Intelligence):**\n",
    "    *   Creating computer systems to perform tasks requiring human intelligence.\n",
    "    *   Examples: Natural language understanding, pattern recognition, decision-making.\n",
    "    *   Still far from achieving general intelligence.\n",
    "\n",
    "*   **Machine Learning (ML):**\n",
    "    *   A subfield of AI (see Figure 2).\n",
    "    *   Focuses on developing learning algorithms.\n",
    "    *   Enables computers to learn from data and make predictions without explicit programming.\n",
    "    *   Involves algorithms that:\n",
    "        *   Identify patterns.\n",
    "        *   Learn from data.\n",
    "        *   Improve with more data and feedback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c0555-88f6-4515-8c99-aa56b0769d54",
   "metadata": {},
   "source": [
    "<img src=\"images2/Figure2.webp\" width=\"500px\">\n",
    "\n",
    "> **Figure 2** Deep learning is a subcategory of machine learning focused on implementing deep neural networks. Machine learning is a subcategory of AI that is concerned with algorithms that learn from data. AI is the broader concept of machines being able to perform tasks that typically require human intelligence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbab57",
   "metadata": {},
   "source": [
    "**Machine Learning (ML):**\n",
    "*   Integral to AI evolution, powering advancements like LLMs.\n",
    "*   Behind technologies:\n",
    "    *   Recommendation systems.\n",
    "    *   Spam filtering.\n",
    "    *   Voice recognition.\n",
    "    *   Self-driving cars.\n",
    "*   Enhances AI capabilities: Adapts to new inputs, moving beyond rule-based systems.\n",
    "\n",
    "**Deep Learning (DL):**\n",
    "*   A subcategory of ML using deep neural networks.\n",
    "*   Inspired by the human brain's neuron interconnections.\n",
    "*   \"Deep\" refers to multiple hidden layers modeling complex relationships.\n",
    "*   Excels at unstructured data (images, audio, text), well-suited for LLMs.\n",
    "\n",
    "**Predictive Modeling Workflow:**\n",
    "*   Supervised learning in ML and DL (summarized in Figure 3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b82961",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images2/Figure3.webp\" width=\"600px\">\n",
    "\n",
    "> **Figure 3** The supervised learning workflow for predictive modeling consists of a training stage where a model is trained on labeled examples in a training dataset. The trained model can then be used to predict the labels of new observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1cc410",
   "metadata": {},
   "source": [
    "**Supervised Learning Workflow:**\n",
    "\n",
    "*   **Training:**\n",
    "    *   Model trained on a dataset of examples and labels.\n",
    "    *   Example: Spam classifier (emails labeled as \"spam\" or \"not spam\").\n",
    "*   **Inference:**\n",
    "    *   Trained model predicts labels for new, unseen data.\n",
    "    *   Example: Classifying new emails as \"spam\" or \"not spam\".\n",
    "*   **Evaluation:**\n",
    "    *   Model evaluation ensures performance criteria are met.\n",
    "\n",
    "**LLMs Training:**\n",
    "\n",
    "*   **Classification:** Similar workflow to Figure 3.\n",
    "*   **Text Generation:**\n",
    "    *   Labels derived from the text itself (next-word prediction).\n",
    "    *   During inference, the LLM generates new text from a prompt.\n",
    "    *   Figure 3 still applies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4aba9",
   "metadata": {},
   "source": [
    "## 1.3 Installing PyTorch\n",
    "PyTorch can be installed just like any other Python library or package. However, since PyTorch is a comprehensive library featuring CPU- and GPU-compatible codes, the installation may require additional explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f0834",
   "metadata": {},
   "source": [
    "For instance, there are two versions of PyTorch: a leaner version that only supports CPU computing and a full version that supports both CPU and GPU computing. If your machine has a CUDA-compatible GPU that can be used for deep learning (ideally, an NVIDIA T4, RTX 2080 Ti, or newer), I recommend installing the GPU version. Regardless, the default command for installing PyTorch in a code terminal is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd619c8",
   "metadata": {},
   "source": [
    "Suppose your computer supports a CUDA-compatible GPU. In that case, it will automatically install the PyTorch version that supports GPU acceleration via CUDA, assuming the Python environment you’re working on has the necessary dependencies (like pip) installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a65005f",
   "metadata": {},
   "source": [
    "To explicitly install the CUDA-compatible version of PyTorch, it’s often better to specify the CUDA you want PyTorch to be compatible with. PyTorch’s official website (https://pytorch.org) provides the commands to install PyTorch with CUDA support for different operating systems. Figure 4 shows a command that will also install PyTorch, as well as the torchvision and torchaudio libraries, which are optional for this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59348a1",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images2/Figure4.webp\" width=\"700px\">\n",
    "\n",
    "> **Figure 4** Access the PyTorch installation recommendation on https://pytorch.org to customize and select the installation command for your system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb44badd",
   "metadata": {},
   "source": [
    "I use PyTorch 2.7.0 for the examples, so I recommend that you use the following command to install the exact version to guarantee compatibility with this lesson:\n",
    "\n",
    "`pip install torch==2.7.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09125b9",
   "metadata": {},
   "source": [
    "However, as mentioned earlier, given your operating system, the installation command might differ slightly from the one shown here. Thus, I recommend that you visit https://pytorch.org and use the installation menu (see figure 4) to select the installation command for your operating system. Remember to replace torch with torch==2.4.0 in the command.\n",
    "\n",
    "To check the version of PyTorch, execute the following code in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5261a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd026f5",
   "metadata": {},
   "source": [
    "After installing PyTorch, you can check whether your installation recognizes your built-in NVIDIA GPU by running the following code in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5588940a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6917b092",
   "metadata": {},
   "source": [
    "If the command returns True, you are all set. If the command returns False, your computer may not have a compatible GPU, or PyTorch does not recognize it. While GPUs are not required for the initial chapters in this book, which are focused on implementing LLMs for educational purposes, they can significantly speed up deep learning–related computations.\n",
    "\n",
    "If you don’t have access to a GPU, there are several cloud computing providers where users can run GPU computations against an hourly cost. A popular Jupyter notebook–like environment is Google Colab (https://colab.research.google.com), which provides time-limited access to GPUs as of this writing. Using the Runtime menu, it is possible to select a GPU, as shown in the screenshot in figure 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c50687",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images2/Figure5.webp\" width=\"700px\">\n",
    "\n",
    "> **Figure 5** Select a GPU device for Google Colab under the Runtime/Change Runtime Type menu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2100cf2e-7459-4ab3-92a8-43e86ab35a9b",
   "metadata": {},
   "source": [
    "## 2 Understanding tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9dd3f2",
   "metadata": {},
   "source": [
    "Tensors: Mathematical objects generalizing vectors and matrices to higher dimensions.\n",
    "\n",
    "*   Characterized by their **order** (or rank) = number of dimensions.\n",
    "*   Examples:\n",
    "    *   Scalar (number): rank 0\n",
    "    *   Vector: rank 1\n",
    "    *   Matrix: rank 2\n",
    "*   See Figure 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c484e87-bfc9-4105-b0a7-1e23b2a72a30",
   "metadata": {},
   "source": [
    "<img src=\"images2/Figure6.webp\" width=\"700px\">\n",
    "\n",
    "> **Figure 6** Tensors with different ranks. Here 0D corresponds to rank 0, 1D to rank 1, and 2D to rank 2. A three-dimensional vector, which consists of three elements, is still a rank 1 tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f26ac",
   "metadata": {},
   "source": [
    "## Tensors: Data Containers and Array Libraries\n",
    "\n",
    "*   **What are Tensors?**\n",
    "    *   From a computational view, tensors are data containers.\n",
    "    *   They hold multidimensional data (each dimension can represent a feature).\n",
    "\n",
    "*   **Tensor Libraries (like PyTorch):**\n",
    "    *   Efficiently create, manipulate, and compute with these arrays.\n",
    "    *   Function essentially as array libraries.\n",
    "\n",
    "*   **PyTorch Tensors vs. NumPy Arrays:**\n",
    "    *   Similar to NumPy arrays.\n",
    "    *   **Key additional features for Deep Learning:**\n",
    "        *   Automatic differentiation engine (simplifies computing gradients - see section 4).\n",
    "        *   Support for GPU computations (speeds up deep neural network training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d7f785-e048-42bc-9182-a556af6bb7f4",
   "metadata": {},
   "source": [
    "### 2.1 Scalars, vectors, matrices, and tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b338b",
   "metadata": {},
   "source": [
    "PyTorch Tensors: Data containers for array-like structures.\n",
    "\n",
    "*   **Scalar:** 0-dimensional tensor (e.g., a number).\n",
    "*   **Vector:** 1-dimensional tensor.\n",
    "*   **Matrix:** 2-dimensional tensor.\n",
    "*   Higher-dimensional tensors: Referred to as 3D tensor, 4D tensor, etc.\n",
    "*   Creation: Use the `torch.tensor` function. See [PyTorch documentation](https://pytorch.org/docs/stable/tensors.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a464d6-cec8-4363-87bd-ea4f900baced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# create a 0D tensor (scalar) from a Python integer\n",
    "tensor0d = torch.tensor(1)\n",
    "\n",
    "# create a 1D tensor (vector) from a Python list\n",
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "\n",
    "# create a 2D tensor from a nested Python list\n",
    "tensor2d = torch.tensor([[1, 2], \n",
    "                         [3, 4]])\n",
    "\n",
    "# create a 3D tensor from a nested Python list\n",
    "tensor3d_1 = torch.tensor([[[1, 2], [3, 4]], \n",
    "                           [[5, 6], [7, 8]]])\n",
    "\n",
    "# create a 3D tensor from NumPy array\n",
    "ary3d = np.array([[[1, 2], [3, 4]], \n",
    "                  [[5, 6], [7, 8]]])\n",
    "tensor3d_2 = torch.tensor(ary3d)  # Copies NumPy array\n",
    "tensor3d_3 = torch.from_numpy(ary3d)  # Shares memory with NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe14c47-499a-4d48-b354-a0e6fd957872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "ary3d[0, 0, 0] = 999\n",
    "print(tensor3d_2) # remains unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3e4c23a-cdba-46f5-a2dc-5fb32bf9117b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[999,   2],\n",
      "         [  3,   4]],\n",
      "\n",
      "        [[  5,   6],\n",
      "         [  7,   8]]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor3d_3) # changes because of memory sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f94723",
   "metadata": {},
   "source": [
    "> **Exercise 1:** Create the following tensors:\n",
    "> 1. A 1D tensor (vector) containing the integers 5, 6, 7.\n",
    "> 2. A 2x3 tensor (matrix) containing floating-point numbers of your choice. Check its `shape` and `dtype`.\n",
    "> 3. A 3D tensor with shape (2, 2, 2) initialized with zeros, using `torch.zeros()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dec48d-2b60-41a2-ac06-fef7e718605a",
   "metadata": {},
   "source": [
    "### 2.2 Tensor data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0854d",
   "metadata": {},
   "source": [
    "PyTorch Tensor Data Types:\n",
    "* Adopts default 64-bit integer data type from Python\n",
    "* Access data type using the `.dtype` attribute\n",
    "* Example: `tensor.dtype`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f48c014-e1a2-4a53-b5c5-125812d4034c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "print(tensor1d.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836d6d5",
   "metadata": {},
   "source": [
    "If we create tensors from Python floats, PyTorch creates tensors with a 32-bit precision by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5429a086-9de2-4ac7-9f14-d087a7507394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "floatvec = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(floatvec.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6fc9de",
   "metadata": {},
   "source": [
    "### Why PyTorch Uses 32-bit Precision by Default\n",
    "\n",
    "* **Balance between precision and efficiency**\n",
    "  * 32-bit float offers sufficient precision for most deep learning tasks\n",
    "  * Consumes less memory than 64-bit floating-point numbers\n",
    "  * Requires fewer computational resources\n",
    "\n",
    "* **GPU Optimization**\n",
    "  * GPU architectures are specifically optimized for 32-bit computations\n",
    "  * Significantly speeds up model training and inference\n",
    "\n",
    "### Changing Tensor Precision\n",
    "\n",
    "You can change a tensor's precision using the `.to()` method:\n",
    "* Example: Convert a 64-bit integer tensor to a 32-bit float tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9a438d1-49bb-481c-8442-7cc2bb3dd4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "floatvec = tensor1d.to(torch.float32)\n",
    "print(floatvec.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec8863",
   "metadata": {},
   "source": [
    "For more information about different tensor data types available in PyTorch, check the official documentation at https://pytorch.org/docs/stable/tensors.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020deb5-aa02-4524-b311-c010f4ad27ff",
   "metadata": {},
   "source": [
    "### 2.3 Common PyTorch tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179238b",
   "metadata": {},
   "source": [
    "Comprehensive coverage of all the different PyTorch tensor operations and commands is outside the scope of this lesson. However, I will briefly describe relevant operations as we introduce them throughout the class.\n",
    "\n",
    "We have already introduced the `torch.tensor()` function to create new tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c02095f2-8a48-4953-b3c9-5313d4362ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d = torch.tensor([[1, 2, 3], \n",
    "                         [4, 5, 6]])\n",
    "tensor2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bec105",
   "metadata": {},
   "source": [
    "In addition, the `.shape` attribute allows us to access the shape of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f33e1d45-5b2c-4afe-b4b2-66ac4099fd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e6dac",
   "metadata": {},
   "source": [
    "As you can see, `.shape` returns `[2, 3]`, meaning the tensor has two rows and three columns. To reshape the tensor into a 3 × 2 tensor, we can use the `.reshape` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a4129d-f870-4e03-9c32-cd8521cb83fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d.reshape(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776e7af",
   "metadata": {},
   "source": [
    "However, note that the more common command for reshaping tensors in PyTorch is `.view()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "589ac0a7-adc7-41f3-b721-155f580e9369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d.view(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c13bd",
   "metadata": {},
   "source": [
    "#### PyTorch Syntax Options\n",
    "\n",
    "* PyTorch often provides multiple syntax options for the same operation\n",
    "  * Initially followed Lua Torch conventions\n",
    "  * Later added NumPy-like syntax by popular demand\n",
    "\n",
    "* `.view()` vs `.reshape()`:\n",
    "  * **`.view()`**: Requires contiguous data (elements stored sequentially in memory); fails otherwise\n",
    "  * **`.reshape()`**: Works with any data; copies if necessary to achieve desired shape\n",
    "\n",
    "#### Tensor Transposition\n",
    "\n",
    "* Use `.T` to transpose a tensor (flip across diagonal)\n",
    "* Transposition ≠ Reshaping\n",
    "  * Transposition rearranges elements while preserving dimensions\n",
    "  * See example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "344e307f-ba5d-4f9a-a791-2c75a3d1417e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc7d9b",
   "metadata": {},
   "source": [
    "Lastly, the common way to multiply two matrices in PyTorch is the `.matmul` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19a75030-6a41-4ca8-9aae-c507ae79225c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 32],\n",
       "        [32, 77]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d.matmul(tensor2d.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb62f56",
   "metadata": {},
   "source": [
    "However, we can also adopt the `@` operator, which accomplishes the same thing more compactly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7c950bc-d640-4203-b210-3ac8932fe4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 32],\n",
       "        [32, 77]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d @ tensor2d.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c9287a",
   "metadata": {},
   "source": [
    "For readers who’d like to browse through all the different tensor operations available in PyTorch (we won’t need most of these), I recommend checking out the official documentation at https://pytorch.org/docs/stable/tensors.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f904e",
   "metadata": {},
   "source": [
    "> **Exercise 2:** Given the following tensors:\n",
    "> ```python\n",
    "> t1 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "> t2 = torch.tensor([[5., 6.], [7., 8.]])\n",
    "> ```\n",
    "> Perform these operations:\n",
    "> 1. Reshape `t1` into a 4x1 tensor.\n",
    "> 2. Transpose `t2` using `.T`.\n",
    "> 3. Calculate the element-wise sum of `t1` and `t2`.\n",
    "> 4. Calculate the matrix multiplication of `t1` and `t2` using the `@` operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c15bdeb-78e2-4870-8a4f-a9f591666f38",
   "metadata": {},
   "source": [
    "## 3 Seeing models as computation graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13926d6c",
   "metadata": {},
   "source": [
    "#### PyTorch's Automatic Differentiation (Autograd)\n",
    "\n",
    "* **Autograd**: PyTorch's engine that automatically computes gradients in dynamic computational graphs\n",
    "* **Key features**:\n",
    "  * Tracks operations on tensors\n",
    "  * Calculates gradients efficiently\n",
    "  * Enables backpropagation for neural network training\n",
    "\n",
    "#### Computational Graphs\n",
    "\n",
    "* **Definition**: Directed graphs representing mathematical expressions\n",
    "* **In deep learning**: \n",
    "  * Visualize the sequence of calculations in neural networks\n",
    "  * Essential for computing gradients during backpropagation\n",
    "  * Form the foundation of model training algorithms\n",
    "\n",
    "#### Example: Logistic Regression\n",
    "\n",
    "* We'll examine a simple logistic regression classifier (single-layer neural network)\n",
    "* **Characteristics**:\n",
    "  * Produces scores between 0 and 1\n",
    "  * Compares predictions to true class labels (0 or 1)\n",
    "  * Demonstrates how computation flows through a graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22af61e9-0443-4705-94d7-24c21add09c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0852)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0])  # true label\n",
    "x1 = torch.tensor([1.1]) # input feature\n",
    "w1 = torch.tensor([2.2]) # weight parameter\n",
    "b = torch.tensor([0.0])  # bias unit\n",
    "\n",
    "z = x1 * w1 + b          # net input\n",
    "a = torch.sigmoid(z)     # activation & output\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e5566a",
   "metadata": {},
   "source": [
    "If not all components in the preceding code make sense to you, don’t worry. The point of this example is not to implement a logistic regression classifier but rather to illustrate how we can think of a sequence of computations as a computation graph, as shown in figure 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e16c3-07df-44b6-9106-a42fb24452a9",
   "metadata": {},
   "source": [
    "<img src=\"images2/Figure7.webp\" width=\"700px\">\n",
    "\n",
    "> **Figure 7:** A logistic regression forward pass as a computation graph. The input feature $x_1$ is multiplied by a model weight $w_1$ and passed through an activation function $s$ after adding the bias. The loss is computed by comparing the model output $a$ with a given label $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84736a19",
   "metadata": {},
   "source": [
    "*   PyTorch builds a computation graph in the background.\n",
    "*   This graph is used to calculate gradients of the loss function w.r.t. model parameters (e.g., `w1` and `b`).\n",
    "*   Gradients are essential for training the model.\n",
    "    *   Used in optimization algorithms like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca5ca1",
   "metadata": {},
   "source": [
    "> **Note:** A loss function is a mathematical measure used to quantify the difference between the predicted output of a model and the actual target values, guiding the optimization process to improve model accuracy by minimizing this difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9424f26-2bac-47e7-b834-92ece802247c",
   "metadata": {},
   "source": [
    "## 4 Automatic differentiation made easy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c8522",
   "metadata": {},
   "source": [
    "*   PyTorch builds a computational graph automatically when `requires_grad=True` for a tensor.\n",
    "*   This is essential for computing gradients.\n",
    "*   Gradients are crucial for training neural networks using backpropagation.\n",
    "    *   Backpropagation is an application of the chain rule (see [Figure 8](images2/Figure8.webp))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa2ee4-6f1d-448d-8707-67cd5278233c",
   "metadata": {},
   "source": [
    "<img src=\"images2/Figure8.webp\" width=\"700px\">\n",
    "\n",
    "> **Figure 8:** The most common way of computing the loss gradients in a computation graph involves applying the chain rule from right to left, also called reverse-model automatic differentiation or backpropagation. We start from the output layer (or the loss itself) and work backward through the network to the input layer. We do this to compute the gradient of the loss with respect to each parameter (weights and biases) in the network, which informs how we update these parameters during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b3d97",
   "metadata": {},
   "source": [
    "### Partial derivatives and gradients\n",
    "*   **Partial Derivatives:** Measure the rate at which a function changes with respect to one of its variables.\n",
    "*   **Gradients:** A vector containing all partial derivatives of a multivariate function.\n",
    "\n",
    "*   **Calculus Concepts (Simplified):**\n",
    "    *   Don't worry if you're unfamiliar with partial derivatives, gradients, or the chain rule.\n",
    "    *   **Chain Rule (High-level):** A method to compute gradients of a loss function w.r.t. model parameters in a computation graph.\n",
    "        *   Provides information to update parameters to minimize the loss (e.g., using gradient descent).\n",
    "        *   Training loop implementation revisited in Section 7.\n",
    "\n",
    "*   **PyTorch Autograd Engine:**\n",
    "    *   The second core component of PyTorch.\n",
    "    *   Constructs a computational graph in the background by tracking every operation on tensors.\n",
    "    *   Enables automatic gradient computation.\n",
    "    *   Example: Use the `torch.autograd.grad` function to compute gradients (e.g., `grad(loss, w1)`), as shown in the following listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebf5cef7-48d6-4d2a-8ab0-0fb10bdd7d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "z = x1 * w1 + b \n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "# retain_graph=True tells PyTorch to keep the computation graph in memory\n",
    "# after computing gradients. This allows us to compute multiple gradients\n",
    "# from the same graph. Without it, the graph would be freed after the first\n",
    "# gradient computation, making the second one impossible.\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b = grad(loss, b, retain_graph=True)\n",
    "\n",
    "print(grad_L_w1)\n",
    "print(grad_L_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cbed22",
   "metadata": {},
   "source": [
    "Using `grad` manually:\n",
    "*   Useful for experimentation, debugging, and demonstrating concepts.\n",
    "\n",
    "In practice, PyTorch automates this:\n",
    "*   Call `.backward()` on the loss tensor.\n",
    "*   PyTorch computes gradients for all leaf nodes (tensors with `requires_grad=True`).\n",
    "*   Gradients are stored in the `.grad` attribute of these tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93c5875d-f6b2-492c-b5ef-7e132f93a4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18642471",
   "metadata": {},
   "source": [
    "#### Autograd Summary:\n",
    "\n",
    "*   Autograd explained using calculus concepts.\n",
    "*   Don't be overwhelmed by the math details.\n",
    "*   Key takeaway: PyTorch handles calculus automatically.\n",
    "*   This is done via the `.backward()` method.\n",
    "*   No need to compute derivatives or gradients by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bef84",
   "metadata": {},
   "source": [
    "> **Exercise 3:** \n",
    "> 1. Define `x = torch.tensor(2.0, requires_grad=True)`.\n",
    "> 2. Calculate `y = 3*x**2 + 5`.\n",
    "> 3. Compute the gradient of `y` with respect to `x` (dy/dx) using `y.backward()`. Print the result stored in `x.grad`. (Expected result: 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53bdd7d-44e6-40ab-8a5a-4eef74ef35dc",
   "metadata": {},
   "source": [
    "## 5. Implementing multilayer neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ce736",
   "metadata": {},
   "source": [
    "Focus: PyTorch for Deep Neural Networks\n",
    "\n",
    "*   Implementing NNs in PyTorch\n",
    "*   Concrete example: Multilayer Perceptron (Fully Connected NN)\n",
    "*   Illustrated in Figure 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cb9787-2bc8-4379-9e8c-a3401ac63c51",
   "metadata": {},
   "source": [
    "<img src=\"images2/Figure9.webp\" width=\"600px\">\n",
    "\n",
    "> **Figure 9:** A multilayer perceptron with two hidden layers. Each node represents a unit in the respective layer. For illustration purposes, each layer has a very small number of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d86679",
   "metadata": {},
   "source": [
    "#### Implementing Neural Networks in PyTorch:\n",
    "\n",
    "*   **Subclass `torch.nn.Module`**: Define custom network architectures by inheriting from this base class.\n",
    "*   **`torch.nn.Module` Benefits**:\n",
    "    *   Provides essential functionality for building and training models.\n",
    "    *   Encapsulates layers and operations.\n",
    "    *   Automatically tracks model parameters.\n",
    "*   **Define Layers in `__init__`**: Set up the network's layers (e.g., Linear, ReLU) in the constructor.\n",
    "*   **Define Forward Pass in `forward`**: Specify how data flows through the layers to create the computation graph.\n",
    "*   **`backward` Method**:\n",
    "    *   Used during training to compute gradients.\n",
    "    *   Typically **do not** need to implement yourself (PyTorch handles this via autograd, see section 7).\n",
    "*   **Example**: The following code illustrates a typical usage with a multilayer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84b749e1-7768-4cfe-94d6-a08c7feff4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "                \n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # output layer\n",
    "            torch.nn.Linear(20, num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8197f0c",
   "metadata": {},
   "source": [
    "We can then instantiate a new neural network object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5b59e2e-1930-456d-93b9-f69263e3adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(50, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c5eaa",
   "metadata": {},
   "source": [
    "Before using this new model object, we can call print on the model to see a summary of its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39d02a21-33e7-4879-8fd2-d6309faf2f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264b7de",
   "metadata": {},
   "source": [
    "*   **Using `torch.nn.Sequential`**:\n",
    "    *   Not strictly required, but simplifies code for sequential layers.\n",
    "    *   Allows calling `self.layers` in `forward` instead of individual layers.\n",
    "\n",
    "*   **Next**: Check the total number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94535738-de02-4c2a-9b44-1cd186fa990a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable model parameters: 2213\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of trainable model parameters:\", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e5f88",
   "metadata": {},
   "source": [
    "*   **Trainable Parameters**:\n",
    "    *   Parameters with `requires_grad=True` are trainable and updated during training (see section 7).\n",
    "    *   In our neural network, these are found in the `torch.nn.Linear` layers.\n",
    "    *   `torch.nn.Linear` layers (also known as feedforward or fully connected) multiply inputs by a weight matrix and add a bias vector.\n",
    "\n",
    "*   **Accessing Parameters**:\n",
    "    *   The first `Linear` layer is at index 0 in `model.layers`.\n",
    "    *   Access its weight matrix using: `model.layers[0].weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c394106-ad71-4ccb-a3c9-9b60af3fa748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1182,  0.0606, -0.1292,  ..., -0.1126,  0.0735, -0.0597],\n",
      "        [-0.0249,  0.0154, -0.0476,  ..., -0.1001, -0.1288,  0.1295],\n",
      "        [ 0.0641,  0.0018, -0.0367,  ..., -0.0990, -0.0424, -0.0043],\n",
      "        ...,\n",
      "        [ 0.0618,  0.0867,  0.1361,  ..., -0.0254,  0.0399,  0.1006],\n",
      "        [ 0.0842, -0.0512, -0.0960,  ..., -0.1091,  0.1242, -0.0428],\n",
      "        [ 0.0518, -0.1390, -0.0923,  ..., -0.0954, -0.0668, -0.0037]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac7a5a",
   "metadata": {},
   "source": [
    "Since this large matrix is not shown in its entirety, let’s use the `.shape` attribute to show its dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cad840",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[0].weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35b370",
   "metadata": {},
   "source": [
    "*   Accessing Bias: Similar to weights, use `model.layers[0].bias`.\n",
    "*   Weight Matrix Details:\n",
    "    *   Dimensions: 30x50.\n",
    "    *   `requires_grad=True` (default): Entries are trainable.\n",
    "*   Weight Initialization:\n",
    "    *   Initialized with small random numbers (differs each time).\n",
    "    *   **Purpose:** Break symmetry during training for effective learning.\n",
    "*   Reproducibility:\n",
    "    *   Make initialization reproducible by seeding the random number generator.\n",
    "    *   Use `torch.manual_seed()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b201882b-9285-4db9-bb63-43afe6a2ff9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = NeuralNetwork(50, 3)\n",
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b857a1f6",
   "metadata": {},
   "source": [
    "Now that we have spent some time inspecting the NeuralNetwork instance, let’s briefly see how it’s used via the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57eadbae-90fe-43a3-a33f-c23a095ba42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "X = torch.rand((1, 50))\n",
    "out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5bec1",
   "metadata": {},
   "source": [
    "#### Forward Pass in Neural Networks\n",
    "\n",
    "- **Input Generation**: We created a random 50-dimensional feature vector as input\n",
    "- **Model Execution**: Calling `model(X)` automatically executes the forward pass\n",
    "- **Forward Pass Definition**: Process of calculating outputs by passing inputs through all network layers\n",
    "- **Output Interpretation**: The three returned values are scores for each output node\n",
    "\n",
    "#### Understanding `grad_fn`\n",
    "\n",
    "- **Purpose**: Indicates the last operation used in computational graph\n",
    "- **Example**: `grad_fn=<AddmmBackward0>` shows tensor was created via:\n",
    "  - Matrix multiplication (mm)\n",
    "  - Addition operation (Add)\n",
    "- **Usage**: PyTorch uses this information during backpropagation\n",
    "\n",
    "#### Optimizing Inference\n",
    "\n",
    "- **Problem**: Tracking gradients during inference is wasteful\n",
    "- **Solution**: Use `torch.no_grad()` context manager\n",
    "- **Benefits**:\n",
    "  - Prevents unnecessary computation\n",
    "  - Reduces memory consumption\n",
    "  - Improves inference speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48d720cb-ef73-4b7b-92e0-8198a072defd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21a8d9",
   "metadata": {},
   "source": [
    "#### PyTorch Model Outputs\n",
    "\n",
    "* **Default Behavior**: Models return raw logits (last layer outputs without activation)\n",
    "* **Reason**: PyTorch loss functions (like `CrossEntropyLoss`) internally combine:\n",
    "  * Softmax (or sigmoid for binary classification)\n",
    "  * Negative log-likelihood loss\n",
    "* **Benefits**:\n",
    "  * Numerical stability\n",
    "  * Computational efficiency\n",
    "* **Important**: To get probability distributions, explicitly apply softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10df3640-83c3-4061-a74d-08f07a5cc6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3113, 0.3934, 0.2952]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = torch.softmax(model(X), dim=1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94fc3f0",
   "metadata": {},
   "source": [
    "* **Output Interpretation (Post-Softmax):**\n",
    "    * Values represent class-membership probabilities.\n",
    "    * Probabilities sum to 1.\n",
    "* **Observation (Untrained Model):**\n",
    "    * Probabilities are roughly equal for the random input.\n",
    "    * This is expected for a randomly initialized model before training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3134d2",
   "metadata": {},
   "source": [
    "> **Exercise 4:** Modify the `NeuralNetwork` class:\n",
    "> 1. Change the number of units in the first hidden layer from 30 to 10.\n",
    "> 2. Instantiate the modified network with `num_inputs=50` and `num_outputs=3`.\n",
    "> 3. Print the modified model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19858180-0f26-43a8-b2c3-7ed40abf9f85",
   "metadata": {},
   "source": [
    "## 6 Setting up efficient data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ecdf90",
   "metadata": {},
   "source": [
    "#### Efficient Data Loaders in PyTorch\n",
    "\n",
    "*   Crucial for training deep learning models.\n",
    "*   Iterated over during training.\n",
    "*   Overall idea behind data loading in PyTorch is illustrated in [Figure 10](images2/Figure10.webp)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f98d8fc-5618-47a2-bc72-153818972a24",
   "metadata": {},
   "source": [
    "<img src=\"images2/Figure10.webp\" width=\"700px\">\n",
    "\n",
    "> **Figure 10:** PyTorch implements a `Dataset` and a `DataLoader` class. The `Dataset` class is used to instantiate objects that define how each data record is loaded. The `DataLoader` handles how the data is shuffled and assembled into batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9406661",
   "metadata": {},
   "source": [
    "*   **Implementing Custom Dataset & Data Loaders**\n",
    "    *   Following [Figure 10](images2/Figure10.webp), we'll implement a custom `Dataset` class.\n",
    "    *   This class will be used to create training and test datasets.\n",
    "    *   These datasets will then feed into data loaders.\n",
    "\n",
    "*   **Creating a Simple Toy Dataset**\n",
    "    *   Let's start by creating a toy dataset for demonstration.\n",
    "    *   **Training Data:**\n",
    "        *   5 examples, 2 features each.\n",
    "        *   Labels: 3 for class 0, 2 for class 1.\n",
    "    *   **Test Data:**\n",
    "        *   2 examples, 2 features each.\n",
    "        *   Labels: 1 for class 0, 1 for class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9dc2745-8be8-4344-80ef-325f02cda7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88283948-5fca-461a-98a1-788b6be191d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6],\n",
    "])\n",
    "\n",
    "y_test = torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d883da",
   "metadata": {},
   "source": [
    "> **Note:** PyTorch requires that class labels start with label 0, and the largest class label value should not exceed the number of output nodes minus 1 (since Python index counting starts at zero). So, if we have class labels 0, 1, 2, 3, and 4, the neural network output layer should consist of five nodes.\n",
    "\n",
    "Next, we create a custom dataset class, ToyDataset, by subclassing from PyTorch’s Dataset parent class, as shown in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edf323e2-1789-41a0-8e44-f3cab16e5f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        one_x = self.features[index]\n",
    "        one_y = self.labels[index]        \n",
    "        return one_x, one_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786d71d0",
   "metadata": {},
   "source": [
    "The custom `ToyDataset` class is primarily used to instantiate a PyTorch `DataLoader`.\n",
    "\n",
    "Before using the `DataLoader`, let's review the structure of the `ToyDataset`:\n",
    "\n",
    "*   **Main Components:** A custom `Dataset` class in PyTorch typically requires three methods:\n",
    "    *   `__init__`: Constructor to set up attributes (like data, file paths, etc.). In our case, we store the `X` and `y` tensors.\n",
    "    *   `__getitem__(index)`: Defines how to retrieve a single data item (features and label) given an `index`. The `DataLoader` will provide this index.\n",
    "    *   `__len__`: Returns the total number of items in the dataset. We use `.shape[0]` on the labels tensor to get the number of rows.\n",
    "\n",
    "*   **Example Usage:**\n",
    "    *   In `__init__`, we assign `X` to `self.features` and `y` to `self.labels`.\n",
    "    *   In `__getitem__`, we return `self.features[index]` and `self.labels[index]`.\n",
    "    *   In `__len__`, we return `self.labels.shape[0]`.\n",
    "\n",
    "*   **Verification:** We can double-check the length of the training dataset using `len(train_ds)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7014705-1fdc-4f72-b892-d8db8bebc331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4f226",
   "metadata": {},
   "source": [
    "Now that we’ve defined a PyTorch Dataset class we can use for our toy dataset,\\\n",
    "we can use PyTorch’s `DataLoader` class to sample from it, as shown in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ec6627a-4c3f-481a-b794-d2131be95eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c9446de-5e4b-44fa-bf9a-a63e2661027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ToyDataset(X_test, y_test)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6014b9",
   "metadata": {},
   "source": [
    "After instantiating the training data loader, we can iterate over it. The iteration over the test_loader works similarly but is omitted for brevity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99d4404c-9884-419f-979c-f659742d86ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]) tensor([1, 0])\n",
      "Batch 2: tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
      "Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\", x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44116bbe",
   "metadata": {},
   "source": [
    "*   **Training Epoch:** Iterating over the training dataset once, visiting each example exactly once.\n",
    "*   **Shuffling:**\n",
    "    *   `shuffle=True` randomizes example order each epoch.\n",
    "    *   `torch.manual_seed(123)` ensures the *first* epoch's shuffle is reproducible.\n",
    "    *   Subsequent epochs will have different shuffling (desired for training stability).\n",
    "*   **Batch Size & `drop_last`:**\n",
    "    *   Batch size 2 with 5 examples results in a last batch of size 1 (5 % 2 != 0).\n",
    "    *   Small last batches can disturb training convergence.\n",
    "    *   Set `drop_last=True` to discard the last incomplete batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d003f7e-7a80-40bf-a7fb-7a0d7dbba9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a791241",
   "metadata": {},
   "source": [
    "Now, iterating over the training loader, we can see that the last batch is omitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4db4d7f4-82da-44a4-b94e-ee04665d9c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
      "Batch 2: tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]) tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx+1}:\", x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab57fbd9",
   "metadata": {},
   "source": [
    "#### Understanding `num_workers` in `DataLoader`\n",
    "\n",
    "*   **`num_workers=0`:**\n",
    "    *   Data loading happens in the **main process**.\n",
    "    *   Can cause a **bottleneck** during training, especially with GPUs.\n",
    "    *   CPU is busy loading data, while the GPU waits idle.\n",
    "\n",
    "*   **`num_workers > 0`:**\n",
    "    *   Launches **multiple worker processes** for parallel data loading.\n",
    "    *   Frees the main process to focus on model training.\n",
    "    *   Better utilizes system resources (CPU and GPU).\n",
    "    *   See [Figure 11](images2/Figure11.webp) for illustration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb03ed57-df38-4ee0-a553-0863450df39b",
   "metadata": {},
   "source": [
    "<img src=\"images2/Figure11.webp\" width=\"700px\">\n",
    "\n",
    "> **Figure 11:** Loading data without multiple workers (setting `num_workers=0`) will create a data loading bottleneck where the model sits idle until the next batch is loaded (left). If multiple workers are enabled, the data loader can queue up the next batch in the background (right)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee069a",
   "metadata": {},
   "source": [
    "#### Considerations for `num_workers > 0`\n",
    "\n",
    "*   **Small Datasets:**\n",
    "    *   May not provide speedup.\n",
    "    *   Can add overhead (spinning up processes).\n",
    "    *   Total training time is already very short.\n",
    "\n",
    "*   **Interactive Environments (e.g., Jupyter):**\n",
    "    *   Can lead to issues (resource sharing, crashes).\n",
    "\n",
    "*   **General Advice:**\n",
    "    *   It's a tradeoff.\n",
    "    *   Adapt to dataset size and computational environment.\n",
    "\n",
    "*   **Practical Tip:**\n",
    "    *   `num_workers=4` often works well for real-world datasets.\n",
    "    *   Optimal setting depends on hardware and `Dataset` implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3763ae6",
   "metadata": {},
   "source": [
    "> **Exercise 5:** \n",
    "> - Create a new `DataLoader` for the `train_ds` with a `batch_size` of 3. Iterate through this new loader and print the shape of the features (`x`) and labels (`y`) for each batch. Observe how the batches are formed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d904ca82-e50f-4f3d-a3ac-fc6ca53dd00e",
   "metadata": {},
   "source": [
    "## 7. A typical training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76210dd3",
   "metadata": {},
   "source": [
    "Let’s now train a neural network on the toy dataset. The following cell shows the training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93f1791a-d887-4fc5-a307-5e5bde9e06f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75\n",
      "Epoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65\n",
      "Epoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44\n",
      "Epoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13\n",
      "Epoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03\n",
      "Epoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "\n",
    "        logits = model(features)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, labels) # Loss function\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        ### LOGGING\n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train/Val Loss: {loss:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "    # Optional model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3ae14",
   "metadata": {},
   "source": [
    "-   **Training Outcome:** Loss reached 0 after 3 epochs, indicating convergence on the training set.\n",
    "-   **Model Initialization:** Model initialized with 2 inputs and 2 outputs, matching the toy dataset's features and class labels.\n",
    "-   **Optimizer:** Used Stochastic Gradient Descent (SGD) with a learning rate (`lr`) of 0.5.\n",
    "-   **Hyperparameters:**\n",
    "    -   Learning rate (`lr`) and number of epochs are tunable hyperparameters.\n",
    "    -   Choose them by experimenting and observing loss convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2471dc",
   "metadata": {},
   "source": [
    "> **Note:** An optimizer, such as AdamW, is a tool used in training deep neural networks to adjust the model's weights to minimize loss, improve regularization, and enhance generalization by dynamically adjusting learning rates and penalizing larger weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db293da",
   "metadata": {},
   "source": [
    "Here's a summary of key concepts from the training loop:\n",
    "\n",
    "*   **Validation Dataset:**\n",
    "    *   Used for finding optimal hyperparameter settings.\n",
    "    *   Similar to a test set, but used *multiple times* (test set used only *once* to avoid evaluation bias).\n",
    "\n",
    "*   **Model Modes (`model.train()`, `model.eval()`):**\n",
    "    *   Set the model to training or evaluation mode.\n",
    "    *   Crucial for layers with different behavior during training vs. inference (e.g., Dropout, Batch Normalization).\n",
    "    *   Redundant in simple models without such layers, but *best practice* to include for code reusability and future model changes.\n",
    "\n",
    "*   **Loss Calculation & Optimization:**\n",
    "    *   Pass logits directly to `F.cross_entropy`. Softmax is applied *internally* for efficiency and numerical stability.\n",
    "    *   `loss.backward()`: Calculates gradients for all parameters in the computation graph.\n",
    "    *   `optimizer.step()`: Updates model parameters using the calculated gradients (e.g., for SGD: `param = param - learning_rate * grad`).\n",
    "    *   Remember `optimizer.zero_grad()` before `loss.backward()` to prevent gradient accumulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8403d2c",
   "metadata": {},
   "source": [
    "> **Note:** To prevent undesired gradient accumulation, it is important to include an `optimizer.zero_grad()` call in each update round to reset the gradients to 0. Otherwise, the gradients will accumulate, which may be undesired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2086d",
   "metadata": {},
   "source": [
    "After we have trained the model, we can use it to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00dcf57f-6a7e-4af7-aa5a-df2cb0866fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8569, -4.1618],\n",
      "        [ 2.5382, -3.7548],\n",
      "        [ 2.0944, -3.1820],\n",
      "        [-1.4814,  1.4816],\n",
      "        [-1.7176,  1.7342]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_train)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afecb075",
   "metadata": {},
   "source": [
    "#### Interpreting Model Output & Making Predictions**\n",
    "\n",
    "*   **Output Interpretation:**\n",
    "    *   The model's output (logits or probabilities after softmax) represents class scores/probabilities for each input example.\n",
    "    *   Example: For the first input, `[0.9991, 0.0009]` indicates a 99.91% probability for class 0 and 0.09% for class 1.\n",
    "    *   (`torch.set_printoptions` can be used for better output readability).\n",
    "*   **Getting Class Labels:**\n",
    "    *   Use `torch.argmax` to find the index (class label) with the highest score/probability.\n",
    "    *   `torch.argmax(..., dim=1)`: Finds the max index along each **row** (correct for getting predictions per example).\n",
    "    *   `torch.argmax(..., dim=0)`: Finds the max index along each **column**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19be7390-18b8-43f9-9841-d7fb1919f6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.9991,     0.0009],\n",
      "        [    0.9982,     0.0018],\n",
      "        [    0.9949,     0.0051],\n",
      "        [    0.0491,     0.9509],\n",
      "        [    0.0307,     0.9693]])\n",
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "probas = torch.softmax(outputs, dim=1)\n",
    "print(probas)\n",
    "\n",
    "predictions = torch.argmax(probas, dim=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8be390",
   "metadata": {},
   "source": [
    "Note that it is unnecessary to compute softmax probabilities to obtain the class labels.\\\n",
    "We could also apply the argmax function to the logits (outputs) directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07e7e530-f8d3-429c-9f5e-cf8078078c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.argmax(outputs, dim=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a30c93a",
   "metadata": {},
   "source": [
    "#### Verifying Training Predictions\n",
    "\n",
    "*   We have computed the predicted labels for the training dataset.\n",
    "*   For small datasets like this one, we can visually compare predictions to true labels.\n",
    "*   To verify programmatically, use the `==` comparison operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f756f0d-63c8-41b5-a5d8-01baa847e026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions == y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ff238",
   "metadata": {},
   "source": [
    "Using `torch.sum`, we can count the number of correct predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da274bb0-f11c-4c81-a880-7a031fbf2943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(predictions == y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce340a",
   "metadata": {},
   "source": [
    "**Training Accuracy**\n",
    "*   Achieved 100% accuracy (5/5 correct) on the training set.\n",
    "*   Need a general function to compute accuracy for any dataset size.\n",
    "*   Implementing `compute_accuracy` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16d62314-8dee-45b0-8f55-9e5aae2b24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "\n",
    "    model = model.eval()\n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "    \n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        compare = labels == predictions\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return (correct / total_examples).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e879078",
   "metadata": {},
   "source": [
    "**`compute_accuracy` Function**\n",
    "\n",
    "*   Iterates over data loader to compute correct predictions and accuracy.\n",
    "*   Designed for large datasets where models process data in batches due to memory limits.\n",
    "*   Scales to datasets of arbitrary size by processing data in chunks (batches) similar to training.\n",
    "*   Internal logic (converting logits to class labels) is similar to previous steps.\n",
    "\n",
    "We can then apply the function to the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f6c9c17-2a5f-46c0-804b-873f169b729a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70154d01",
   "metadata": {},
   "source": [
    "Similarly, we can apply the function to the test set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "311ed864-e21e-4aac-97c7-c6086caef27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f816f8",
   "metadata": {},
   "source": [
    "> **Exercise 7:** The `compute_accuracy` function was used on the training and test loaders. Now, use the trained `model` to:\n",
    "> 1. Get the raw `logits` for the `X_test` tensor.\n",
    "> 2. Convert the `logits` to predicted class labels (0 or 1) using `torch.argmax`.\n",
    "> 3. Print the predicted labels and compare them to the actual `y_test` labels (`tensor([0, 1])`). Do they match?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5cd469-3a45-4394-944b-3ce543f41dac",
   "metadata": {},
   "source": [
    "## 8 Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84788e2",
   "metadata": {},
   "source": [
    "Now that we’ve trained our model, let’s see how to save it so we can reuse it later.\\\n",
    "Here’s the recommended way of saving and loading models in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b013127d-a2c3-4b04-9fb3-a6a7c88d83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853654ef",
   "metadata": {},
   "source": [
    "*   **`model.state_dict()`**: A Python dictionary mapping each layer to its trainable parameters (weights and biases).\n",
    "*   **Filename**: \"`model.pth`\" is an arbitrary name. Common conventions are `.pth` and `.pt`.\n",
    "*   **Restoring**: After saving, the model can be restored from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2b428c2-3a44-4d91-97c4-8298cf2b51eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork(2, 2) # needs to match the original model exactly\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287e7c1a",
   "metadata": {},
   "source": [
    "*   **Loading the state dictionary:**\n",
    "    *   `torch.load(\"model.pth\")` reads the file and reconstructs the Python dictionary containing the model's parameters.\n",
    "*   **Applying the state dictionary:**\n",
    "    *   `model.load_state_dict()` applies these parameters to the model instance, restoring its learned state.\n",
    "*   **Model Instance Requirement:**\n",
    "    *   An instance of the model (`model = NeuralNetwork(2, 2)`) is needed in memory to apply the loaded parameters.\n",
    "    *   The architecture (`NeuralNetwork(2, 2)`) must exactly match the original saved model.\n",
    "    *   This line is not strictly necessary if loading in the same session where the model was saved, but included for illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e8e66",
   "metadata": {},
   "source": [
    "> **Exercise 8:**\\\n",
    "> You have saved the model state to \"model.pth\" and loaded it back.\n",
    "> 1. Use the newly loaded `model` (the one created just before `load_state_dict`) to compute the accuracy on the `test_loader`.\n",
    "> 2. Verify that the accuracy is the same as the accuracy computed *before* saving the model (which should be 1.0 in this case). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f32c05c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook serves as an introduction to PyTorch, guiding through the essential steps of building and training a simple neural network. Key topics covered include:\n",
    "\n",
    "1.  **Tensor Basics:** Creating and manipulating PyTorch tensors, including operations like indexing, reshaping, transposition, and matrix multiplication.\n",
    "2.  **Neural Network Definition:** Constructing a basic neural network model using `torch.nn.Module`, defining layers like `Linear`.\n",
    "3.  **Dataset and DataLoader:** Preparing data (likely using `TensorDataset`) and loading it efficiently in batches using `DataLoader`.\n",
    "4.  **Training Process:** Setting up a loss function (e.g., `CrossEntropyLoss`) and an optimizer (e.g., `SGD`), followed by a training loop that performs forward pass, calculates loss, performs backward pass (backpropagation), and updates model weights.\n",
    "5.  **Evaluation:** Assessing the model's performance on a test set by calculating accuracy.\n",
    "6.  **Saving and Loading Models:** Persisting the trained model's state (`state_dict`) to a file and loading it back into a new model instance for inference or further training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee170145",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AB2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
