{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convolutional Neural Networks (CNNs)\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Computer Vision](#1-Computer-Vision)  \n",
        "   &nbsp;&nbsp;1.1. [Image Data](#11-Image-Data)  \n",
        "2. [Convolutional Filters](#2-Convolutional-Filters)  \n",
        "   &nbsp;&nbsp;2.1. [Feature Detectors](#21-Feature-Detectors)  \n",
        "   &nbsp;&nbsp;2.2. [Translation Equivariance](#22-Translation-Equivariance)  \n",
        "   &nbsp;&nbsp;2.3. [Padding](#23-Padding)  \n",
        "   &nbsp;&nbsp;2.4. [Strided Convolutions](#24-Strided-Convolutions)  \n",
        "   &nbsp;&nbsp;2.5. [Multi-dimensional Convolutions](#25-Multi-dimensional-Convolutions)  \n",
        "   &nbsp;&nbsp;2.6. [Pooling](#26-Pooling)  \n",
        "   &nbsp;&nbsp;2.7. [Multilayer Convolutions](#27-Multilayer-Convolutions)  \n",
        "   &nbsp;&nbsp;2.8. [Example Network Architectures](#28-Example-Network-Architectures)  \n",
        "3. [Visualizing Trained CNNs](#3-Visualizing-Trained-CNNs)  \n",
        "   &nbsp;&nbsp;3.1. [Visual Cortex](#31-Visual-Cortex)  \n",
        "   &nbsp;&nbsp;3.2. [Visualizing Trained Filters](#32-Visualizing-Trained-Filters)  \n",
        "   &nbsp;&nbsp;3.3. [Saliency Maps](#33-Saliency-Maps)   \n",
        "4. [Object Detection](#4-Object-Detection)  \n",
        "   &nbsp;&nbsp;4.1. [Bounding Boxes](#41-Bounding-Boxes)  \n",
        "   &nbsp;&nbsp;4.2. [Intersection-over-Union (IoU)](#42-Intersection-over-Union-IoU)  \n",
        "   &nbsp;&nbsp;4.3. [Sliding Windows](#43-Sliding-Windows)  \n",
        "   &nbsp;&nbsp;4.4. [Detection Across Scales](#44-Detection-Across-Scales)  \n",
        "   &nbsp;&nbsp;4.5. [Non-Max Suppression (NMS)](#45-Non-Max-Suppression-NMS)  \n",
        "   &nbsp;&nbsp;4.6. [Fast Region CNNs (R-CNN family)](#46-Fast-Region-CNNs-R-CNN-family)  \n",
        "5. [Image Segmentation](#5-Image-Segmentation)  \n",
        "   &nbsp;&nbsp;5.1. [Convolutional Segmentation](#51-Convolutional-Segmentation)  \n",
        "   &nbsp;&nbsp;5.2. [Up-sampling](#52-Up-sampling)  \n",
        "   &nbsp;&nbsp;5.3. [Fully Convolutional Networks (FCNs) & Transpose Convolution](#53-Fully-Convolutional-Networks-FCNs--Transpose-Convolution)  \n",
        "   &nbsp;&nbsp;5.4. [The U-Net Architecture](#54-The-U-Net-Architecture)  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Computer Vision\n",
        "<a id='1-Computer-Vision'></a>\n",
        "\n",
        "* **What is it?** Computer vision is a field focused on enabling computers to \"see\" and interpret images and videos, much like humans do. It's a major area where machine learning, especially deep learning, has made huge impacts.\n",
        "\n",
        "* **Why CNNs?** While traditional methods involved hand-crafting features, CNNs learn these features automatically from data, leading to better performance in many tasks.\n",
        "\n",
        "* **Common Applications in Biomedical Engineering (and beyond):**\n",
        "\n",
        "  * **Image Classification:** E.g., classifying medical scans as benign or malignant.\n",
        "\n",
        "  * **Object Detection:** E.g., identifying cells or anomalies in microscopy images.\n",
        "\n",
        "  * **Image Segmentation:** E.g., outlining organs or tumors in MRI/CT scans.\n",
        "\n",
        "  * **Caption Generation:** Describing medical images.\n",
        "\n",
        "  * **Image Synthesis:** Generating synthetic medical images for training or data augmentation.\n",
        "\n",
        "  * **Inpainting:** Reconstructing missing parts of an image (e.g., removing artifacts).\n",
        "\n",
        "  * **Super-resolution:** Enhancing the resolution of medical images.\n",
        "\n",
        "  * **Depth Prediction:** Estimating 3D structure from 2D medical images.\n",
        "\n",
        "  * **Scene Reconstruction:** Building 3D models from multiple 2D scans.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Image Data\n",
        "<a id='11-Image-Data'></a>\n",
        "\n",
        "* **Structure:** Images are grids of pixels. Each pixel has intensity values (e.g., grey-scale) or color values (e.g., Red, Green, Blue - RGB channels).\n",
        "\n",
        "  * Medical images can also be 3D (voxels, like in MRI) or sequences over time (videos).\n",
        "\n",
        "* **Challenges with Standard Neural Networks:**\n",
        "\n",
        "  * **High Dimensionality:** Many pixels = massive parameters for standard fully connected networks.\n",
        "\n",
        "  * **Ignoring Structure:** Standard networks treat pixels as unstructured data, missing the crucial spatial relationships (nearby pixels are often correlated). Randomly shuffling pixels makes an image unrecognizable.\n",
        "\n",
        "* **CNNs Exploit Image Structure:** CNNs are designed with inductive biases that leverage the structured nature of images, leading to:\n",
        "\n",
        "  * Fewer parameters.\n",
        "\n",
        "  * Better generalization (performing well on unseen data).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Convolutional Filters\n",
        "<a id='2-Convolutional-Filters'></a>\n",
        "\n",
        "* **Motivation:** Standard fully connected networks are impractical for large images due to the huge number of parameters and the need to learn invariances (e.g., an object is the same regardless of its position) from scratch.\n",
        "\n",
        "* **Key Concepts for CNNs:**\n",
        "\n",
        "  * **Hierarchy:** Features are learned in a hierarchy – simple features (edges) combine to form more complex features (shapes, parts of objects), which then form objects.\n",
        "\n",
        "  * **Locality:** Initial feature detection happens in small local regions of an image.\n",
        "\n",
        "  * **Equivariance:** If an input shifts, the feature representation also shifts.\n",
        "\n",
        "  * **Invariance:** The final output (e.g., classification) should be unchanged by certain transformations (like small shifts).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Feature Detectors\n",
        "<a id='21-Feature-Detectors'></a>\n",
        "\n",
        "* **Receptive Field:** A unit in a CNN's first layer looks at a small patch of the input image, called its receptive field.\n",
        "\n",
        "  <img src=\"image/Figure_1_a.png\" width=\"250px\">\n",
        "\n",
        "  *(a) A hidden unit receives input from a 3x3 patch (receptive field).)*\n",
        "\n",
        "* **Kernel/Filter:** The weights for this unit also form a small grid, called a filter or kernel. This filter is designed to detect a specific low-level feature (e.g., an edge, a corner).\n",
        "\n",
        "  <img src=\"image/Figure_1_b.png\" width=\"250px\">\n",
        "\n",
        "  *(b) The weights (kernel) for the hidden unit.)*\n",
        "\n",
        "* **How it works:** The unit calculates a weighted sum of the pixel values in its receptive field (plus a bias) and applies an activation function (commonly ReLU).\n",
        "\n",
        "  * $z = \\text{ReLU}(w^T x + w_0)$\n",
        "\n",
        "* **Feature Detection:** The unit gives a high output when the image patch it's looking at \"matches\" its kernel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Translation Equivariance\n",
        "<a id='22-Translation-Equivariance'></a>\n",
        "\n",
        "* **The Idea:** If a feature (like an eye) appears in one part of an image, the same pattern of pixels in another part should also represent an eye.\n",
        "\n",
        "* **Weight Sharing:** To achieve this, the same filter (set of weights) is applied across different locations in the image. This is the core idea of **convolution**.\n",
        "\n",
        "  * Units in a layer that share the same weights form a **feature map**.\n",
        "\n",
        "  * This drastically reduces the number of parameters to learn.\n",
        "\n",
        "  * Connections are **sparse** (each unit only connects to a local patch).\n",
        "\n",
        "  <img src=\"image/Figure_2.png\" width=\"250px\">\n",
        "\n",
        "  *(Illustration of 1D convolution with shared weights (same color = same weight).)*\n",
        "\n",
        "* **2D Convolution:** For an image $I$ and a filter $K$, the feature map $C$ is given by:\n",
        "\n",
        "  * $C(j,k) = \\sum_l \\sum_m I(j+l, k+m) K(l,m)$ \n",
        "  \n",
        "  (this is technically cross-correlation but called convolution in ML).\n",
        "\n",
        "  <img src=\"image/Figure_3.png\" width=\"550px\">\n",
        "\n",
        "  *(Example of a 3x3 image convolved with a 2x2 filter, producing a 2x2 feature map.)*\n",
        "\n",
        "* **Example: Edge Detection:**\n",
        "\n",
        "  * Hand-crafted filters can detect edges:\n",
        "\n",
        "    * Vertical edge filter:\n",
        "\n",
        "      | -1 |  0 |  1 |\n",
        "      |----|----|----|\n",
        "      | -1 |  0 |  1 |\n",
        "      | -1 |  0 |  1 |\n",
        "\n",
        "    * Horizontal edge filter:\n",
        "\n",
        "      | -1 | -1 | -1 |\n",
        "      |----|----|----|\n",
        "      |  0 |  0 |  0 |\n",
        "      |  1 |  1 |  1 |\n",
        "\n",
        "  <img src=\"image/Figure_4.png\" width=\"550px\">\n",
        "\n",
        "  *(a) Original image, (b) Vertical edge detection, (c) Horizontal edge detection.)*\n",
        "\n",
        "* **Advantages of Convolutional Structure:**\n",
        "\n",
        "  * Sparse connections (fewer weights).\n",
        "\n",
        "  * Shared weights (fewer parameters, less data needed).\n",
        "\n",
        "  * Can be applied to images of different sizes.\n",
        "\n",
        "  * Efficient computation on GPUs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Padding\n",
        "<a id='23-Padding'></a>\n",
        "\n",
        "* **Problem:** Convolution reduces the size of the feature map. (e.g., $J \\times K$ image with $M \\times M$ filter gives $(J-M+1) \\times (K-M+1)$ map).\n",
        "\n",
        "* **Solution:** Add extra pixels (padding) around the image border.\n",
        "\n",
        "  * **Valid Convolution:** No padding ($P=0$).\n",
        "\n",
        "  * **Same Convolution:** Padding chosen so output map has the same size as input ($P=(M-1)/2$ for odd $M$). Commonly padded with zeros.\n",
        "\n",
        "  <img src=\"image/Figure_5.png\" width=\"350px\">\n",
        "\n",
        "  *(A 4x4 image padded to become a 6x6 image.)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Strided Convolutions\n",
        "<a id='24-Strided-Convolutions'></a>\n",
        "\n",
        "* **Purpose:** To reduce the size of feature maps significantly.\n",
        "\n",
        "* **How it works:** Instead of moving the filter one pixel at a time, move it in larger steps (stride $S$).\n",
        "\n",
        "* Output size with stride $S$ and padding $P$:\n",
        "\n",
        "  * $\\lfloor (J+2P-M)/S + 1 \\rfloor \\times \\lfloor (K+2P-M)/S + 1 \\rfloor$ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Multi-dimensional Convolutions\n",
        "<a id='25-Multi-dimensional-Convolutions'></a>\n",
        "\n",
        "* **Color Images:** Have multiple channels (e.g., 3 for RGB).\n",
        "\n",
        "* **Filters for Multiple Input Channels:** The filter also has a depth matching the number of input channels (e.g., $M \\times M \\times C_{IN}$). It processes all input channels simultaneously to produce a single 2D feature map.\n",
        "\n",
        "  <img src=\"image/Figure_6_a.png\" width=\"350px\">\n",
        "\n",
        "  *(a) Filter processing R, G, B channels.)*\n",
        "\n",
        "  <img src=\"image/Figure_6_b.png\" width=\"350px\">\n",
        "\n",
        "  *(b) Kernel visualization for a 3x3x3 filter.)*\n",
        "\n",
        "* **Multiple Output Channels:** To detect various features, a convolutional layer uses multiple filters. Each filter produces its own 2D feature map (output channel).\n",
        "\n",
        "  * If there are $C_{OUT}$ filters, the output is a stack of $C_{OUT}$ feature maps.\n",
        "\n",
        "  * Filter tensor dimensionality: $M \\times M \\times C_{IN} \\times C_{OUT}$.\n",
        "\n",
        "  * Total parameters: $(M^2 C_{IN} + 1) C_{OUT}$ (including bias for each output channel).\n",
        "\n",
        "  <img src=\"image/Figure_7.png\" width=\"350px\">\n",
        "\n",
        "  *(Extending to multiple independent filter channels.)*\n",
        "\n",
        "* **$1 \\times 1$ Convolutions:**\n",
        "\n",
        "  * Filter size is $1 \\times 1 \\times C_{IN}$.\n",
        "\n",
        "  * Used to change the number of channels (depth) without changing spatial dimensions.\n",
        "\n",
        "  * Can reduce dimensionality or introduce more non-linearity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.6 Pooling\n",
        "<a id='26-Pooling'></a>\n",
        "\n",
        "* **Purpose:**\n",
        "\n",
        "  * Introduce some local translation invariance (small shifts in feature location don't change output much).\n",
        "\n",
        "  * Reduce dimensionality (down-sample feature maps).\n",
        "\n",
        "* **How it works:** Applied to outputs of convolutional layers. A pooling unit takes input from a receptive field and applies a fixed function (no learnable parameters).\n",
        "\n",
        "* **Max-Pooling:** Outputs the maximum value from the receptive field.\n",
        "\n",
        "  * Preserves information about feature presence and strength, discards some precise positional info.\n",
        "\n",
        "  <img src=\"image/Figure_8.png\" width=\"450px\">\n",
        "\n",
        "  *(Max-pooling with 2x2 blocks and stride 2.)*\n",
        "\n",
        "* **Average Pooling:** Outputs the average value.\n",
        "\n",
        "* **Stride:** Often, stride = filter size for non-overlapping pooling.\n",
        "\n",
        "* **Other Invariances:** Pooling across channels can help learn other invariances (e.g., rotation if different channels detect features at different orientations).\n",
        "\n",
        "* **Variable Input Sizes:** Pooling can help adapt CNNs to process images of varying sizes by adjusting stride to keep pooled output size constant.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.7 Multilayer Convolutions\n",
        "<a id='27-Multilayer-Convolutions'></a>\n",
        "\n",
        "* **Deep CNNs:** Stack multiple layers of \\[Convolution -> Activation -> Pooling\\].\n",
        "\n",
        "* **Hierarchical Feature Learning:**\n",
        "\n",
        "  * Early layers learn simple features (edges, textures).\n",
        "\n",
        "  * Later layers combine these to learn more complex features (parts of objects, objects).\n",
        "\n",
        "* **Effective Receptive Field:** The receptive field of a unit in a deeper layer, when mapped back to the original input image, is larger than that of units in earlier layers.\n",
        "\n",
        "  <img src=\"image/Figure_9.png\" width=\"450px\">\n",
        "\n",
        "  *(Effective receptive field grows with network depth.)*\n",
        "\n",
        "* **Fully Connected Layers:** Often, the final stages of a CNN for classification are one or more standard fully connected layers. These combine information from across the entire (now down-sampled) feature map.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.8 Example Network Architectures\n",
        "<a id='28-Example-Network-Architectures'></a>\n",
        "\n",
        "* **LeNet (LeCun et al., 1989, 1998):** Early CNN for handwritten digit recognition.\n",
        "\n",
        "* **ImageNet Challenge:** A large dataset (millions of images, thousands of categories) that spurred CNN development.\n",
        "\n",
        "* **AlexNet (Krizhevsky et al., 2012):** Breakthrough CNN that won ImageNet 2012. Used ReLUs, GPUs, dropout.\n",
        "\n",
        "* **VGG-16 (Simonyan & Zisserman, 2014):**\n",
        "\n",
        "  * Known for its simple and uniform architecture (16 learnable layers).\n",
        "\n",
        "  * Uses small $3 \\times 3$ convolutional filters exclusively.\n",
        "\n",
        "  * Stacks multiple convolutional layers before each max-pooling layer.\n",
        "\n",
        "  * Input: $224 \\times 224 \\times 3$ image.\n",
        "\n",
        "  * **Architecture:**\n",
        "    * Sequence of blocks:\n",
        "      * (Conv3x3 → ReLU) layers (multiple per block)\n",
        "      * MaxPool2x2 after each group of conv layers\n",
        "    * After each pooling, number of channels increases:\n",
        "      * 64 → 128 → 256 → 512\n",
        "    * Ends with fully connected (dense) layers\n",
        "\n",
        "  <img src=\"image/Figure_10.png\" width=\"650px\">\n",
        "\n",
        "  *(VGG-16 architecture.)*\n",
        "\n",
        "  * Total parameters: \\~138 million (mostly in fully connected layers).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visualizing Trained CNNs\n",
        "<a id='3-Visualizing-Trained-CNNs'></a>\n",
        "\n",
        "Understanding what a trained CNN has learned.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Visual Cortex\n",
        "<a id='31-Visual-Cortex'></a>\n",
        "\n",
        "* **Inspiration for CNNs:** Early neuroscience research on the mammalian visual cortex.\n",
        "\n",
        "  * **Simple Cells (Hubel & Wiesel, 1959):** Respond to edges at specific orientations and locations. Modeled by **Gabor filters**.\n",
        "\n",
        "  <img src=\"image/Figure_11.png\" width=\"450px\">\n",
        "\n",
        "  *(Examples of Gabor filters.)*\n",
        "\n",
        "  * **Complex Cells:** Respond to more complex stimuli, combine outputs of simple cells, show some invariance to small shifts (like pooling).\n",
        "\n",
        "* **Neocognitron (Fukushima, 1980):** Forerunner of CNNs, inspired by visual cortex, had local receptive fields, shared weights, and pooling, but lacked end-to-end training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Visualizing Trained Filters\n",
        "<a id='32-Visualizing-Trained-Filters'></a>\n",
        "\n",
        "* **First Layer Filters:** Easy to visualize as they operate directly on image patches.\n",
        "\n",
        "  * When trained on natural images (like ImageNet), these filters often resemble Gabor filters (detecting edges and textures) and color blobs.\n",
        "\n",
        "  <img src=\"image/Figure_12.png\" width=\"450px\">\n",
        "\n",
        "  *(Learned filters from AlexNet's first layer. Note similarity to Gabor filters.)*\n",
        "\n",
        "* **Deeper Layer Filters:** Harder to interpret directly as they operate on feature maps from previous layers.\n",
        "\n",
        "  * **Method 1: Finding Maximally Activating Patches:** Present many image patches and see which ones cause the highest activation for a given unit/channel.\n",
        "\n",
        "    * Shows hierarchical structure: Layer 1 (edges), Layer 2 (textures, simple shapes), Layer 3 (object parts), Layer 5 (entire objects).\n",
        "\n",
        "  <img src=\"image/Figure_13.png\" width=\"450px\">\n",
        "\n",
        "  *(Image patches causing strongest activations in different layers of a CNN.)*\n",
        "\n",
        "  * **Method 2: Optimization:** Numerically optimize an input image (starting from noise or a base image) to maximize the activation of a specific unit or class score. Requires regularization to look like natural images.\n",
        "\n",
        "  <img src=\"image/Figure_14.png\" width=\"450px\">\n",
        "\n",
        "  *(Synthetic images generated by maximizing class probability.)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Saliency Maps\n",
        "<a id='33-Saliency-Maps'></a>\n",
        "\n",
        "* **Purpose:** Identify regions of an input image most influential for a particular classification.\n",
        "\n",
        "* **Grad-CAM (Gradient Class Activation Mapping):**\n",
        "\n",
        "  * Focuses on the final convolutional layer (high-level semantics + spatial info).\n",
        "\n",
        "  * Calculates gradients of the class score with respect to feature map activations in this layer.\n",
        "\n",
        "  * These gradients are averaged per channel to get weights ($\\alpha_k$).\n",
        "\n",
        "  * A weighted sum of feature maps ($L = \\sum_k \\alpha_k A^{(k)}$) creates a heatmap showing important regions.\n",
        "\n",
        "  <img src=\"image/Figure_15.png\" width=\"450px\">\n",
        "\n",
        "  *(Saliency maps for 'dog' and 'cat' classes using Grad-CAM.)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Object Detection\n",
        "<a id='4-Object-Detection'></a>\n",
        "\n",
        "Beyond classifying an entire image, finding multiple objects and their locations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Bounding Boxes\n",
        "<a id='41-Bounding-Boxes'></a>\n",
        "\n",
        "* **Definition:** A rectangle tightly enclosing an object in an image.\n",
        "\n",
        "* **Representation:** Typically $(b_x, b_y, b_W, b_H)$ - center coordinates, width, and height.\n",
        "\n",
        "* **CNN for Localization:** A CNN can be trained to output not only class probabilities but also 4 continuous values for bounding box coordinates.\n",
        "\n",
        "  <img src=\"image/Figure_19.png\" width=\"450px\">\n",
        "\n",
        "  *(Image with objects and their bounding boxes.)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Intersection-over-Union (IoU)\n",
        "<a id='42-Intersection-over-Union-IoU'></a>\n",
        "\n",
        "* **Purpose:** A metric to measure the accuracy of a predicted bounding box against a ground truth box.\n",
        "\n",
        "* **Definition:** $IoU = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}$\n",
        "\n",
        "  <img src=\"image/Figure_20.png\" width=\"450px\">\n",
        "\n",
        "  *(Illustration of Intersection-over-Union.)*\n",
        "\n",
        "* **Usage:** Values range from 0 to 1. A prediction is often considered correct if IoU > 0.5. Not typically used directly as a loss function due to difficulty in optimization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Sliding Windows\n",
        "<a id='43-Sliding-Windows'></a>\n",
        "\n",
        "* **Traditional Approach:**\n",
        "\n",
        "  1. Train a classifier on cropped object examples and background examples.\n",
        "\n",
        "  2. Slide this classifier (window) across a new image at all possible locations and scales.\n",
        "\n",
        "  3. Where the classifier gives a high score, an object is detected.\n",
        "\n",
        "* **Drawbacks:** Computationally expensive, especially with deep CNNs and many scales/locations.\n",
        "\n",
        "* **Efficient Implementation in CNNs:**\n",
        "\n",
        "  * Convolutional layers inherently perform a sliding operation.\n",
        "\n",
        "  * A CNN trained on fixed-size inputs can be applied to a larger image by converting its fully connected layers into equivalent convolutional layers. This processes the entire image in one pass, effectively evaluating all window positions simultaneously.\n",
        "\n",
        "  <img src=\"image/Figure_21.png\" width=\"350px\">\n",
        "\n",
        "  *(Overlapping sliding windows lead to redundant computations.)*\n",
        "\n",
        "  <img src=\"image/Figure_22.png\" width=\"450px\">\n",
        "\n",
        "  *(Simple CNN for object detection.)*\n",
        "\n",
        "  <img src=\"image/Figure_23.png\" width=\"450px\">\n",
        "\n",
        "  *(Applying the CNN to a larger image efficiently by extending convolutional/pooling layers.)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Detection Across Scales\n",
        "<a id='44-Detection-Across-Scales'></a>\n",
        "\n",
        "* **Challenge:** Objects appear at different sizes and aspect ratios.\n",
        "\n",
        "* **Method:** Instead of different sized detectors, create multiple scaled/resized versions of the input image. Apply a fixed-size detector to each. Transform detected bounding boxes back to original image coordinates.\n",
        "\n",
        "  <img src=\"image/Figure_24.png\" width=\"450px\">\n",
        "\n",
        "  *(Detecting objects at multiple scales by scaling the image.)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 Non-Max Suppression (NMS)\n",
        "<a id='45-Non-Max-Suppression-NMS'></a>\n",
        "\n",
        "* **Problem:** Sliding window approaches often detect the same object multiple times with slightly different, overlapping bounding boxes.\n",
        "\n",
        "* **Solution (for each class):**\n",
        "\n",
        "  1. Get all predicted bounding boxes with their confidence scores.\n",
        "\n",
        "  2. Discard boxes below a confidence threshold.\n",
        "\n",
        "  3. Select the box with the highest confidence. This is a detection.\n",
        "\n",
        "  4. Discard any other boxes that have a high IoU (e.g., > 0.5) with this selected box.\n",
        "\n",
        "  5. Repeat from step 3 with the remaining boxes until no boxes are left.\n",
        "\n",
        "  <img src=\"image/Figure_25.png\" width=\"450px\">\n",
        "\n",
        "  *(Non-max suppression removes redundant overlapping detections.)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.6 Fast Region CNNs (R-CNN family)\n",
        "<a id='46-Fast-Region-CNNs-R-CNN-family'></a>\n",
        "\n",
        "* **Idea:** Instead of densely scanning, first use a cheaper method (region proposal network or algorithm like Selective Search) to identify a smaller set of candidate regions in the image that likely contain objects. Then, apply the powerful CNN classifier only to these regions.\n",
        "\n",
        "* **Examples:** R-CNN, Fast R-CNN, Faster R-CNN (integrates region proposal into the network for end-to-end training).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Image Segmentation\n",
        "<a id='5-Image-Segmentation'></a>\n",
        "\n",
        "Assigning a class label to *every pixel* in an image.\n",
        "\n",
        "* **Semantic Segmentation:** Divides image into regions corresponding to semantic classes (e.g., \"cell nucleus\", \"cytoplasm\", \"background\" in a microscopy image).\n",
        "\n",
        "  <img src=\"image/Figure_26_a.png\" width=\"450px\"> <img src=\"image/Figure_26_b.png\" width=\"450px\">\n",
        "\n",
        "  *(Example of an image and its semantic segmentation.)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Convolutional Segmentation\n",
        "<a id='51-Convolutional-Segmentation'></a>\n",
        "\n",
        "* **Naive Approach:** Train a CNN to classify the central pixel of an input patch. Slide this patch over the image. Very inefficient due to redundant computations.\n",
        "\n",
        "* **Efficient Approach (Fully Convolutional):** Use a CNN where all layers are convolutional (no fully connected layers that fix input size). If no pooling or striding, output map is same size as input. Still costly if many layers/channels are needed for high accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Up-sampling\n",
        "<a id='52-Up-sampling'></a>\n",
        "\n",
        "* **Problem:** Typical CNNs use pooling/striding, which reduces feature map resolution. For segmentation, we need a full-resolution output.\n",
        "\n",
        "* **Solution:** Add up-sampling layers to an encoder-decoder architecture.\n",
        "\n",
        "  * **Encoder:** Standard CNN that down-samples, extracting features.\n",
        "\n",
        "  * **Decoder:** Up-samples the low-resolution feature maps back to original image resolution, producing the segmentation map.\n",
        "\n",
        "  <img src=\"image/Figure_27.png\" width=\"650px\">\n",
        "\n",
        "  *(Encoder-decoder architecture for segmentation.)*\n",
        "\n",
        "* **Unpooling (analogue of pooling):**\n",
        "\n",
        "  * **Average Unpooling:** Copy input value to all corresponding output units in a block.\n",
        "\n",
        "  <img src=\"image/Figure_28_a.png\" width=\"450px\">\n",
        "\n",
        "  *(Average unpooling.)*\n",
        "\n",
        "  * **Max-Unpooling:** Copy input value to one location in the output block (e.g., top-left or, better, the location of the max from the corresponding max-pooling layer during encoding), others zero.\n",
        "\n",
        "  <img src=\"image/Figure_28_b.png\" width=\"450px\">\n",
        "\n",
        "  *(Max unpooling - basic.)*\n",
        "\n",
        "  <img src=\"image/Figure_29.png\" width=\"650px\">\n",
        "\n",
        "  *(Max unpooling preserving spatial information from encoder's max-pooling indices.)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Fully Convolutional Networks (FCNs) & Transpose Convolution\n",
        "<a id='53-Fully-Convolutional-Networks-FCNs--Transpose-Convolution'></a>\n",
        "\n",
        "* **Transpose Convolution (or \"deconvolution\", \"fractionally strided convolution\"):** A learnable up-sampling layer.\n",
        "\n",
        "  * Each input unit contributes to a patch in the output layer via a filter.\n",
        "\n",
        "  * Stride is effectively < 1 (e.g., moving 1 step in input moves >1 step in output).\n",
        "\n",
        "  * Overlapping contributions are summed or averaged.\n",
        "\n",
        "      <img src=\"image/Figure_30.png\" width=\"250px\">\n",
        "\n",
        "  *(Transpose convolution for up-sampling.)*\n",
        "\n",
        "* **Fully Convolutional Network (Long et al., 2014):** A network using only convolutional layers (including transpose convolutions for up-sampling), no pooling layers that require fixed input sizes. Can process arbitrarily sized images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 The U-Net Architecture\n",
        "<a id='54-The-U-Net-Architecture'></a>\n",
        "\n",
        "* **Key Idea (Ronneberger et al., 2015):** An encoder-decoder architecture with \"skip connections.\"\n",
        "\n",
        "  * **Symmetric Structure:** Down-sampling path (encoder) and up-sampling path (decoder).\n",
        "\n",
        "  * **Skip Connections:** Feature maps from the encoder are concatenated with the corresponding feature maps in the decoder. This provides the up-sampling path with high-resolution spatial information from earlier layers, which helps in precise localization for segmentation.\n",
        "\n",
        "  * Very successful in biomedical image segmentation.\n",
        "\n",
        "  <img src=\"image/Figure_31.png\" width=\"650px\">\n",
        "\n",
        "  *(U-Net architecture with skip connections.)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd9d1fb",
      "metadata": {},
      "source": [
        "### 5.5 Frameworks over PyTorch\n",
        "\n",
        "| Framework/Library                                    | Link                                                                                           | GitHub Stars | Description                                                                                   |\n",
        "|------------------------------------------------------|------------------------------------------------------------------------------------------------|--------------|-----------------------------------------------------------------------------------------------|\n",
        "| MMDetection                                          | [https://github.com/open-mmlab/mmdetection](https://github.com/open-mmlab/mmdetection)         | ~27k         | OpenMMLab’s toolbox for object detection and instance segmentation based on PyTorch.          |\n",
        "| PyTorch Lightning                                    | [https://www.pytorchlightning.ai/](https://www.pytorchlightning.ai/)                           | ~26k         | Lightweight PyTorch wrapper for scalable, reproducible, and organized deep learning research. |\n",
        "| Detectron2                                           | [https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2) | ~26k         | Facebook AI Research's next-gen library for object detection and segmentation in PyTorch.     |\n",
        "| fastai                                               | [https://www.fast.ai/](https://www.fast.ai/)                                                   | ~25k         | High-level library built on PyTorch for rapid prototyping and research, especially in vision, text, and tabular data. |\n",
        "| PyTorch Image Models (timm)                          | [https://github.com/huggingface/pytorch-image-models](https://github.com/huggingface/pytorch-image-models) | ~21k         | Collection of image models, layers, utilities, and pretrained weights for PyTorch.            |\n",
        "| torchvision                                          | [https://pytorch.org/vision/stable/index.html](https://pytorch.org/vision/stable/index.html)   | ~17k         | Official PyTorch vision library with datasets, model architectures, and image transformations.|\n",
        "| Kornia                                               | [https://kornia.org/](https://kornia.org/)                                                     | ~8k          | Computer vision library for PyTorch, with differentiable image processing and geometry ops.   |\n",
        "| segmentation_models.pytorch                          | [https://github.com/qubvel/segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch) | ~6k          | Collection of image segmentation architectures and pretrained weights for PyTorch.            |\n",
        "| MONAI                                                | [https://monai.io/](https://monai.io/)                                                         | ~5k          | Medical Open Network for AI: PyTorch-based framework for deep learning in healthcare imaging. |\n",
        "| TorchIO                                              | [https://torchio.readthedocs.io/](https://torchio.readthedocs.io/)                             | ~2k          | Medical imaging preprocessing and augmentation for deep learning in PyTorch.                  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Reference:**\n",
        "\n",
        "Bishop, C. M. (2024). *Deep Learning: Foundations and Concepts*. Springer. (Chapter 10: Convolutional Neural Networks).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Markdown",
      "language": "markdown",
      "name": "markdown"
    },
    "language_info": {
      "codemirror_mode": "gfm",
      "file_extension": ".md",
      "mimetype": "text/markdown",
      "name": "markdown",
      "pygments_lexer": "markdown"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
