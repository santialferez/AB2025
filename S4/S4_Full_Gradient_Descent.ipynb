{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fcce81c",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimization\n",
    "\n",
    "**Note:** The PyTorch code snippets provided in this document are illustrative examples. They demonstrate how to use specific PyTorch components in a simplified context and may require additional definitions (e.g., model architecture, data loading, complete training loops) to be fully runnable.\n",
    "\n",
    "## 2. Gradient Descent Optimization\n",
    "\n",
    "Finding the best settings (weights and biases) for a neural network is complex. We can't just solve an equation. Instead, we use iterative numerical methods.\n",
    "\n",
    "* **Goal:** Find the weights `w` that minimize an error function `E(w)`.\n",
    "* **Process:**\n",
    "    1.  Start with an initial guess for weights: $w^{(0)}$.\n",
    "    2.  Update weights in steps: $w^{(\\tau)} = w^{(\\tau-1)} + \\Delta w^{(\\tau-1)}$\n",
    "        * $\\tau$ is the iteration step.\n",
    "        * Different algorithms choose $\\Delta w^{(\\tau)}$ differently.\n",
    "* The initial choice of $w^{(0)}$ can affect the solution. It's often good to try multiple random starting points.\n",
    "\n",
    "### 2.1 Use of Gradient Information\n",
    "\n",
    "* The **gradient** ($\\nabla E$) tells us the direction of the steepest increase in error.\n",
    "* Using gradient information is much faster than just evaluating the error function.\n",
    "    * Without gradients, finding the minimum might take $\\mathcal{O}(W^3)$ steps (W = number of parameters).\n",
    "    * With gradients (using backpropagation), it's closer to $\\mathcal{O}(W^2)$ steps.\n",
    "* This efficiency is why gradient-based methods are standard for training neural networks.\n",
    "\n",
    "### 2.2 Batch Gradient Descent\n",
    "\n",
    "* **Idea:** Take a small step in the direction of the negative gradient (steepest decrease in error).\n",
    "* **Update rule:** $w^{(\\tau)} = w^{(\\tau-1)} - \\eta \\nabla E(w^{(\\tau-1)})$\n",
    "    * $\\eta$ (eta) is the **learning rate** (a small positive number controlling step size).\n",
    "* The gradient $\\nabla E$ is calculated using the **entire training dataset** at each step.\n",
    "* This is called a \"batch\" method.\n",
    "* **PyTorch Note:** Full batch gradient descent is less common in deep learning with large datasets. Optimizers like SGD are typically used with mini-batches.\n",
    "\n",
    "### 2.3 Stochastic Gradient Descent (SGD)\n",
    "\n",
    "* Batch methods are slow for very large datasets.\n",
    "* **Idea:** Update weights based on **one data point at a time**.\n",
    "* The error function is a sum of errors for each data point: $E(w) = \\sum_{n=1}^{N} E_n(w)$.\n",
    "* **Update rule:** $w^{(\\tau)} = w^{(\\tau-1)} - \\eta \\nabla E_n(w^{(\\tau-1)})$\n",
    "    * Cycle through the data points.\n",
    "    * One full pass through all data = one **training epoch**.\n",
    "* **Advantages:**\n",
    "    * More efficient for large, redundant datasets.\n",
    "    * Can help escape poor local minima because the error surface for a single point is different from the overall error surface.\n",
    "* PyTorch `optim.SGD` docs: [https://pytorch.org/docs/stable/generated/torch.optim.SGD.html](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) (Used with mini-batches as shown below)\n",
    "\n",
    "### 2.4 Mini-batches\n",
    "\n",
    "* **Problem with SGD:** Gradients from single data points are noisy estimates of the true gradient.\n",
    "* **Idea:** Use a small subset of data points (a **mini-batch**) to calculate the gradient at each step.\n",
    "    * A compromise between batch and pure stochastic gradient descent.\n",
    "* **Mini-batch size:**\n",
    "    * Error in estimating the mean (gradient) decreases with $\\sigma/\\sqrt{N}$ (N = batch size). Diminishing returns for very large batches.\n",
    "    * Often chosen based on hardware efficiency (e.g., powers of 2 like 32, 64, 128).\n",
    "* **Important:** Randomly shuffle data before forming mini-batches to avoid correlations. Reshuffle between epochs.\n",
    "* Often still called \"SGD\" even when using mini-batches.\n",
    "\n",
    "* **PyTorch Example (Using `DataLoader` for Mini-batches with SGD):**\n",
    "    ```python\n",
    "    import torch\n",
    "    import torch.optim as optim\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "    # Assume X_train and y_train are your training data and labels as tensors\n",
    "    X_train = torch.randn(100, 10) # 100 samples, 10 features\n",
    "    y_train = torch.randn(100, 1)  # 100 labels\n",
    "    \n",
    "    # Example model\n",
    "    model = nn.Linear(10, 1) \n",
    "    \n",
    "    # SGD Optimizer (typically used with mini-batches)\n",
    "    optimizer_sgd = optim.SGD(model.parameters(), lr=0.01) \n",
    "\n",
    "    batch_size = 32\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Example snippet of a training loop:\n",
    "    # num_epochs = 10\n",
    "    # for epoch in range(num_epochs):\n",
    "    #     for inputs, labels in train_loader: # inputs and labels are a mini-batch\n",
    "    #         optimizer_sgd.zero_grad() \n",
    "    #         outputs = model(inputs)\n",
    "    #         # loss = criterion(outputs, labels) # Assuming criterion is defined\n",
    "    #         # loss.backward()         \n",
    "    #         # optimizer_sgd.step()    \n",
    "    #         pass # Placeholder for actual loss calculation and backprop\n",
    "    ```\n",
    "    * PyTorch `DataLoader` docs: [https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "\n",
    "### 2.5 Parameter Initialization\n",
    "\n",
    "* Iterative algorithms need a starting point for weights $w^{(0)}$.\n",
    "* **Symmetry Breaking:** If all weights connected to a set of hidden units are initialized the same (e.g., to zero), those units will always learn the same thing, making them redundant.\n",
    "    * **Solution:** Initialize weights randomly (e.g., from a uniform or Gaussian distribution).\n",
    "* **Distribution choice:**\n",
    "    * Uniform: $[-\\epsilon, \\epsilon]$\n",
    "    * Gaussian: $\\mathcal{N}(0, \\epsilon^2)$\n",
    "* **He Initialization:** A common heuristic for choosing $\\epsilon$, especially with ReLU activation functions. Aims to keep the variance of activations stable across layers. For a unit with M inputs: $\\epsilon = \\sqrt{2/M}$.\n",
    "* **Biases:** Often initialized to small positive values (e.g., 0.01 or 0.1) to ensure ReLU units are active initially.\n",
    "* **Transfer Learning:** Initialize with weights from a network trained on a different (but related) task.\n",
    "\n",
    "* **PyTorch Example (Default Initialization):**\n",
    "    PyTorch layers (like `nn.Linear`, `nn.Conv2d`) have default initializations. For instance, `nn.Linear` often uses a Kaiming (He) uniform initialization for weights when an activation like ReLU is expected, and a uniform initialization for biases. You can find more details and other initialization methods in `torch.nn.init`.\n",
    "    ```python\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    # Default initialization when creating a layer\n",
    "    linear_layer = nn.Linear(10, 5) # Weights and biases are initialized automatically\n",
    "    # The specific default depends on the PyTorch version and layer type.\n",
    "    # print(linear_layer.weight)\n",
    "    # print(linear_layer.bias)\n",
    "    ```\n",
    "    * PyTorch `torch.nn.init` docs: [https://pytorch.org/docs/stable/nn.init.html](https://pytorch.org/docs/stable/nn.init.html)\n",
    "\n",
    "## 3. Convergence\n",
    "\n",
    "How fast and reliably does our algorithm find the minimum?\n",
    "\n",
    "* **The \"Valley\" Problem:** If the error surface has very different curvatures in different directions (like a long, narrow valley), standard gradient descent can be slow.\n",
    "    * The negative gradient often doesn't point directly to the minimum.\n",
    "    * It might oscillate across the narrow part of the valley, making slow progress along the length of the valley.\n",
    "    * <img src=\"image/Figure_3.png\" width=\"60%\">\n",
    "* **Learning Rate $\\eta$:**\n",
    "    * Too small: Very slow learning.\n",
    "    * Too large: Oscillations can become unstable and diverge.\n",
    "* **Mathematical Insight (Quadratic Approximation near minimum):**\n",
    "    * The distance to the minimum along each principal direction (eigenvector $u_i$ of the Hessian matrix) changes by a factor $(1 - \\eta \\lambda_i)$ at each step, where $\\lambda_i$ is the eigenvalue.\n",
    "    * For convergence: $|1 - \\eta \\lambda_i| < 1$. This limits $\\eta < 2/\\lambda_{max}$.\n",
    "    * Convergence rate is often limited by the smallest eigenvalue $\\lambda_{min}$ (slowest direction).\n",
    "    * If $\\lambda_{min} / \\lambda_{max}$ is very small (ill-conditioned Hessian), progress is slow.\n",
    "\n",
    "### 3.1 Momentum\n",
    "\n",
    "* **Idea:** Add \"inertia\" to the movement through weight space to smooth out oscillations and speed up progress in consistent directions.\n",
    "* **Update rule:** $\\Delta w^{(\\tau-1)} = -\\eta \\nabla E(w^{(\\tau-1)}) + \\mu \\Delta w^{(\\tau-2)}$\n",
    "    * $\\mu$ (mu) is the **momentum parameter** (e.g., 0.9).\n",
    "    * The current update includes a fraction of the previous update.\n",
    "* **Effect:**\n",
    "    * In flat regions (low curvature), effective learning rate becomes $\\eta / (1-\\mu)$, speeding up.\n",
    "        * <img src=\"image/Figure_4.png\" width=\"60%\">\n",
    "    * In oscillatory regions (high curvature), momentum terms tend to cancel, damping oscillations.\n",
    "        * <img src=\"image/Figure_5.png\" width=\"60%\">\n",
    "    * Overall, faster convergence without divergent oscillations.\n",
    "        * <img src=\"image/Figure_6.png\" width=\"60%\">\n",
    "* Introduces another hyperparameter $\\mu$ to tune ($0 \\le \\mu < 1$).\n",
    "\n",
    "* **PyTorch Example (SGD with Momentum):**\n",
    "    ```python\n",
    "    import torch.optim as optim\n",
    "    import torch.nn as nn # Added for model definition\n",
    "    model = nn.Linear(10,1) # Example model\n",
    "    optimizer_momentum = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    ```\n",
    "    * (Momentum is a parameter of `optim.SGD`. See [PyTorch optim.SGD docs](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html))\n",
    "\n",
    "* **Nesterov Momentum:** A slight modification. First, make a step based on previous momentum, *then* calculate the gradient at this new \"lookahead\" position.\n",
    "    * Update: $\\Delta w^{(\\tau-1)} = -\\eta \\nabla E(w^{(\\tau-1)} + \\mu \\Delta w^{(\\tau-2)}) + \\mu \\Delta w^{(\\tau-2)}$\n",
    "    * Can improve convergence for batch gradient descent; less clear for SGD.\n",
    "* **PyTorch Example (SGD with Nesterov Momentum):**\n",
    "    ```python\n",
    "    import torch.optim as optim\n",
    "    import torch.nn as nn # Added for model definition\n",
    "    model = nn.Linear(10,1) # Example model\n",
    "    optimizer_nesterov = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "    ```\n",
    "    * (Nesterov is a parameter of `optim.SGD`. See [PyTorch optim.SGD docs](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html))\n",
    "\n",
    "\n",
    "### 3.2 Learning Rate Schedule\n",
    "\n",
    "* **Idea:** Change the learning rate $\\eta$ during training.\n",
    "    * Start with a larger $\\eta$ for faster initial progress.\n",
    "    * Reduce $\\eta$ over time to allow finer adjustments as we get closer to the minimum.\n",
    "* **Update rule with schedule:** $w^{(\\tau)} = w^{(\\tau-1)} - \\eta^{(\\tau-1)} \\nabla E_n(w^{(\\tau-1)})$\n",
    "* **Examples of schedules:**\n",
    "    * Linear decay: $\\eta^{(\\tau)} = (1-\\tau/K)\\eta^{(0)} + (\\tau/K)\\eta^{(K)}$\n",
    "    * Power law decay: $\\eta^{(\\tau)} = \\eta^{(0)}(1+\\tau/s)^{-c}$\n",
    "    * Exponential decay: $\\eta^{(\\tau)} = \\eta^{(0)}c^{\\tau/s}$\n",
    "* Hyperparameters ($\\eta^{(0)}$, K, s, c) need empirical tuning. Monitoring the learning curve (error vs. iterations) is helpful.\n",
    "\n",
    "* **PyTorch Example (Learning Rate Scheduler `StepLR`):**\n",
    "    ```python\n",
    "    import torch.optim as optim\n",
    "    from torch.optim.lr_scheduler import StepLR\n",
    "    import torch.nn as nn\n",
    "\n",
    "    model = nn.Linear(10,1) # Example model\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    # Reduces LR by a factor of gamma (0.1) every step_size (30) epochs\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1) \n",
    "\n",
    "    # In a training loop (after optimizer.step()):\n",
    "    # num_epochs = 100\n",
    "    # for epoch in range(num_epochs):\n",
    "    #     # ... train for one epoch: zero_grad, forward, loss, backward, optimizer.step() ...\n",
    "    #     scheduler.step() # Update the learning rate\n",
    "    ```\n",
    "    * PyTorch `lr_scheduler` docs: [https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n",
    "\n",
    "### 3.3 RMSProp and Adam\n",
    "\n",
    "These algorithms adapt the learning rate for *each parameter individually*.\n",
    "\n",
    "* **AdaGrad (Adaptive Gradient):**\n",
    "    * Reduces learning rates for parameters that have had large gradients historically.\n",
    "    * Accumulates squared gradients: $r_i^{(\\tau)} = r_i^{(\\tau-1)} + (\\frac{\\partial E}{\\partial w_i})^2$\n",
    "    * Update: $w_i^{(\\tau)} = w_i^{(\\tau-1)} - \\frac{\\eta}{\\sqrt{r_i^{(\\tau)}} + \\delta} \\frac{\\partial E}{\\partial w_i}$\n",
    "        * $\\delta$ is a small constant for numerical stability.\n",
    "    * **Problem:** Learning rates can become too small, prematurely stopping learning.\n",
    "\n",
    "* **PyTorch Example (AdaGrad Optimizer):**\n",
    "    ```python\n",
    "    import torch.optim as optim\n",
    "    import torch.nn as nn\n",
    "    model = nn.Linear(10,1) # Example model\n",
    "    optimizer_adagrad = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "    ```\n",
    "    * PyTorch `optim.Adagrad` docs: [https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html)\n",
    "\n",
    "\n",
    "* **RMSProp (Root Mean Square Propagation):**\n",
    "    * Addresses AdaGrad's problem by using an exponentially weighted moving average of squared gradients, instead of accumulating them indefinitely.\n",
    "    * Moving average: $r_i^{(\\tau)} = \\beta r_i^{(\\tau-1)} + (1-\\beta)(\\frac{\\partial E}{\\partial w_i})^2$ (typical $\\beta=0.9$)\n",
    "    * Update: $w_i^{(\\tau)} = w_i^{(\\tau-1)} - \\frac{\\eta}{\\sqrt{r_i^{(\\tau)}} + \\delta} \\frac{\\partial E}{\\partial w_i}$\n",
    "\n",
    "* **PyTorch Example (RMSprop Optimizer):**\n",
    "    ```python\n",
    "    import torch.optim as optim\n",
    "    import torch.nn as nn\n",
    "    model = nn.Linear(10,1) # Example model\n",
    "    # alpha is equivalent to beta in the formula\n",
    "    optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99) \n",
    "    ```\n",
    "    * PyTorch `optim.RMSprop` docs: [https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html)\n",
    "\n",
    "\n",
    "* **Adam (Adaptive Moments):**\n",
    "    * Combines RMSProp with momentum.\n",
    "    * Stores exponentially weighted moving averages of both the gradients (1st moment, like momentum) and squared gradients (2nd moment, like RMSProp).\n",
    "        * **Why per-parameter momentum?** Allows each parameter to have its own \"velocity\" or \"inertia\". Parameters with consistent gradient directions accelerate, while those with oscillating gradients have their updates dampened. This adapts the update strength individually for each parameter.\n",
    "    * 1st moment: $s_i^{(\\tau)} = \\beta_1 s_i^{(\\tau-1)} + (1-\\beta_1)\\frac{\\partial E}{\\partial w_i}$\n",
    "    * 2nd moment: $r_i^{(\\tau)} = \\beta_2 r_i^{(\\tau-1)} + (1-\\beta_2)(\\frac{\\partial E}{\\partial w_i})^2$\n",
    "    * Bias correction (important early in training as $s, r$ start at 0):\n",
    "        * $\\hat{s}_i^{(\\tau)} = s_i^{(\\tau)} / (1-\\beta_1^\\tau)$\n",
    "        * $\\hat{r}_i^{(\\tau)} = r_i^{(\\tau)} / (1-\\beta_2^\\tau)$\n",
    "    * Update: $w_i^{(\\tau)} = w_i^{(\\tau-1)} - \\eta \\frac{\\hat{s}_i^{(\\tau)}}{\\sqrt{\\hat{r}_i^{(\\tau)}} + \\delta}$\n",
    "    * Typical values: $\\beta_1=0.9$, $\\beta_2=0.99$.\n",
    "    * Adam is a very popular and often default choice for deep learning optimization.\n",
    "\n",
    "* **PyTorch Example (Adam Optimizer):**\n",
    "    ```python\n",
    "    import torch.optim as optim\n",
    "    import torch.nn as nn\n",
    "    model = nn.Linear(10,1) # Example model\n",
    "    optimizer_adam = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "    ```\n",
    "    * PyTorch `optim.Adam` docs: [https://pytorch.org/docs/stable/generated/torch.optim.Adam.html](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)\n",
    "\n",
    "\n",
    "## 4. Normalization\n",
    "\n",
    "Adjusting the scale of variables can make training more effective and stable.\n",
    "\n",
    "### 4.1 Data Normalization\n",
    "\n",
    "* **Problem:** Input variables might have vastly different ranges (e.g., height in meters vs. platelet count in 100,000s).\n",
    "    * This can lead to an error surface with very different curvatures (like the \"valley\" problem), making training hard.\n",
    "    * <img src=\"image/Figure_7.png\" width=\"60%\">\n",
    "* **Solution:** Rescale input features before training.\n",
    "    1.  Calculate mean ($\\mu_i$) and standard deviation ($\\sigma_i$) for each feature $i$ across the training set.\n",
    "        * $\\mu_i = \\frac{1}{N} \\sum_{n=1}^N x_{ni}$\n",
    "        * $\\sigma_i^2 = \\frac{1}{N} \\sum_{n=1}^N (x_{ni} - \\mu_i)^2$\n",
    "    2.  Normalize: $\\tilde{x}_{ni} = \\frac{x_{ni} - \\mu_i}{\\sigma_i}$\n",
    "        * Normalized features have zero mean and unit variance.\n",
    "* **Important:** Use the *same* $\\mu_i$ and $\\sigma_i$ (calculated from training data) to normalize validation and test data.\n",
    "\n",
    "* **PyTorch Example (Data Normalization with `transforms` for image data):**\n",
    "    This is typically done as a preprocessing step, often when loading image data.\n",
    "    ```python\n",
    "    import torchvision.transforms as transforms\n",
    "    import torch\n",
    "    from PIL import Image # For image loading example\n",
    "\n",
    "    # Example for image data (e.g., 3 channels: R, G, B)\n",
    "    # These are typical means and stds for ImageNet dataset\n",
    "    means = [0.485, 0.456, 0.406] \n",
    "    stds = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize(256), # Example transform\n",
    "        transforms.CenterCrop(224), # Example transform\n",
    "        transforms.ToTensor(), # Converts PIL image or numpy.ndarray to tensor\n",
    "        transforms.Normalize(mean=means, std=stds)\n",
    "    ])\n",
    "    \n",
    "    # Apply this transform when creating your Dataset or loading an image\n",
    "    # Assuming you have an image file \"path_to_image.jpg\"\n",
    "    # try:\n",
    "    #     img = Image.open(\"path_to_image.jpg\")\n",
    "    #     normalized_img_tensor = data_transform(img)\n",
    "    # except FileNotFoundError:\n",
    "    #     print(\"Image file not found, using dummy tensor for demonstration.\")\n",
    "    #     normalized_img_tensor = torch.randn(3, 224, 224)\n",
    "\n",
    "\n",
    "    # For general tensor data (not images):\n",
    "    X = torch.randn(100, 5) # 100 samples, 5 features\n",
    "    mean_vals = X.mean(dim=0, keepdim=True) # Calculate mean per feature\n",
    "    std_vals = X.std(dim=0, keepdim=True)   # Calculate std per feature\n",
    "    X_normalized = (X - mean_vals) / (std_vals + 1e-5) # Add epsilon for stability\n",
    "    ```\n",
    "    * PyTorch `transforms.Normalize` docs: [https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html](https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html)\n",
    "\n",
    "### 4.2 Batch Normalization (Batch Norm)\n",
    "\n",
    "* **Idea:** Apply normalization to the activations *within* hidden layers of the network, not just inputs.\n",
    "* **Motivation:**\n",
    "    * **Internal Covariate Shift:** The distribution of inputs to deeper layers changes as the parameters of earlier layers are updated. Batch Norm aims to reduce this.\n",
    "    * **Vanishing/Exploding Gradients:** In very deep networks, gradients can become extremely small or large as they are backpropagated. Batch Norm helps stabilize gradients.\n",
    "* **How it works (for pre-activations $a_i$ in a layer, per mini-batch of size K):**\n",
    "    1.  Calculate mini-batch mean: $\\mu_i = \\frac{1}{K} \\sum_{n=1}^K a_{ni}$\n",
    "    2.  Calculate mini-batch variance: $\\sigma_i^2 = \\frac{1}{K} \\sum_{n=1}^K (a_{ni} - \\mu_i)^2$\n",
    "    3.  Normalize: $\\hat{a}_{ni} = \\frac{a_{ni} - \\mu_i}{\\sqrt{\\sigma_i^2 + \\delta}}$\n",
    "    4.  **Scale and Shift:** Introduce learnable parameters $\\gamma_i$ (scale) and $\\beta_i$ (shift) for each hidden unit $i$.\n",
    "        * $\\tilde{a}_{ni} = \\gamma_i \\hat{a}_{ni} + \\beta_i$\n",
    "        * This allows the network to learn the optimal mean and variance for each unit, if needed. $\\gamma_i$ and $\\beta_i$ are learned along with weights.\n",
    "* **During Inference (Testing):**\n",
    "    * We don't have mini-batches.\n",
    "    * Use moving averages of $\\mu_i$ and $\\sigma_i^2$ (and the learned $\\gamma_i, \\beta_i$) collected during training. The `nn.BatchNorm` layers handle this automatically based on `model.train()` or `model.eval()` mode.\n",
    "* **Why it works well is still debated:** Original motivation was reducing internal covariate shift, but newer studies suggest it makes the optimization landscape smoother.\n",
    "* Illustration: <img src=\"image/Figure_8_a.png\" width=\"60%\"> \n",
    "\n",
    "* **PyTorch Example (Batch Normalization Layer):**\n",
    "    ```python\n",
    "    import torch.nn as nn\n",
    "    import torch\n",
    "\n",
    "    # For 1D data (e.g., after a Linear layer before activation)\n",
    "    num_features_1d = 64 # Number of features from the previous layer\n",
    "    batch_norm_1d = nn.BatchNorm1d(num_features_1d)\n",
    "\n",
    "    # For 2D data (e.g., after a Conv2d layer, num_features is num_channels)\n",
    "    num_channels_2d = 16\n",
    "    batch_norm_2d = nn.BatchNorm2d(num_channels_2d)\n",
    "\n",
    "    # Example usage in a model:\n",
    "    model_bn = nn.Sequential(\n",
    "        nn.Linear(10, num_features_1d),\n",
    "        nn.BatchNorm1d(num_features_1d), # Apply batch norm\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_features_1d, 1)\n",
    "    )\n",
    "    \n",
    "    # Dummy input for demonstration\n",
    "    # input_tensor = torch.randn(32, 10) # Batch of 32, 10 features\n",
    "    # model_bn.train() # Set model to training mode\n",
    "    # output_train = model_bn(input_tensor)\n",
    "    # model_bn.eval()  # Set model to evaluation mode (uses running stats)\n",
    "    # output_eval = model_bn(input_tensor)\n",
    "    ```\n",
    "    * PyTorch `nn.BatchNorm1d` docs: [https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)\n",
    "    * PyTorch `nn.BatchNorm2d` docs: [https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)\n",
    "\n",
    "### 4.3 Layer Normalization (Layer Norm)\n",
    "\n",
    "* **Alternative to Batch Norm.**\n",
    "* **Motivation:**\n",
    "    * Batch Norm can be tricky if mini-batch size is very small (noisy stats).\n",
    "    * Less efficient for some architectures (e.g., Recurrent Neural Networks, Transformers) where statistics might change at each time step.\n",
    "* **How it works (for pre-activations $a_{ni}$ for a single data point $n$, across M hidden units in a layer):**\n",
    "    1.  Calculate mean across units for data point $n$: $\\mu_n = \\frac{1}{M} \\sum_{i=1}^M a_{ni}$\n",
    "    2.  Calculate variance across units for data point $n$: $\\sigma_n^2 = \\frac{1}{M} \\sum_{i=1}^M (a_{ni} - \\mu_n)^2$\n",
    "    3.  Normalize: $\\hat{a}_{ni} = \\frac{a_{ni} - \\mu_n}{\\sqrt{\\sigma_n^2 + \\delta}}$\n",
    "    4.  **Scale and Shift:** Also uses learnable $\\gamma_i$ and $\\beta_i$ for each unit $i$: $\\tilde{a}_{ni} = \\gamma_i \\hat{a}_{ni} + \\beta_i$.\n",
    "* **Key Difference from Batch Norm:** Normalization is done *across features/hidden units for each data point independently*, instead of across data points in a batch for each feature/hidden unit independently.\n",
    "* The same normalization function can be used during training and inference (no need for moving averages).\n",
    "* Illustration: <img src=\"image/Figure_8_b.png\" width=\"40%\">\n",
    "\n",
    "* **PyTorch Example (Layer Normalization Layer):**\n",
    "    ```python\n",
    "    import torch.nn as nn\n",
    "    import torch\n",
    "\n",
    "    # normalized_shape is the shape of the input tensor part to be normalized.\n",
    "    # If input is (N, C, H, W), normalized_shape can be [C, H, W] to normalize over C, H, W.\n",
    "    # If input is (N, L, D) for sequences, normalized_shape can be [D] (normalize last dim - common in Transformers)\n",
    "    # or [L,D] (normalize last two dims).\n",
    "    \n",
    "    # Example: Input tensor of shape (batch_size, num_features)\n",
    "    num_features = 64\n",
    "    # normalized_shape is [num_features]\n",
    "    layer_norm_fc = nn.LayerNorm(num_features) \n",
    "\n",
    "    # Example: Input tensor for sequences (batch_size, sequence_length, embedding_dim)\n",
    "    embedding_dim = 128\n",
    "    # This will normalize over the embedding_dim for each element in the sequence\n",
    "    layer_norm_seq = nn.LayerNorm(embedding_dim) \n",
    "    \n",
    "    # Example usage in a model:\n",
    "    model_ln = nn.Sequential(\n",
    "        nn.Linear(10, num_features),\n",
    "        nn.LayerNorm(num_features), # Apply layer norm\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_features, 1)\n",
    "    )\n",
    "    dummy_input = torch.randn(32, 10) # Batch of 32, 10 features\n",
    "    output = model_ln(dummy_input)\n",
    "    ```\n",
    "    * PyTorch `nn.LayerNorm` docs: [https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n",
    "\n",
    "---\n",
    "**Reference:**\n",
    "This summary is based on concepts primarily discussed in:\n",
    "* Bishop, C. M. (2024). *Deep Learning: Foundations and Concepts*. Springer. (Chapter 7: Gradient Descent).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
