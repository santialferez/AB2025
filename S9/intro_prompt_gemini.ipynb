{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Introducción a la ingeniería de prompts (Actualizado con Gemini)\n",
    "\n",
    "\n",
    "\n",
    " <!--\n",
    "\n",
    " Original lesson date: June 2, 2025\n",
    "\n",
    " This version updated to use Gemini models via OpenAI-compatible API,\n",
    "\n",
    " includes new sections on RAG and MCP, and is condensed for a 1-hour lesson.\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Configuración del Entorno\n",
    "\n",
    "\n",
    "\n",
    " Primero, instalaremos la biblioteca de OpenAI y configuraremos nuestra clave API para usar los modelos Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai -q # Descomenta para instalar en Colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ejecutar la siguiente celda en Colab, tras haber creado un secreto con el nombre de `GOOGLE_API_KEY` que contenga tu clave API de Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key no encontrada. Por favor, ingrésala manualmente:\n",
      "Usando el modelo Gemini: gemini-2.0-flash\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json # Para formateo de JSON y manejo de argumentos de herramientas\n",
    "\n",
    "# Intenta obtener la clave API desde los secretos de Colab\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "except ImportError:\n",
    "    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        print(\"API Key no encontrada. Por favor, ingrésala manualmente:\")\n",
    "        GOOGLE_API_KEY = input()\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Se requiere una Google API Key para ejecutar este notebook.\")\n",
    "\n",
    "# Configuración del cliente de OpenAI para usar Gemini\n",
    "client = OpenAI(\n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\" \n",
    "print(f\"Usando el modelo Gemini: {GEMINI_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ¿Qué es un Modelo de Lenguaje Grande (LLM)?\n",
    "\n",
    "\n",
    "\n",
    " Los LLMs son sistemas de IA entrenados para entender y generar texto similar al humano. Aprenden procesando grandes cantidades de datos textuales para predecir la siguiente palabra en un contexto dado. Su gran tamaño es clave para su potencia.\n",
    "\n",
    "\n",
    "\n",
    " ### Breve Historia\n",
    "\n",
    "\n",
    "\n",
    " * **c. 2014: RNNs (Redes Neuronales Recurrentes)** dominaban, pero tenían limitaciones con secuencias largas.\n",
    "\n",
    " * **Sept 2014: Attention Mechanism.** Permitió a los modelos enfocarse en partes relevantes de la entrada, superando cuellos de botella. (Referencia: *Neural Machine Translation by Jointly Learning to Align and Translate*)\n",
    "\n",
    " * **Jun 2017: Transformer Architecture.** Revolucionó el campo, eliminando la necesidad de RNNs. (Referencia: *Attention Is All You Need*)\n",
    "\n",
    " * **Jun 2018: GPT-1.** Usó solo el decodificador del Transformer para predecir la siguiente palabra. (Referencia: *Improving Language Understanding by Generative Pre-Training*)\n",
    "\n",
    " * **Feb 2019: GPT-2.** Mostró capacidades multitarea sin entrenamiento específico, evidenciando el poder del preentrenamiento a gran escala. (Referencia: *Language Models are Unsupervised Multitask Learners*)\n",
    "\n",
    "\n",
    "\n",
    " GPT-2 ya podía realizar tareas como resumen, traducción y generación de contenido, lo que planteó tanto entusiasmo como preocupaciones sobre su uso malicioso. Esto nos lleva a la importancia del **prompt crafting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Prompt Crafting: El Arte de Hablar con las Máquinas\n",
    "\n",
    "\n",
    "\n",
    " Una aplicación de LLM actúa como una **capa de transformación** entre el problema del usuario y el LLM. El \"prompt crafting\" es el diseño cuidadoso de las entradas (prompts) para guiar al LLM.\n",
    "\n",
    "\n",
    "\n",
    " Exploremos algunas técnicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de completado usando Gemini\n",
    "def completion_gemini(user_message, system_message=\"You are a base language model that performs text completion. You predict the next words or tokens to complete the given text. Do not engage in conversation or provide explanations unless specifically requested. Simply complete the text naturally.\", model=GEMINI_MODEL, temperature=0.7):\n",
    "    try:\n",
    "        messages = []\n",
    "        if system_message:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error al generar la completación: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un Transformer es un modelo de red neuronal que se basa en mecanismos de autoatención, destacando por su capacidad para procesar secuencias de datos en paralelo. Esto lo hace especialmente eficiente para tareas de procesamiento del lenguaje natural como la traducción automática, la generación de texto y el análisis de sentimientos.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ejemplo rápido\n",
    "print(completion_gemini(\"Explica brevemente qué es un Transformer en IA.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ### Técnica 1: Few-Shot Prompting\n",
    "\n",
    "\n",
    "\n",
    " Se proporcionan al LLM algunos ejemplos de la tarea deseada para guiar su respuesta.\n",
    "\n",
    "\n",
    "\n",
    " ```markdown\n",
    "\n",
    " > How are you doing today?\n",
    "\n",
    " < ¿Cómo estás hoy?\n",
    "\n",
    " > My name is John.\n",
    "\n",
    " < Mi nombre es John.\n",
    "\n",
    " > Can I have fries with that?\n",
    "\n",
    " ```\n",
    "\n",
    " El LLM continuará el patrón.\n",
    "\n",
    "\n",
    "\n",
    " * **Clave:** Simplicidad y efectividad. El contexto inicial es crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Me da papas fritas con eso?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"Q: How are you doing today?\n",
    "A: ¿Cómo estás hoy?\n",
    "Q: My name is John.\n",
    "A: Mi nombre es John.\n",
    "Q: Can I have fries with that?\n",
    "A:\"\"\"\n",
    "\n",
    "\n",
    "print(completion_gemini(few_shot_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ### Técnica 2: Chain of Thought Reasoning (CoT)\n",
    "\n",
    "\n",
    "\n",
    " Se instruye al LLM para que \"piense paso a paso\" o se le muestran ejemplos de razonamiento. Esto mejora la lógica en problemas complejos.\n",
    "\n",
    "\n",
    "\n",
    " **Ejemplo con CoT (few-shot):**\n",
    "\n",
    " ```markdown\n",
    "\n",
    " P: Jim tiene el doble de la edad de Steve. Jim tiene 12 años, ¿cuántos años tiene Steve?\n",
    "\n",
    " R: En forma de ecuación: 12 = 2*a donde a es la edad de Steve. Dividiendo ambos lados por 2 vemos que a = 6. Steve tiene 6 años.\n",
    "\n",
    " P: A un panadero le lleva una hora hacer un pastel. ¿Cuánto tiempo tardan 3 panaderos en hacer 3 pasteles?\n",
    "\n",
    " R:\n",
    "\n",
    " ```\n",
    "\n",
    " **Respuesta esperada (correcta):** Si cada panadero tarda 1 hora en hacer un pastel, entonces 3 panaderos tardarán 1 hora en hacer 3 pasteles.\n",
    "\n",
    "\n",
    "\n",
    " **Zero-shot CoT:** Simplemente añadir \"Pensemos paso a paso\" puede ayudar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cada panadero tarda una hora en hacer un pastel.\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought_prompt = \"\"\"Q: Jim tiene el doble de la edad de Steve. Jim tiene 12 años, ¿cuántos años tiene Steve?\n",
    "A: En forma de ecuación: 12 = 2*a donde a es la edad de Steve. Dividiendo ambos lados por 2 vemos que a = 6. Steve tiene 6 años.\n",
    "Q: A un panadero le lleva una hora hacer un pastel. ¿Cuánto tiempo tardan 3 panaderos en hacer 3 pasteles?\n",
    "A:\"\"\"\n",
    "\n",
    "print(completion_gemini(chain_of_thought_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un panadero puede hacer un pastel en una hora.\n",
      "Por lo tanto, 3 panaderos pueden hacer 3 pasteles en una hora.\n",
      "La respuesta es 1 hora.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "zero_shot_cot_prompt = \"\"\"Q: A un panadero le lleva una hora hacer un pastel. ¿Cuánto tiempo tardan 3 panaderos en hacer 3 pasteles?\n",
    "A: Pensemos paso a paso.\"\"\"\n",
    "\n",
    "print(completion_gemini(zero_shot_cot_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ### Técnica 3: Imitación de Documentos\n",
    "\n",
    "\n",
    "\n",
    " Estructurar el prompt para imitar formatos conocidos (transcripciones, código, etc.) ayuda al LLM a entender el contexto.\n",
    "\n",
    "\n",
    "\n",
    " **Ejemplo (Soporte Técnico):**\n",
    "\n",
    " ```markdown\n",
    "\n",
    " # IT Support Assistant\n",
    "\n",
    " The following is a transcript between an award-winning IT support rep and a customer.\n",
    "\n",
    "\n",
    "\n",
    " ## Customer:\n",
    "\n",
    " My cable is out! And I'm going to miss the Super Bowl!\n",
    "\n",
    "\n",
    "\n",
    " ## Support Assistant:\n",
    "\n",
    " ```\n",
    "\n",
    " Esto le da al LLM pistas sobre el rol, la estructura y el tono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh no! That's terrible timing! I understand how frustrating it is to miss the Super Bowl because of a cable outage. Let's get this fixed for you ASAP. \n",
      "\n",
      "First, can you tell me a little more about what's happening? Are you getting any error messages on your TV screen? Is the picture just black, or is it pixelated or fuzzy? Knowing a bit more will help me troubleshoot more effectively.\n",
      "\n",
      "In the meantime, I'll start checking for any known outages in your area. Let's get you back to the game! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_mimicry_prompt = \"\"\"# IT Support Assistant\n",
    "The following is a transcript between an award-winning IT support rep and a customer.\n",
    "\n",
    "## Customer: \n",
    "My cable is out! And I'm going to miss the Super Bowl!\n",
    "\n",
    "## Support Assistant:\"\"\"\n",
    "\n",
    "print(completion_gemini(document_mimicry_prompt, system_message=\"You are an award-winning IT support representative. Be helpful and empathetic.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Creando el Prompt: Un Proceso\n",
    "\n",
    "\n",
    "\n",
    " 1.  **Recopilar contexto:** ¿Qué información necesita el LLM?\n",
    "\n",
    " 2.  **Rankear contexto:** ¿Qué es lo más importante?\n",
    "\n",
    " 3.  **Recortar contexto:** Evitar información innecesaria (los LLMs tienen ventanas de contexto limitadas).\n",
    "\n",
    " 4.  **Ensamblar el prompt:** Combinar todo de forma clara.\n",
    "\n",
    "\n",
    "\n",
    " <!-- Nota para Colab: Las siguientes imágenes son referenciadas desde una carpeta local 'images'.\n",
    "\n",
    "      Asegúrate de que esta carpeta esté en tu Google Drive y montada,\n",
    "\n",
    "      o reemplaza 'src' con URLs directas a las imágenes si están alojadas online. -->\n",
    "\n",
    " <img style=\"width: 70%\" src=\"images/fig-14.png\" id=\"llm-app\" alt=\"[Diagrama conceptual de una aplicación LLM]\"/>\n",
    "\n",
    "\n",
    "\n",
    " *Leyenda: Diagrama conceptual de una aplicación LLM, mostrando la capa de transformación entre el usuario y el modelo.*\n",
    "\n",
    "\n",
    "\n",
    " ### El Chat y las \"Tools\" (Function Calling)\n",
    "\n",
    "\n",
    "\n",
    " Con la llegada de modelos conversacionales como ChatGPT, surgieron formatos como ChatML, que usan roles (`system`, `user`, `assistant`). Los **mensajes de sistema** son cruciales para guiar el comportamiento del LLM.\n",
    "\n",
    "\n",
    "\n",
    " <img style=\"width: 70%\" src=\"images/fig-17.png\" id=\"llm-app-chatml\" alt=\"[Ejemplo de estructura de mensajes en ChatML]\"/>\n",
    "\n",
    " *Leyenda: Ejemplo de estructura de mensajes en ChatML, mostrando roles de sistema, usuario y asistente.*\n",
    "\n",
    "\n",
    "\n",
    " El **Function Calling** (o \"Tool Calling\") extiende esto, permitiendo a los LLMs interactuar con APIs externas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Estructuración de Salida en Modo JSON\n",
    "\n",
    "\n",
    "\n",
    " Los LLMs pueden generar respuestas en JSON, útil para la integración. Se usa `response_format={\"type\": \"json_object\"}`. Para estructuras complejas, Pydantic es una buena opción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"description\": \"Una canasta con una manzana roja y un plátano amarillo.\",\n",
      "  \"fruits\": [\n",
      "    {\n",
      "      \"name\": \"manzana\",\n",
      "      \"color\": \"roja\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"plátano\",\n",
      "      \"color\": \"amarillo\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# !pip install pydantic -q # Descomenta para instalar en Colab\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Fruit(BaseModel):\n",
    "    name: str = Field(description=\"The name of the fruit\")\n",
    "    color: str = Field(description=\"The color of the fruit\")\n",
    "\n",
    "class FruitBasket(BaseModel):\n",
    "    fruits: List[Fruit] = Field(description=\"A list of fruits\")\n",
    "    description: str = Field(description=\"A brief description\")\n",
    "\n",
    "def chat_gemini_pydantic_json(user_prompt, system_message=\"You are a helpful assistant designed to output structured JSON data.\", model=GEMINI_MODEL):\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            response_format=FruitBasket\n",
    "        )\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Ejemplo Pydantic\n",
    "pydantic_user_prompt = \"Describe una canasta con una manzana roja y un plátano amarillo.\"\n",
    "print(chat_gemini_pydantic_json(pydantic_user_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Tool Calling (Function Calling) con Gemini\n",
    "\n",
    "\n",
    "\n",
    " El \"Tool Calling\" permite a los LLMs interactuar con funciones externas que tú defines, ampliando sus capacidades más allá de la generación de texto. Es como darle al LLM herramientas para realizar tareas específicas.\n",
    "\n",
    "\n",
    "\n",
    " **Flujo Detallado:**\n",
    "\n",
    "\n",
    "\n",
    " 1.  **Definición de la Herramienta (Tu Código Python):**\n",
    "\n",
    "     * Creas una función Python normal que realiza una tarea específica (ej., obtener el clima, buscar en una base de datos, calcular algo).\n",
    "\n",
    "     * Esta función debe devolver idealmente una cadena (a menudo JSON) que el LLM pueda entender.\n",
    "\n",
    "\n",
    "\n",
    " 2.  **Especificación de la Herramienta para el LLM (JSON Schema):**\n",
    "\n",
    "     * Le describes esta función al LLM usando un formato específico (JSON Schema).\n",
    "\n",
    "     * Esta descripción incluye:\n",
    "\n",
    "         * `name`: Nombre de la función (debe coincidir con tu función Python).\n",
    "\n",
    "         * `description`: Qué hace la función (para que el LLM sepa cuándo usarla).\n",
    "\n",
    "         * `parameters`: Qué argumentos necesita la función, sus tipos y descripciones.\n",
    "\n",
    "\n",
    "\n",
    " 3.  **Llamada Inicial al LLM:**\n",
    "\n",
    "     * Envías el prompt del usuario al LLM.\n",
    "\n",
    "     * Junto con el prompt, envías la lista de `tools` (las especificaciones de tus herramientas).\n",
    "\n",
    "     * Usas `tool_choice=\"auto\"` para que el LLM decida si necesita usar alguna herramienta para responder al prompt del usuario.\n",
    "\n",
    "\n",
    "\n",
    " 4.  **Respuesta del LLM (Decisión de Usar Herramienta):**\n",
    "\n",
    "     * Si el LLM determina que necesita una herramienta, en lugar de una respuesta textual directa, su mensaje contendrá `tool_calls`.\n",
    "\n",
    "     * Cada `tool_call` indica:\n",
    "\n",
    "         * `id`: Un identificador único para esta llamada.\n",
    "\n",
    "         * `function.name`: El nombre de la función que quiere ejecutar.\n",
    "\n",
    "         * `function.arguments`: Una cadena JSON con los argumentos que el LLM cree que tu función necesita.\n",
    "\n",
    "\n",
    "\n",
    " 5.  **Ejecución de la Herramienta (Tu Código):**\n",
    "\n",
    "     * Tu código recibe estos `tool_calls`.\n",
    "\n",
    "     * Para cada `tool_call`:\n",
    "\n",
    "         * Identificas qué función Python ejecutar basándote en `function.name`.\n",
    "\n",
    "         * Parseas `function.arguments` (de JSON a un diccionario Python).\n",
    "\n",
    "         * Llamas a tu función Python con esos argumentos.\n",
    "\n",
    "\n",
    "\n",
    " 6.  **Envío de Resultados al LLM:**\n",
    "\n",
    "     * Añades un nuevo mensaje al historial de la conversación. Este mensaje tiene:\n",
    "\n",
    "         * `role`: \"tool\"\n",
    "\n",
    "         * `tool_call_id`: El mismo ID de la `tool_call` original.\n",
    "\n",
    "         * `name`: El nombre de la función que se ejecutó.\n",
    "\n",
    "         * `content`: El resultado que devolvió tu función Python.\n",
    "\n",
    "\n",
    "\n",
    " 7.  **Llamada Final al LLM:**\n",
    "\n",
    "     * Vuelves a llamar al LLM con el historial de conversación actualizado (que ahora incluye la solicitud de herramienta y su resultado).\n",
    "\n",
    "     * El LLM usará el resultado de la herramienta para formular una respuesta final y natural al prompt original del usuario.\n",
    "\n",
    "\n",
    "\n",
    " <img style=\"width: 70%\" src=\"images/fig-18.png\" id=\"llm-app-tools\" alt=\"[Diagrama de flujo de Function Calling]\"/>\n",
    "\n",
    " *Leyenda: Flujo simplificado de Function Calling, mostrando la interacción entre el usuario, el LLM y las herramientas externas.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What's the weather like in London today?\n",
      "\n",
      "LLM decidió: usar herramientas\n",
      "\n",
      "Ejecutando: get_current_weather con {'location': 'London', 'unit': 'celsius'}\n",
      "\n",
      "Resultado: {\"location\":\"London\",\"temperature\":\"10\",\"unit\":\"celsius\",\"condition\":\"Nublado\",\"error\":null}\n",
      "\n",
      "Generando respuesta final...\n",
      "\n",
      "\n",
      "Respuesta final: The weather in London today is Nublado with a temperature of 10 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Literal\n",
    "import json\n",
    "\n",
    "# Modelos Pydantic para definir las herramientas\n",
    "class WeatherRequest(BaseModel):\n",
    "    location: str = Field(description=\"La ciudad, ej. San Francisco, CA\")\n",
    "    unit: Literal[\"celsius\", \"fahrenheit\"] = Field(default=\"celsius\", description=\"Unidad de temperatura\")\n",
    "\n",
    "class WeatherResponse(BaseModel):\n",
    "    location: str\n",
    "    temperature: str\n",
    "    unit: str\n",
    "    condition: str\n",
    "    error: Optional[str] = None\n",
    "\n",
    "# Función de herramienta simplificada\n",
    "def get_current_weather(request: WeatherRequest) -> WeatherResponse:\n",
    "    \"\"\"Obtiene el clima actual para una ubicación dada (simulado).\"\"\"\n",
    "    weather_data = {\n",
    "        \"tokyo\": {\"temperature\": \"15\", \"condition\": \"Soleado\"},\n",
    "        \"london\": {\"temperature\": \"10\", \"condition\": \"Nublado\"},\n",
    "        \"paris\": {\"temperature\": \"12\", \"condition\": \"Lluvioso\"}\n",
    "    }\n",
    "    \n",
    "    city = request.location.lower().split(\",\")[0]\n",
    "    \n",
    "    if city in weather_data:\n",
    "        return WeatherResponse(\n",
    "            location=request.location,\n",
    "            temperature=weather_data[city][\"temperature\"],\n",
    "            unit=request.unit,\n",
    "            condition=weather_data[city][\"condition\"]\n",
    "        )\n",
    "    \n",
    "    return WeatherResponse(\n",
    "        location=request.location,\n",
    "        temperature=\"\",\n",
    "        unit=request.unit,\n",
    "        condition=\"\",\n",
    "        error=\"Clima no encontrado\"\n",
    "    )\n",
    "\n",
    "# Generación automática de tool specs desde Pydantic\n",
    "def generate_tool_spec(model_class: BaseModel, function_name: str, description: str):\n",
    "    \"\"\"Genera automáticamente la especificación de herramienta desde un modelo Pydantic.\"\"\"\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": function_name,\n",
    "            \"description\": description,\n",
    "            \"parameters\": model_class.model_json_schema()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Tool specs generadas automáticamente\n",
    "tools_specs = [\n",
    "    generate_tool_spec(\n",
    "        WeatherRequest, \n",
    "        \"get_current_weather\", \n",
    "        \"Obtiene el clima actual en una ubicación específica.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Registro de herramientas disponibles\n",
    "AVAILABLE_TOOLS = {\n",
    "    \"get_current_weather\": {\n",
    "        \"function\": get_current_weather,\n",
    "        \"input_model\": WeatherRequest\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_conversation_with_tools(user_prompt: str, verbose: bool = False) -> str:\n",
    "    \"\"\"Ejecuta una conversación con herramientas usando Pydantic para validación.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "    if verbose: print(f\"User: {user_prompt}\\n\")\n",
    "\n",
    "    try:\n",
    "        # Primera llamada al LLM\n",
    "        response = client.chat.completions.create(\n",
    "            model=GEMINI_MODEL,\n",
    "            messages=messages,\n",
    "            tools=tools_specs,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        \n",
    "        response_message = response.choices[0].message\n",
    "        if verbose: print(f\"LLM decidió: {'usar herramientas' if response_message.tool_calls else 'responder directamente'}\\n\")\n",
    "\n",
    "        # Si no hay tool calls, devolver respuesta directa\n",
    "        if not response_message.tool_calls:\n",
    "            return response_message.content\n",
    "\n",
    "        # Procesar tool calls\n",
    "        messages.append(response_message)\n",
    "        \n",
    "        for tool_call in response_message.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            \n",
    "            if function_name not in AVAILABLE_TOOLS:\n",
    "                if verbose: print(f\"Herramienta no encontrada: {function_name}\\n\")\n",
    "                continue\n",
    "                \n",
    "            tool_info = AVAILABLE_TOOLS[function_name]\n",
    "            \n",
    "            # Validar argumentos con Pydantic\n",
    "            try:\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                validated_input = tool_info[\"input_model\"](**function_args)\n",
    "                if verbose: print(f\"Ejecutando: {function_name} con {validated_input.model_dump()}\\n\")\n",
    "                \n",
    "                # Ejecutar función\n",
    "                result = tool_info[\"function\"](validated_input)\n",
    "                tool_response = result.model_dump_json()\n",
    "                if verbose: print(f\"Resultado: {tool_response}\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                tool_response = json.dumps({\"error\": f\"Error validando argumentos: {str(e)}\"})\n",
    "                if verbose: print(f\"Error: {tool_response}\\n\")\n",
    "            \n",
    "            # Añadir respuesta de herramienta al historial\n",
    "            messages.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": tool_response\n",
    "            })\n",
    "        \n",
    "        # Segunda llamada al LLM con resultados de herramientas\n",
    "        if verbose: print(\"Generando respuesta final...\\n\")\n",
    "        final_response = client.chat.completions.create(\n",
    "            model=GEMINI_MODEL, \n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        return final_response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error en Tool Calling: {e}\"\n",
    "\n",
    "# Ejemplo de uso\n",
    "user_weather_prompt = \"What's the weather like in London today?\"\n",
    "final_answer = run_conversation_with_tools(user_weather_prompt, verbose=True)\n",
    "print(f\"\\nRespuesta final: {final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant U as 👤 Usuario\n",
    "    participant M as 📝 Main Function\n",
    "    participant L as 🤖 LLM (Gemini)\n",
    "    participant P as 🛡️ Pydantic\n",
    "    participant T as ⚙️ Tool Function\n",
    "    \n",
    "    U->>M: \"What's the weather in London?\"\n",
    "    M->>L: Primera llamada con tools_specs\n",
    "    \n",
    "    Note over L: Analiza si necesita herramientas\n",
    "    L->>M: response_message con tool_calls\n",
    "    \n",
    "    Note over M: Para cada tool_call\n",
    "    M->>P: Validar argumentos con WeatherRequest\n",
    "    P->>M: validated_input: WeatherRequest(location=\"London\", unit=\"celsius\")\n",
    "    \n",
    "    M->>T: get_current_weather(validated_input)\n",
    "    T->>M: WeatherResponse(location=\"London\", temperature=\"10\", ...)\n",
    "    \n",
    "    M->>M: Convertir a JSON con model_dump_json()\n",
    "    M->>L: Segunda llamada con tool results\n",
    "    \n",
    "    Note over L: Procesa resultados y genera respuesta natural\n",
    "    L->>M: \"The weather in London is...\"\n",
    "    M->>U: Respuesta final\n",
    "    \n",
    "    rect rgb(240, 248, 255)\n",
    "        Note over P: 🛡️ Pydantic garantiza:<br/>- Validación de tipos<br/>- Estructura correcta<br/>- Manejo de errores\n",
    "    end\n",
    "    \n",
    "    rect rgb(248, 255, 240)\n",
    "        Note over T: ⚙️ Tool Function:<br/>- Recibe objeto tipado<br/>- Retorna objeto tipado<br/>- Lógica de negocio limpia\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "\n",
    "\n",
    " **RAG** mejora los LLMs al darles acceso a información externa y actualizada.\n",
    "\n",
    "\n",
    "\n",
    " **Flujo Básico:**\n",
    "\n",
    " 1.  **Consulta del Usuario.**\n",
    "\n",
    " 2.  **Recuperación (Retrieval):** Buscar información relevante en una base de datos (corpus).\n",
    "\n",
    " 3.  **Aumentación (Augmentation):** Combinar la información recuperada con la consulta original.\n",
    "\n",
    " 4.  **Generación (Generation):** El LLM usa el prompt enriquecido para responder.\n",
    "\n",
    "\n",
    "\n",
    " **Beneficios:** Reduce alucinaciones, usa datos actuales, permite citar fuentes.\n",
    "\n",
    "\n",
    "\n",
    " ### Ejemplo Básico de RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n",
      "[-0.009576111100614071, 0.024940399453043938, 0.00013856856094207615, -0.04367725923657417] ...\n",
      "\n",
      "Pregunta RAG: ¿Cuántas lunas tiene Marte?\n",
      "Respuesta RAG: Marte tiene dos lunas.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuración del modelo de embeddings\n",
    "EMBEDDINGS_MODEL = \"gemini-embedding-exp-03-07\"\n",
    "\n",
    "# Ejemplo básico de embeddings\n",
    "prompt = \"\"\"\n",
    "    The quick brown fox jumps over the lazy dog.\n",
    "\"\"\"\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=EMBEDDINGS_MODEL,\n",
    "    input=prompt,\n",
    ")\n",
    "\n",
    "print(len(response.data[0].embedding))\n",
    "print(response.data[0].embedding[:4], '...')\n",
    "\n",
    "# Corpus de documentos mejorado para RAG\n",
    "document_corpus = {\n",
    "    \"doc1\": \"El Sol es una estrella en el centro de nuestro sistema solar.\",\n",
    "    \"doc2\": \"La Luna es el satélite natural de la Tierra.\",\n",
    "    \"doc3\": \"Marte, el 'planeta rojo', tiene dos lunas: Fobos y Deimos.\"\n",
    "}\n",
    "\n",
    "def get_embedding(text, model=EMBEDDINGS_MODEL):\n",
    "    \"\"\"Obtiene el embedding de un texto usando Gemini.\"\"\"\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=text,\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calcula la similitud coseno entre dos vectores.\"\"\"\n",
    "    import math\n",
    "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "    magnitude1 = math.sqrt(sum(a * a for a in vec1))\n",
    "    magnitude2 = math.sqrt(sum(a * a for a in vec2))\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "def retrieve_relevant_documents(query, corpus, top_k=1):\n",
    "    \"\"\"Recupera documentos relevantes usando embeddings para mejor precisión.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    \n",
    "    # Calcular embeddings para todos los documentos\n",
    "    doc_embeddings = {}\n",
    "    for doc_id, text in corpus.items():\n",
    "        embedding = get_embedding(text)\n",
    "        if embedding:\n",
    "            doc_embeddings[doc_id] = embedding\n",
    "    \n",
    "    # Calcular similitudes\n",
    "    doc_scores = {}\n",
    "    for doc_id, doc_embedding in doc_embeddings.items():\n",
    "        similarity = cosine_similarity(query_embedding, doc_embedding)\n",
    "        doc_scores[doc_id] = similarity\n",
    "    \n",
    "    # Ordenar por similitud y devolver los top_k\n",
    "    sorted_docs = sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    return [corpus[doc_id] for doc_id, score in sorted_docs[:top_k] if score > 0]\n",
    "\n",
    "def rag_generate_answer(user_query, corpus, model=GEMINI_MODEL):\n",
    "    retrieved_texts = retrieve_relevant_documents(user_query, corpus)\n",
    "    context_for_llm = \"\\n\\n\".join(retrieved_texts) if retrieved_texts else \"No se encontró información específica.\"\n",
    "    \n",
    "    system_message_rag = \"Responde basándote en el contexto. Si no es suficiente, indícalo.\"\n",
    "    prompt_with_context = f\"Contexto:\\n{context_for_llm}\\n\\nPregunta: {user_query}\\nRespuesta:\"\n",
    "    return completion_gemini(prompt_with_context, system_message=system_message_rag, model=model)\n",
    "\n",
    "query_rag = \"¿Cuántas lunas tiene Marte?\"\n",
    "print(f\"\\nPregunta RAG: {query_rag}\\nRespuesta RAG: {rag_generate_answer(query_rag, document_corpus)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    A[\"👤 Usuario hace pregunta:<br/>¿Cuántas lunas tiene Marte?\"] --> B[\"🔍 Obtener embedding<br/>de la consulta\"]\n",
    "    \n",
    "    C[\"📚 Corpus de documentos:<br/>doc1: El Sol es una estrella...<br/>doc2: La Luna es el satélite...<br/>doc3: Marte tiene dos lunas...\"] --> D[\"🔢 Obtener embeddings<br/>de todos los documentos\"]\n",
    "    \n",
    "    B --> E[\"📊 Calcular similitud coseno<br/>entre consulta y cada documento\"]\n",
    "    D --> E\n",
    "    \n",
    "    E --> F[\"📋 Ordenar documentos<br/>por similitud (descendente)\"]\n",
    "    \n",
    "    F --> G[\"🎯 Seleccionar top_k<br/>documentos más relevantes\"]\n",
    "    \n",
    "    G --> H[\"📝 Construir contexto<br/>combinando textos recuperados\"]\n",
    "    \n",
    "    H --> I[\"🤖 Enviar al LLM con prompt:<br/>Contexto: [documentos recuperados]<br/>Pregunta: [consulta original]\"]\n",
    "    \n",
    "    I --> J[\"✅ Respuesta generada:<br/>Marte tiene dos lunas\"]\n",
    "    \n",
    "    subgraph \"🔧 Funciones del proceso\"\n",
    "        B1[\"get_embedding()\"]\n",
    "        B2[\"cosine_similarity()\"]\n",
    "        B3[\"retrieve_relevant_documents()\"]\n",
    "        B4[\"rag_generate_answer()\"]\n",
    "    end\n",
    "    \n",
    "    subgraph \"📈 Datos vectoriales\"\n",
    "        V1[\"Vector consulta:<br/>[0.1, -0.3, 0.7, ...]\"]\n",
    "        V2[\"Vector doc1:<br/>[0.2, -0.1, 0.4, ...]\"]\n",
    "        V3[\"Vector doc2:<br/>[0.5, 0.2, -0.1, ...]\"]\n",
    "        V4[\"Vector doc3:<br/>[0.1, -0.2, 0.8, ...]\"]\n",
    "    end\n",
    "    \n",
    "    B -.-> B1\n",
    "    E -.-> B2\n",
    "    G -.-> B3\n",
    "    I -.-> B4\n",
    "    \n",
    "    B --> V1\n",
    "    D --> V2\n",
    "    D --> V3\n",
    "    D --> V4\n",
    "    \n",
    "    style A fill:#e1f5fe\n",
    "    style J fill:#e8f5e8\n",
    "    style G fill:#fff3e0\n",
    "    style I fill:#f3e5f5\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Context Protocol (MCP) \n",
    "\n",
    "### ¿Qué es MCP?\n",
    "\n",
    "El **Model Context Protocol (MCP)** es un estándar abierto desarrollado por Anthropic que permite conectar modelos de IA (como Claude, ChatGPT, etc.) con sistemas externos y fuentes de datos de manera estandarizada. Es como un \"protocolo universal\" que resuelve el problema N×M: en lugar de crear integraciones personalizadas para cada combinación de modelo de IA y herramienta externa, MCP proporciona una interfaz común.\n",
    "\n",
    "### ¿Cómo se usa?\n",
    "\n",
    "MCP utiliza una arquitectura **cliente-servidor**:\n",
    "\n",
    "- **MCP Client**: Integrado en la aplicación de IA (como Claude Desktop)\n",
    "- **MCP Server**: Expone datos y herramientas específicas (GitHub, Google Drive, bases de datos, etc.)\n",
    "\n",
    "**Flujo típico:**\n",
    "1. El usuario hace una pregunta al modelo de IA\n",
    "2. El cliente MCP identifica qué servidores pueden ayudar\n",
    "3. Se obtiene información de los servidores MCP relevantes\n",
    "4. El modelo genera una respuesta usando ese contexto adicional\n",
    "\n",
    "### ¿Cómo implementarlo?\n",
    "\n",
    "#### Ejemplo básico de servidor MCP:\n",
    "\n",
    "```python\n",
    "# Estructura simplificada de un servidor MCP\n",
    "class MCPServer:\n",
    "    def list_tools(self):\n",
    "        \"\"\"Retorna herramientas disponibles\"\"\"\n",
    "        return [{\n",
    "            \"name\": \"get_user_data\",\n",
    "            \"description\": \"Obtiene datos del usuario\",\n",
    "            \"parameters\": {\"user_id\": \"string\"}\n",
    "        }]\n",
    "    \n",
    "    def call_tool(self, name, arguments):\n",
    "        \"\"\"Ejecuta la herramienta solicitada\"\"\"\n",
    "        if name == \"get_user_data\":\n",
    "            return fetch_user_from_database(arguments[\"user_id\"])\n",
    "```\n",
    "\n",
    "#### Integración con Claude Desktop:\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"my-server\": {\n",
    "      \"command\": \"python\",\n",
    "      \"args\": [\"my_mcp_server.py\"]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Beneficios Clave\n",
    "\n",
    "- **Estandarización**: Un protocolo para todas las integraciones\n",
    "- **Interoperabilidad**: Cualquier cliente MCP puede usar cualquier servidor MCP\n",
    "- **Escalabilidad**: Fácil agregar nuevas fuentes de datos sin reescribir código\n",
    "\n",
    "### Referencias\n",
    "\n",
    "1. **Anthropic Official Announcement**: [Introducing the Model Context Protocol](https://www.anthropic.com/news/model-context-protocol) - Announcement oficial de Anthropic del 25 de noviembre de 2024\n",
    "\n",
    "2. **Technical Deep Dive**: [What Is the Model Context Protocol (MCP) and How It Works](https://www.descope.com/learn/post/mcp) - Guía técnica detallada publicada en abril de 2025\n",
    "\n",
    "MCP está diseñado para ser el \"Language Server Protocol\" del mundo de la IA, simplificando dramáticamente cómo los modelos de lenguaje acceden a información y herramientas externas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Conclusión\n",
    "\n",
    "\n",
    "\n",
    " El prompt crafting es un arte y una ciencia. Entender y aplicar técnicas como few-shot, CoT, imitación de documentos, RAG, MCP, y tool calling nos permite aprovechar el potencial de los LLMs de manera responsable.\n",
    "\n",
    "\n",
    "\n",
    " **Recurso Adicional:** [https://www.promptingguide.ai/](https://www.promptingguide.ai/)\n",
    "\n",
    "\n",
    "\n",
    "**Uso de Langchain con Gemini**: [gemini-langchain-cheatsheet](https://www.philschmid.de/gemini-langchain-cheatsheet)\n",
    "\n",
    " ---\n",
    "\n",
    " Fin de la lección."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
