{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Introducci√≥n a la ingenier√≠a de prompts (Actualizado con Gemini)\n",
    "\n",
    "\n",
    "\n",
    " <!--\n",
    "\n",
    " Original lesson date: June 2, 2025\n",
    "\n",
    " This version updated to use Gemini models via OpenAI-compatible API,\n",
    "\n",
    " includes new sections on RAG and MCP, and is condensed for a 1-hour lesson.\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Configuraci√≥n del Entorno\n",
    "\n",
    "\n",
    "\n",
    " Primero, instalaremos la biblioteca de OpenAI y configuraremos nuestra clave API para usar los modelos Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai -q # Descomenta para instalar en Colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ejecutar la siguiente celda en Colab, tras haber creado un secreto con el nombre de `GOOGLE_API_KEY` que contenga tu clave API de Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key no encontrada. Por favor, ingr√©sala manualmente:\n",
      "Usando el modelo Gemini: gemini-2.0-flash\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json # Para formateo de JSON y manejo de argumentos de herramientas\n",
    "\n",
    "# Intenta obtener la clave API desde los secretos de Colab\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "except ImportError:\n",
    "    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        print(\"API Key no encontrada. Por favor, ingr√©sala manualmente:\")\n",
    "        GOOGLE_API_KEY = input()\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Se requiere una Google API Key para ejecutar este notebook.\")\n",
    "\n",
    "# Configuraci√≥n del cliente de OpenAI para usar Gemini\n",
    "client = OpenAI(\n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\" \n",
    "print(f\"Usando el modelo Gemini: {GEMINI_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ¬øQu√© es un Modelo de Lenguaje Grande (LLM)?\n",
    "\n",
    "\n",
    "\n",
    " Los LLMs son sistemas de IA entrenados para entender y generar texto similar al humano. Aprenden procesando grandes cantidades de datos textuales para predecir la siguiente palabra en un contexto dado. Su gran tama√±o es clave para su potencia.\n",
    "\n",
    "\n",
    "\n",
    " ### Breve Historia\n",
    "\n",
    "\n",
    "\n",
    " * **c. 2014: RNNs (Redes Neuronales Recurrentes)** dominaban, pero ten√≠an limitaciones con secuencias largas.\n",
    "\n",
    " * **Sept 2014: Attention Mechanism.** Permiti√≥ a los modelos enfocarse en partes relevantes de la entrada, superando cuellos de botella. (Referencia: *Neural Machine Translation by Jointly Learning to Align and Translate*)\n",
    "\n",
    " * **Jun 2017: Transformer Architecture.** Revolucion√≥ el campo, eliminando la necesidad de RNNs. (Referencia: *Attention Is All You Need*)\n",
    "\n",
    " * **Jun 2018: GPT-1.** Us√≥ solo el decodificador del Transformer para predecir la siguiente palabra. (Referencia: *Improving Language Understanding by Generative Pre-Training*)\n",
    "\n",
    " * **Feb 2019: GPT-2.** Mostr√≥ capacidades multitarea sin entrenamiento espec√≠fico, evidenciando el poder del preentrenamiento a gran escala. (Referencia: *Language Models are Unsupervised Multitask Learners*)\n",
    "\n",
    "\n",
    "\n",
    " GPT-2 ya pod√≠a realizar tareas como resumen, traducci√≥n y generaci√≥n de contenido, lo que plante√≥ tanto entusiasmo como preocupaciones sobre su uso malicioso. Esto nos lleva a la importancia del **prompt crafting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Prompt Crafting: El Arte de Hablar con las M√°quinas\n",
    "\n",
    "\n",
    "\n",
    " Una aplicaci√≥n de LLM act√∫a como una **capa de transformaci√≥n** entre el problema del usuario y el LLM. El \"prompt crafting\" es el dise√±o cuidadoso de las entradas (prompts) para guiar al LLM.\n",
    "\n",
    "\n",
    "\n",
    " Exploremos algunas t√©cnicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n de completado usando Gemini\n",
    "def completion_gemini(user_message, system_message=\"You are a base language model that performs text completion. You predict the next words or tokens to complete the given text. Do not engage in conversation or provide explanations unless specifically requested. Simply complete the text naturally.\", model=GEMINI_MODEL, temperature=0.7):\n",
    "    try:\n",
    "        messages = []\n",
    "        if system_message:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error al generar la completaci√≥n: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un Transformer es un modelo de red neuronal que se basa en mecanismos de autoatenci√≥n, destacando por su capacidad para procesar secuencias de datos en paralelo. Esto lo hace especialmente eficiente para tareas de procesamiento del lenguaje natural como la traducci√≥n autom√°tica, la generaci√≥n de texto y el an√°lisis de sentimientos.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ejemplo r√°pido\n",
    "print(completion_gemini(\"Explica brevemente qu√© es un Transformer en IA.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ### T√©cnica 1: Few-Shot Prompting\n",
    "\n",
    "\n",
    "\n",
    " Se proporcionan al LLM algunos ejemplos de la tarea deseada para guiar su respuesta.\n",
    "\n",
    "\n",
    "\n",
    " ```markdown\n",
    "\n",
    " > How are you doing today?\n",
    "\n",
    " < ¬øC√≥mo est√°s hoy?\n",
    "\n",
    " > My name is John.\n",
    "\n",
    " < Mi nombre es John.\n",
    "\n",
    " > Can I have fries with that?\n",
    "\n",
    " ```\n",
    "\n",
    " El LLM continuar√° el patr√≥n.\n",
    "\n",
    "\n",
    "\n",
    " * **Clave:** Simplicidad y efectividad. El contexto inicial es crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øMe da papas fritas con eso?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"Q: How are you doing today?\n",
    "A: ¬øC√≥mo est√°s hoy?\n",
    "Q: My name is John.\n",
    "A: Mi nombre es John.\n",
    "Q: Can I have fries with that?\n",
    "A:\"\"\"\n",
    "\n",
    "\n",
    "print(completion_gemini(few_shot_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ### T√©cnica 2: Chain of Thought Reasoning (CoT)\n",
    "\n",
    "\n",
    "\n",
    " Se instruye al LLM para que \"piense paso a paso\" o se le muestran ejemplos de razonamiento. Esto mejora la l√≥gica en problemas complejos.\n",
    "\n",
    "\n",
    "\n",
    " **Ejemplo con CoT (few-shot):**\n",
    "\n",
    " ```markdown\n",
    "\n",
    " P: Jim tiene el doble de la edad de Steve. Jim tiene 12 a√±os, ¬øcu√°ntos a√±os tiene Steve?\n",
    "\n",
    " R: En forma de ecuaci√≥n: 12 = 2*a donde a es la edad de Steve. Dividiendo ambos lados por 2 vemos que a = 6. Steve tiene 6 a√±os.\n",
    "\n",
    " P: A un panadero le lleva una hora hacer un pastel. ¬øCu√°nto tiempo tardan 3 panaderos en hacer 3 pasteles?\n",
    "\n",
    " R:\n",
    "\n",
    " ```\n",
    "\n",
    " **Respuesta esperada (correcta):** Si cada panadero tarda 1 hora en hacer un pastel, entonces 3 panaderos tardar√°n 1 hora en hacer 3 pasteles.\n",
    "\n",
    "\n",
    "\n",
    " **Zero-shot CoT:** Simplemente a√±adir \"Pensemos paso a paso\" puede ayudar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cada panadero tarda una hora en hacer un pastel.\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought_prompt = \"\"\"Q: Jim tiene el doble de la edad de Steve. Jim tiene 12 a√±os, ¬øcu√°ntos a√±os tiene Steve?\n",
    "A: En forma de ecuaci√≥n: 12 = 2*a donde a es la edad de Steve. Dividiendo ambos lados por 2 vemos que a = 6. Steve tiene 6 a√±os.\n",
    "Q: A un panadero le lleva una hora hacer un pastel. ¬øCu√°nto tiempo tardan 3 panaderos en hacer 3 pasteles?\n",
    "A:\"\"\"\n",
    "\n",
    "print(completion_gemini(chain_of_thought_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un panadero puede hacer un pastel en una hora.\n",
      "Por lo tanto, 3 panaderos pueden hacer 3 pasteles en una hora.\n",
      "La respuesta es 1 hora.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "zero_shot_cot_prompt = \"\"\"Q: A un panadero le lleva una hora hacer un pastel. ¬øCu√°nto tiempo tardan 3 panaderos en hacer 3 pasteles?\n",
    "A: Pensemos paso a paso.\"\"\"\n",
    "\n",
    "print(completion_gemini(zero_shot_cot_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ### T√©cnica 3: Imitaci√≥n de Documentos\n",
    "\n",
    "\n",
    "\n",
    " Estructurar el prompt para imitar formatos conocidos (transcripciones, c√≥digo, etc.) ayuda al LLM a entender el contexto.\n",
    "\n",
    "\n",
    "\n",
    " **Ejemplo (Soporte T√©cnico):**\n",
    "\n",
    " ```markdown\n",
    "\n",
    " # IT Support Assistant\n",
    "\n",
    " The following is a transcript between an award-winning IT support rep and a customer.\n",
    "\n",
    "\n",
    "\n",
    " ## Customer:\n",
    "\n",
    " My cable is out! And I'm going to miss the Super Bowl!\n",
    "\n",
    "\n",
    "\n",
    " ## Support Assistant:\n",
    "\n",
    " ```\n",
    "\n",
    " Esto le da al LLM pistas sobre el rol, la estructura y el tono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh no! That's terrible timing! I understand how frustrating it is to miss the Super Bowl because of a cable outage. Let's get this fixed for you ASAP. \n",
      "\n",
      "First, can you tell me a little more about what's happening? Are you getting any error messages on your TV screen? Is the picture just black, or is it pixelated or fuzzy? Knowing a bit more will help me troubleshoot more effectively.\n",
      "\n",
      "In the meantime, I'll start checking for any known outages in your area. Let's get you back to the game! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_mimicry_prompt = \"\"\"# IT Support Assistant\n",
    "The following is a transcript between an award-winning IT support rep and a customer.\n",
    "\n",
    "## Customer: \n",
    "My cable is out! And I'm going to miss the Super Bowl!\n",
    "\n",
    "## Support Assistant:\"\"\"\n",
    "\n",
    "print(completion_gemini(document_mimicry_prompt, system_message=\"You are an award-winning IT support representative. Be helpful and empathetic.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Creando el Prompt: Un Proceso\n",
    "\n",
    "\n",
    "\n",
    " 1.  **Recopilar contexto:** ¬øQu√© informaci√≥n necesita el LLM?\n",
    "\n",
    " 2.  **Rankear contexto:** ¬øQu√© es lo m√°s importante?\n",
    "\n",
    " 3.  **Recortar contexto:** Evitar informaci√≥n innecesaria (los LLMs tienen ventanas de contexto limitadas).\n",
    "\n",
    " 4.  **Ensamblar el prompt:** Combinar todo de forma clara.\n",
    "\n",
    "\n",
    "\n",
    " <!-- Nota para Colab: Las siguientes im√°genes son referenciadas desde una carpeta local 'images'.\n",
    "\n",
    "      Aseg√∫rate de que esta carpeta est√© en tu Google Drive y montada,\n",
    "\n",
    "      o reemplaza 'src' con URLs directas a las im√°genes si est√°n alojadas online. -->\n",
    "\n",
    " <img style=\"width: 70%\" src=\"images/fig-14.png\" id=\"llm-app\" alt=\"[Diagrama conceptual de una aplicaci√≥n LLM]\"/>\n",
    "\n",
    "\n",
    "\n",
    " *Leyenda: Diagrama conceptual de una aplicaci√≥n LLM, mostrando la capa de transformaci√≥n entre el usuario y el modelo.*\n",
    "\n",
    "\n",
    "\n",
    " ### El Chat y las \"Tools\" (Function Calling)\n",
    "\n",
    "\n",
    "\n",
    " Con la llegada de modelos conversacionales como ChatGPT, surgieron formatos como ChatML, que usan roles (`system`, `user`, `assistant`). Los **mensajes de sistema** son cruciales para guiar el comportamiento del LLM.\n",
    "\n",
    "\n",
    "\n",
    " <img style=\"width: 70%\" src=\"images/fig-17.png\" id=\"llm-app-chatml\" alt=\"[Ejemplo de estructura de mensajes en ChatML]\"/>\n",
    "\n",
    " *Leyenda: Ejemplo de estructura de mensajes en ChatML, mostrando roles de sistema, usuario y asistente.*\n",
    "\n",
    "\n",
    "\n",
    " El **Function Calling** (o \"Tool Calling\") extiende esto, permitiendo a los LLMs interactuar con APIs externas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Estructuraci√≥n de Salida en Modo JSON\n",
    "\n",
    "\n",
    "\n",
    " Los LLMs pueden generar respuestas en JSON, √∫til para la integraci√≥n. Se usa `response_format={\"type\": \"json_object\"}`. Para estructuras complejas, Pydantic es una buena opci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"description\": \"Una canasta con una manzana roja y un pl√°tano amarillo.\",\n",
      "  \"fruits\": [\n",
      "    {\n",
      "      \"name\": \"manzana\",\n",
      "      \"color\": \"roja\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"pl√°tano\",\n",
      "      \"color\": \"amarillo\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# !pip install pydantic -q # Descomenta para instalar en Colab\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Fruit(BaseModel):\n",
    "    name: str = Field(description=\"The name of the fruit\")\n",
    "    color: str = Field(description=\"The color of the fruit\")\n",
    "\n",
    "class FruitBasket(BaseModel):\n",
    "    fruits: List[Fruit] = Field(description=\"A list of fruits\")\n",
    "    description: str = Field(description=\"A brief description\")\n",
    "\n",
    "def chat_gemini_pydantic_json(user_prompt, system_message=\"You are a helpful assistant designed to output structured JSON data.\", model=GEMINI_MODEL):\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            response_format=FruitBasket\n",
    "        )\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Ejemplo Pydantic\n",
    "pydantic_user_prompt = \"Describe una canasta con una manzana roja y un pl√°tano amarillo.\"\n",
    "print(chat_gemini_pydantic_json(pydantic_user_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Tool Calling (Function Calling) con Gemini\n",
    "\n",
    "\n",
    "\n",
    " El \"Tool Calling\" permite a los LLMs interactuar con funciones externas que t√∫ defines, ampliando sus capacidades m√°s all√° de la generaci√≥n de texto. Es como darle al LLM herramientas para realizar tareas espec√≠ficas.\n",
    "\n",
    "\n",
    "\n",
    " **Flujo Detallado:**\n",
    "\n",
    "\n",
    "\n",
    " 1.  **Definici√≥n de la Herramienta (Tu C√≥digo Python):**\n",
    "\n",
    "     * Creas una funci√≥n Python normal que realiza una tarea espec√≠fica (ej., obtener el clima, buscar en una base de datos, calcular algo).\n",
    "\n",
    "     * Esta funci√≥n debe devolver idealmente una cadena (a menudo JSON) que el LLM pueda entender.\n",
    "\n",
    "\n",
    "\n",
    " 2.  **Especificaci√≥n de la Herramienta para el LLM (JSON Schema):**\n",
    "\n",
    "     * Le describes esta funci√≥n al LLM usando un formato espec√≠fico (JSON Schema).\n",
    "\n",
    "     * Esta descripci√≥n incluye:\n",
    "\n",
    "         * `name`: Nombre de la funci√≥n (debe coincidir con tu funci√≥n Python).\n",
    "\n",
    "         * `description`: Qu√© hace la funci√≥n (para que el LLM sepa cu√°ndo usarla).\n",
    "\n",
    "         * `parameters`: Qu√© argumentos necesita la funci√≥n, sus tipos y descripciones.\n",
    "\n",
    "\n",
    "\n",
    " 3.  **Llamada Inicial al LLM:**\n",
    "\n",
    "     * Env√≠as el prompt del usuario al LLM.\n",
    "\n",
    "     * Junto con el prompt, env√≠as la lista de `tools` (las especificaciones de tus herramientas).\n",
    "\n",
    "     * Usas `tool_choice=\"auto\"` para que el LLM decida si necesita usar alguna herramienta para responder al prompt del usuario.\n",
    "\n",
    "\n",
    "\n",
    " 4.  **Respuesta del LLM (Decisi√≥n de Usar Herramienta):**\n",
    "\n",
    "     * Si el LLM determina que necesita una herramienta, en lugar de una respuesta textual directa, su mensaje contendr√° `tool_calls`.\n",
    "\n",
    "     * Cada `tool_call` indica:\n",
    "\n",
    "         * `id`: Un identificador √∫nico para esta llamada.\n",
    "\n",
    "         * `function.name`: El nombre de la funci√≥n que quiere ejecutar.\n",
    "\n",
    "         * `function.arguments`: Una cadena JSON con los argumentos que el LLM cree que tu funci√≥n necesita.\n",
    "\n",
    "\n",
    "\n",
    " 5.  **Ejecuci√≥n de la Herramienta (Tu C√≥digo):**\n",
    "\n",
    "     * Tu c√≥digo recibe estos `tool_calls`.\n",
    "\n",
    "     * Para cada `tool_call`:\n",
    "\n",
    "         * Identificas qu√© funci√≥n Python ejecutar bas√°ndote en `function.name`.\n",
    "\n",
    "         * Parseas `function.arguments` (de JSON a un diccionario Python).\n",
    "\n",
    "         * Llamas a tu funci√≥n Python con esos argumentos.\n",
    "\n",
    "\n",
    "\n",
    " 6.  **Env√≠o de Resultados al LLM:**\n",
    "\n",
    "     * A√±ades un nuevo mensaje al historial de la conversaci√≥n. Este mensaje tiene:\n",
    "\n",
    "         * `role`: \"tool\"\n",
    "\n",
    "         * `tool_call_id`: El mismo ID de la `tool_call` original.\n",
    "\n",
    "         * `name`: El nombre de la funci√≥n que se ejecut√≥.\n",
    "\n",
    "         * `content`: El resultado que devolvi√≥ tu funci√≥n Python.\n",
    "\n",
    "\n",
    "\n",
    " 7.  **Llamada Final al LLM:**\n",
    "\n",
    "     * Vuelves a llamar al LLM con el historial de conversaci√≥n actualizado (que ahora incluye la solicitud de herramienta y su resultado).\n",
    "\n",
    "     * El LLM usar√° el resultado de la herramienta para formular una respuesta final y natural al prompt original del usuario.\n",
    "\n",
    "\n",
    "\n",
    " <img style=\"width: 70%\" src=\"images/fig-18.png\" id=\"llm-app-tools\" alt=\"[Diagrama de flujo de Function Calling]\"/>\n",
    "\n",
    " *Leyenda: Flujo simplificado de Function Calling, mostrando la interacci√≥n entre el usuario, el LLM y las herramientas externas.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What's the weather like in London today?\n",
      "\n",
      "LLM decidi√≥: usar herramientas\n",
      "\n",
      "Ejecutando: get_current_weather con {'location': 'London', 'unit': 'celsius'}\n",
      "\n",
      "Resultado: {\"location\":\"London\",\"temperature\":\"10\",\"unit\":\"celsius\",\"condition\":\"Nublado\",\"error\":null}\n",
      "\n",
      "Generando respuesta final...\n",
      "\n",
      "\n",
      "Respuesta final: The weather in London today is Nublado with a temperature of 10 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Literal\n",
    "import json\n",
    "\n",
    "# Modelos Pydantic para definir las herramientas\n",
    "class WeatherRequest(BaseModel):\n",
    "    location: str = Field(description=\"La ciudad, ej. San Francisco, CA\")\n",
    "    unit: Literal[\"celsius\", \"fahrenheit\"] = Field(default=\"celsius\", description=\"Unidad de temperatura\")\n",
    "\n",
    "class WeatherResponse(BaseModel):\n",
    "    location: str\n",
    "    temperature: str\n",
    "    unit: str\n",
    "    condition: str\n",
    "    error: Optional[str] = None\n",
    "\n",
    "# Funci√≥n de herramienta simplificada\n",
    "def get_current_weather(request: WeatherRequest) -> WeatherResponse:\n",
    "    \"\"\"Obtiene el clima actual para una ubicaci√≥n dada (simulado).\"\"\"\n",
    "    weather_data = {\n",
    "        \"tokyo\": {\"temperature\": \"15\", \"condition\": \"Soleado\"},\n",
    "        \"london\": {\"temperature\": \"10\", \"condition\": \"Nublado\"},\n",
    "        \"paris\": {\"temperature\": \"12\", \"condition\": \"Lluvioso\"}\n",
    "    }\n",
    "    \n",
    "    city = request.location.lower().split(\",\")[0]\n",
    "    \n",
    "    if city in weather_data:\n",
    "        return WeatherResponse(\n",
    "            location=request.location,\n",
    "            temperature=weather_data[city][\"temperature\"],\n",
    "            unit=request.unit,\n",
    "            condition=weather_data[city][\"condition\"]\n",
    "        )\n",
    "    \n",
    "    return WeatherResponse(\n",
    "        location=request.location,\n",
    "        temperature=\"\",\n",
    "        unit=request.unit,\n",
    "        condition=\"\",\n",
    "        error=\"Clima no encontrado\"\n",
    "    )\n",
    "\n",
    "# Generaci√≥n autom√°tica de tool specs desde Pydantic\n",
    "def generate_tool_spec(model_class: BaseModel, function_name: str, description: str):\n",
    "    \"\"\"Genera autom√°ticamente la especificaci√≥n de herramienta desde un modelo Pydantic.\"\"\"\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": function_name,\n",
    "            \"description\": description,\n",
    "            \"parameters\": model_class.model_json_schema()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Tool specs generadas autom√°ticamente\n",
    "tools_specs = [\n",
    "    generate_tool_spec(\n",
    "        WeatherRequest, \n",
    "        \"get_current_weather\", \n",
    "        \"Obtiene el clima actual en una ubicaci√≥n espec√≠fica.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Registro de herramientas disponibles\n",
    "AVAILABLE_TOOLS = {\n",
    "    \"get_current_weather\": {\n",
    "        \"function\": get_current_weather,\n",
    "        \"input_model\": WeatherRequest\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_conversation_with_tools(user_prompt: str, verbose: bool = False) -> str:\n",
    "    \"\"\"Ejecuta una conversaci√≥n con herramientas usando Pydantic para validaci√≥n.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "    if verbose: print(f\"User: {user_prompt}\\n\")\n",
    "\n",
    "    try:\n",
    "        # Primera llamada al LLM\n",
    "        response = client.chat.completions.create(\n",
    "            model=GEMINI_MODEL,\n",
    "            messages=messages,\n",
    "            tools=tools_specs,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        \n",
    "        response_message = response.choices[0].message\n",
    "        if verbose: print(f\"LLM decidi√≥: {'usar herramientas' if response_message.tool_calls else 'responder directamente'}\\n\")\n",
    "\n",
    "        # Si no hay tool calls, devolver respuesta directa\n",
    "        if not response_message.tool_calls:\n",
    "            return response_message.content\n",
    "\n",
    "        # Procesar tool calls\n",
    "        messages.append(response_message)\n",
    "        \n",
    "        for tool_call in response_message.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            \n",
    "            if function_name not in AVAILABLE_TOOLS:\n",
    "                if verbose: print(f\"Herramienta no encontrada: {function_name}\\n\")\n",
    "                continue\n",
    "                \n",
    "            tool_info = AVAILABLE_TOOLS[function_name]\n",
    "            \n",
    "            # Validar argumentos con Pydantic\n",
    "            try:\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                validated_input = tool_info[\"input_model\"](**function_args)\n",
    "                if verbose: print(f\"Ejecutando: {function_name} con {validated_input.model_dump()}\\n\")\n",
    "                \n",
    "                # Ejecutar funci√≥n\n",
    "                result = tool_info[\"function\"](validated_input)\n",
    "                tool_response = result.model_dump_json()\n",
    "                if verbose: print(f\"Resultado: {tool_response}\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                tool_response = json.dumps({\"error\": f\"Error validando argumentos: {str(e)}\"})\n",
    "                if verbose: print(f\"Error: {tool_response}\\n\")\n",
    "            \n",
    "            # A√±adir respuesta de herramienta al historial\n",
    "            messages.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": tool_response\n",
    "            })\n",
    "        \n",
    "        # Segunda llamada al LLM con resultados de herramientas\n",
    "        if verbose: print(\"Generando respuesta final...\\n\")\n",
    "        final_response = client.chat.completions.create(\n",
    "            model=GEMINI_MODEL, \n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        return final_response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error en Tool Calling: {e}\"\n",
    "\n",
    "# Ejemplo de uso\n",
    "user_weather_prompt = \"What's the weather like in London today?\"\n",
    "final_answer = run_conversation_with_tools(user_weather_prompt, verbose=True)\n",
    "print(f\"\\nRespuesta final: {final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant U as üë§ Usuario\n",
    "    participant M as üìù Main Function\n",
    "    participant L as ü§ñ LLM (Gemini)\n",
    "    participant P as üõ°Ô∏è Pydantic\n",
    "    participant T as ‚öôÔ∏è Tool Function\n",
    "    \n",
    "    U->>M: \"What's the weather in London?\"\n",
    "    M->>L: Primera llamada con tools_specs\n",
    "    \n",
    "    Note over L: Analiza si necesita herramientas\n",
    "    L->>M: response_message con tool_calls\n",
    "    \n",
    "    Note over M: Para cada tool_call\n",
    "    M->>P: Validar argumentos con WeatherRequest\n",
    "    P->>M: validated_input: WeatherRequest(location=\"London\", unit=\"celsius\")\n",
    "    \n",
    "    M->>T: get_current_weather(validated_input)\n",
    "    T->>M: WeatherResponse(location=\"London\", temperature=\"10\", ...)\n",
    "    \n",
    "    M->>M: Convertir a JSON con model_dump_json()\n",
    "    M->>L: Segunda llamada con tool results\n",
    "    \n",
    "    Note over L: Procesa resultados y genera respuesta natural\n",
    "    L->>M: \"The weather in London is...\"\n",
    "    M->>U: Respuesta final\n",
    "    \n",
    "    rect rgb(240, 248, 255)\n",
    "        Note over P: üõ°Ô∏è Pydantic garantiza:<br/>- Validaci√≥n de tipos<br/>- Estructura correcta<br/>- Manejo de errores\n",
    "    end\n",
    "    \n",
    "    rect rgb(248, 255, 240)\n",
    "        Note over T: ‚öôÔ∏è Tool Function:<br/>- Recibe objeto tipado<br/>- Retorna objeto tipado<br/>- L√≥gica de negocio limpia\n",
    "    end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "\n",
    "\n",
    " **RAG** mejora los LLMs al darles acceso a informaci√≥n externa y actualizada.\n",
    "\n",
    "\n",
    "\n",
    " **Flujo B√°sico:**\n",
    "\n",
    " 1.  **Consulta del Usuario.**\n",
    "\n",
    " 2.  **Recuperaci√≥n (Retrieval):** Buscar informaci√≥n relevante en una base de datos (corpus).\n",
    "\n",
    " 3.  **Aumentaci√≥n (Augmentation):** Combinar la informaci√≥n recuperada con la consulta original.\n",
    "\n",
    " 4.  **Generaci√≥n (Generation):** El LLM usa el prompt enriquecido para responder.\n",
    "\n",
    "\n",
    "\n",
    " **Beneficios:** Reduce alucinaciones, usa datos actuales, permite citar fuentes.\n",
    "\n",
    "\n",
    "\n",
    " ### Ejemplo B√°sico de RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n",
      "[-0.009576111100614071, 0.024940399453043938, 0.00013856856094207615, -0.04367725923657417] ...\n",
      "\n",
      "Pregunta RAG: ¬øCu√°ntas lunas tiene Marte?\n",
      "Respuesta RAG: Marte tiene dos lunas.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n del modelo de embeddings\n",
    "EMBEDDINGS_MODEL = \"gemini-embedding-exp-03-07\"\n",
    "\n",
    "# Ejemplo b√°sico de embeddings\n",
    "prompt = \"\"\"\n",
    "    The quick brown fox jumps over the lazy dog.\n",
    "\"\"\"\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=EMBEDDINGS_MODEL,\n",
    "    input=prompt,\n",
    ")\n",
    "\n",
    "print(len(response.data[0].embedding))\n",
    "print(response.data[0].embedding[:4], '...')\n",
    "\n",
    "# Corpus de documentos mejorado para RAG\n",
    "document_corpus = {\n",
    "    \"doc1\": \"El Sol es una estrella en el centro de nuestro sistema solar.\",\n",
    "    \"doc2\": \"La Luna es el sat√©lite natural de la Tierra.\",\n",
    "    \"doc3\": \"Marte, el 'planeta rojo', tiene dos lunas: Fobos y Deimos.\"\n",
    "}\n",
    "\n",
    "def get_embedding(text, model=EMBEDDINGS_MODEL):\n",
    "    \"\"\"Obtiene el embedding de un texto usando Gemini.\"\"\"\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=text,\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calcula la similitud coseno entre dos vectores.\"\"\"\n",
    "    import math\n",
    "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "    magnitude1 = math.sqrt(sum(a * a for a in vec1))\n",
    "    magnitude2 = math.sqrt(sum(a * a for a in vec2))\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "def retrieve_relevant_documents(query, corpus, top_k=1):\n",
    "    \"\"\"Recupera documentos relevantes usando embeddings para mejor precisi√≥n.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    \n",
    "    # Calcular embeddings para todos los documentos\n",
    "    doc_embeddings = {}\n",
    "    for doc_id, text in corpus.items():\n",
    "        embedding = get_embedding(text)\n",
    "        if embedding:\n",
    "            doc_embeddings[doc_id] = embedding\n",
    "    \n",
    "    # Calcular similitudes\n",
    "    doc_scores = {}\n",
    "    for doc_id, doc_embedding in doc_embeddings.items():\n",
    "        similarity = cosine_similarity(query_embedding, doc_embedding)\n",
    "        doc_scores[doc_id] = similarity\n",
    "    \n",
    "    # Ordenar por similitud y devolver los top_k\n",
    "    sorted_docs = sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    return [corpus[doc_id] for doc_id, score in sorted_docs[:top_k] if score > 0]\n",
    "\n",
    "def rag_generate_answer(user_query, corpus, model=GEMINI_MODEL):\n",
    "    retrieved_texts = retrieve_relevant_documents(user_query, corpus)\n",
    "    context_for_llm = \"\\n\\n\".join(retrieved_texts) if retrieved_texts else \"No se encontr√≥ informaci√≥n espec√≠fica.\"\n",
    "    \n",
    "    system_message_rag = \"Responde bas√°ndote en el contexto. Si no es suficiente, ind√≠calo.\"\n",
    "    prompt_with_context = f\"Contexto:\\n{context_for_llm}\\n\\nPregunta: {user_query}\\nRespuesta:\"\n",
    "    return completion_gemini(prompt_with_context, system_message=system_message_rag, model=model)\n",
    "\n",
    "query_rag = \"¬øCu√°ntas lunas tiene Marte?\"\n",
    "print(f\"\\nPregunta RAG: {query_rag}\\nRespuesta RAG: {rag_generate_answer(query_rag, document_corpus)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    A[\"üë§ Usuario hace pregunta:<br/>¬øCu√°ntas lunas tiene Marte?\"] --> B[\"üîç Obtener embedding<br/>de la consulta\"]\n",
    "    \n",
    "    C[\"üìö Corpus de documentos:<br/>doc1: El Sol es una estrella...<br/>doc2: La Luna es el sat√©lite...<br/>doc3: Marte tiene dos lunas...\"] --> D[\"üî¢ Obtener embeddings<br/>de todos los documentos\"]\n",
    "    \n",
    "    B --> E[\"üìä Calcular similitud coseno<br/>entre consulta y cada documento\"]\n",
    "    D --> E\n",
    "    \n",
    "    E --> F[\"üìã Ordenar documentos<br/>por similitud (descendente)\"]\n",
    "    \n",
    "    F --> G[\"üéØ Seleccionar top_k<br/>documentos m√°s relevantes\"]\n",
    "    \n",
    "    G --> H[\"üìù Construir contexto<br/>combinando textos recuperados\"]\n",
    "    \n",
    "    H --> I[\"ü§ñ Enviar al LLM con prompt:<br/>Contexto: [documentos recuperados]<br/>Pregunta: [consulta original]\"]\n",
    "    \n",
    "    I --> J[\"‚úÖ Respuesta generada:<br/>Marte tiene dos lunas\"]\n",
    "    \n",
    "    subgraph \"üîß Funciones del proceso\"\n",
    "        B1[\"get_embedding()\"]\n",
    "        B2[\"cosine_similarity()\"]\n",
    "        B3[\"retrieve_relevant_documents()\"]\n",
    "        B4[\"rag_generate_answer()\"]\n",
    "    end\n",
    "    \n",
    "    subgraph \"üìà Datos vectoriales\"\n",
    "        V1[\"Vector consulta:<br/>[0.1, -0.3, 0.7, ...]\"]\n",
    "        V2[\"Vector doc1:<br/>[0.2, -0.1, 0.4, ...]\"]\n",
    "        V3[\"Vector doc2:<br/>[0.5, 0.2, -0.1, ...]\"]\n",
    "        V4[\"Vector doc3:<br/>[0.1, -0.2, 0.8, ...]\"]\n",
    "    end\n",
    "    \n",
    "    B -.-> B1\n",
    "    E -.-> B2\n",
    "    G -.-> B3\n",
    "    I -.-> B4\n",
    "    \n",
    "    B --> V1\n",
    "    D --> V2\n",
    "    D --> V3\n",
    "    D --> V4\n",
    "    \n",
    "    style A fill:#e1f5fe\n",
    "    style J fill:#e8f5e8\n",
    "    style G fill:#fff3e0\n",
    "    style I fill:#f3e5f5\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Context Protocol (MCP) \n",
    "\n",
    "### ¬øQu√© es MCP?\n",
    "\n",
    "El **Model Context Protocol (MCP)** es un est√°ndar abierto desarrollado por Anthropic que permite conectar modelos de IA (como Claude, ChatGPT, etc.) con sistemas externos y fuentes de datos de manera estandarizada. Es como un \"protocolo universal\" que resuelve el problema N√óM: en lugar de crear integraciones personalizadas para cada combinaci√≥n de modelo de IA y herramienta externa, MCP proporciona una interfaz com√∫n.\n",
    "\n",
    "### ¬øC√≥mo se usa?\n",
    "\n",
    "MCP utiliza una arquitectura **cliente-servidor**:\n",
    "\n",
    "- **MCP Client**: Integrado en la aplicaci√≥n de IA (como Claude Desktop)\n",
    "- **MCP Server**: Expone datos y herramientas espec√≠ficas (GitHub, Google Drive, bases de datos, etc.)\n",
    "\n",
    "**Flujo t√≠pico:**\n",
    "1. El usuario hace una pregunta al modelo de IA\n",
    "2. El cliente MCP identifica qu√© servidores pueden ayudar\n",
    "3. Se obtiene informaci√≥n de los servidores MCP relevantes\n",
    "4. El modelo genera una respuesta usando ese contexto adicional\n",
    "\n",
    "### ¬øC√≥mo implementarlo?\n",
    "\n",
    "#### Ejemplo b√°sico de servidor MCP:\n",
    "\n",
    "```python\n",
    "# Estructura simplificada de un servidor MCP\n",
    "class MCPServer:\n",
    "    def list_tools(self):\n",
    "        \"\"\"Retorna herramientas disponibles\"\"\"\n",
    "        return [{\n",
    "            \"name\": \"get_user_data\",\n",
    "            \"description\": \"Obtiene datos del usuario\",\n",
    "            \"parameters\": {\"user_id\": \"string\"}\n",
    "        }]\n",
    "    \n",
    "    def call_tool(self, name, arguments):\n",
    "        \"\"\"Ejecuta la herramienta solicitada\"\"\"\n",
    "        if name == \"get_user_data\":\n",
    "            return fetch_user_from_database(arguments[\"user_id\"])\n",
    "```\n",
    "\n",
    "#### Integraci√≥n con Claude Desktop:\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"my-server\": {\n",
    "      \"command\": \"python\",\n",
    "      \"args\": [\"my_mcp_server.py\"]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Beneficios Clave\n",
    "\n",
    "- **Estandarizaci√≥n**: Un protocolo para todas las integraciones\n",
    "- **Interoperabilidad**: Cualquier cliente MCP puede usar cualquier servidor MCP\n",
    "- **Escalabilidad**: F√°cil agregar nuevas fuentes de datos sin reescribir c√≥digo\n",
    "\n",
    "### Referencias\n",
    "\n",
    "1. **Anthropic Official Announcement**: [Introducing the Model Context Protocol](https://www.anthropic.com/news/model-context-protocol) - Announcement oficial de Anthropic del 25 de noviembre de 2024\n",
    "\n",
    "2. **Technical Deep Dive**: [What Is the Model Context Protocol (MCP) and How It Works](https://www.descope.com/learn/post/mcp) - Gu√≠a t√©cnica detallada publicada en abril de 2025\n",
    "\n",
    "MCP est√° dise√±ado para ser el \"Language Server Protocol\" del mundo de la IA, simplificando dram√°ticamente c√≥mo los modelos de lenguaje acceden a informaci√≥n y herramientas externas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Conclusi√≥n\n",
    "\n",
    "\n",
    "\n",
    " El prompt crafting es un arte y una ciencia. Entender y aplicar t√©cnicas como few-shot, CoT, imitaci√≥n de documentos, RAG, MCP, y tool calling nos permite aprovechar el potencial de los LLMs de manera responsable.\n",
    "\n",
    "\n",
    "\n",
    " **Recurso Adicional:** [https://www.promptingguide.ai/](https://www.promptingguide.ai/)\n",
    "\n",
    "\n",
    "\n",
    "**Uso de Langchain con Gemini**: [gemini-langchain-cheatsheet](https://www.philschmid.de/gemini-langchain-cheatsheet)\n",
    "\n",
    " ---\n",
    "\n",
    " Fin de la lecci√≥n."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
